nohup: ignoring input
cora, citeseer, pubmed, amz-photo, amz-computer, cora-full
cora experiment with fixed GCN hidden layer



Initializing normal onedecoder
Namespace(alpha=1, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=0, device=device(type='cuda'), dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 748
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
Epoch: 0001 train_loss= 33.29372 accuracy= 0.60340 time= 0.02654
Epoch: 0011 train_loss= 6.15808 accuracy= 0.70089 time= 0.02275
Epoch: 0021 train_loss= 5.25470 accuracy= 0.71123 time= 0.01851
Epoch: 0031 train_loss= 5.16880 accuracy= 0.71935 time= 0.01837
Epoch: 0041 train_loss= 5.14721 accuracy= 0.71861 time= 0.01838
Epoch: 0051 train_loss= 5.12221 accuracy= 0.72083 time= 0.01832
Epoch: 0061 train_loss= 5.09425 accuracy= 0.72230 time= 0.01837
Epoch: 0071 train_loss= 5.06570 accuracy= 0.72821 time= 0.01839
Epoch: 0081 train_loss= 5.04160 accuracy= 0.72378 time= 0.01835
Epoch: 0091 train_loss= 5.01837 accuracy= 0.72674 time= 0.01844
Epoch: 0101 train_loss= 4.99642 accuracy= 0.72821 time= 0.01836
Epoch: 0111 train_loss= 4.97232 accuracy= 0.73338 time= 0.01830
Epoch: 0121 train_loss= 4.94272 accuracy= 0.73929 time= 0.01832
Epoch: 0131 train_loss= 4.91423 accuracy= 0.74372 time= 0.01837
Epoch: 0141 train_loss= 4.88775 accuracy= 0.74446 time= 0.01837
Epoch: 0151 train_loss= 4.86268 accuracy= 0.74446 time= 0.01831
Epoch: 0161 train_loss= 4.84703 accuracy= 0.74742 time= 0.01841
Epoch: 0171 train_loss= 4.81134 accuracy= 0.74668 time= 0.01833
Epoch: 0181 train_loss= 4.79362 accuracy= 0.74668 time= 0.01835
Epoch: 0191 train_loss= 4.80216 accuracy= 0.75258 time= 0.01836
Epoch: 0201 train_loss= 4.77725 accuracy= 0.74963 time= 0.01835
Epoch: 0211 train_loss= 4.74667 accuracy= 0.74446 time= 0.01831
Epoch: 0221 train_loss= 4.74703 accuracy= 0.74594 time= 0.01836
Epoch: 0231 train_loss= 4.72452 accuracy= 0.74889 time= 0.01828
Epoch: 0241 train_loss= 4.70940 accuracy= 0.74889 time= 0.01830
Epoch: 0251 train_loss= 4.68832 accuracy= 0.74446 time= 0.01834
Epoch: 0261 train_loss= 4.67775 accuracy= 0.74963 time= 0.01839
Epoch: 0271 train_loss= 4.65246 accuracy= 0.74889 time= 0.01835
Epoch: 0281 train_loss= 4.65177 accuracy= 0.74963 time= 0.01838
Epoch: 0291 train_loss= 4.63628 accuracy= 0.75702 time= 0.01835

accuracy 0.75258
auc 0.69823
f1_score 0.44167
Job finished!



Initializing normal onedecoder
Namespace(alpha=1, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=1, device=device(type='cuda'), dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 648
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
Epoch: 0001 train_loss= 4085.50391 accuracy= 0.61595 time= 0.02270
Epoch: 0011 train_loss= 1888.46936 accuracy= 0.61152 time= 0.01916
Epoch: 0021 train_loss= 1243.49023 accuracy= 0.61669 time= 0.01563
Epoch: 0031 train_loss= 1005.47443 accuracy= 0.61891 time= 0.01566
Epoch: 0041 train_loss= 922.05444 accuracy= 0.63294 time= 0.01567
Epoch: 0051 train_loss= 885.45715 accuracy= 0.62334 time= 0.01582
Epoch: 0061 train_loss= 853.81775 accuracy= 0.64549 time= 0.01556
Epoch: 0071 train_loss= 830.26947 accuracy= 0.64106 time= 0.01566
Epoch: 0081 train_loss= 803.57782 accuracy= 0.63220 time= 0.01567
Epoch: 0091 train_loss= 772.99591 accuracy= 0.62555 time= 0.01563
Epoch: 0101 train_loss= 758.32587 accuracy= 0.62999 time= 0.01563
Epoch: 0111 train_loss= 728.44995 accuracy= 0.63072 time= 0.01561
Epoch: 0121 train_loss= 705.86566 accuracy= 0.62186 time= 0.01559
Epoch: 0131 train_loss= 679.52936 accuracy= 0.62038 time= 0.01561
Epoch: 0141 train_loss= 654.34760 accuracy= 0.61965 time= 0.01561
Epoch: 0151 train_loss= 629.67279 accuracy= 0.62703 time= 0.01564
Epoch: 0161 train_loss= 599.97687 accuracy= 0.61448 time= 0.01560
Epoch: 0171 train_loss= 585.51727 accuracy= 0.61743 time= 0.01563
Epoch: 0181 train_loss= 562.19897 accuracy= 0.61078 time= 0.01562
Epoch: 0191 train_loss= 538.30121 accuracy= 0.61521 time= 0.01572
Epoch: 0201 train_loss= 521.70660 accuracy= 0.61300 time= 0.01565
Epoch: 0211 train_loss= 501.76883 accuracy= 0.61669 time= 0.01564
Epoch: 0221 train_loss= 477.98734 accuracy= 0.61152 time= 0.01564
Epoch: 0231 train_loss= 460.59915 accuracy= 0.61078 time= 0.01561
Epoch: 0241 train_loss= 443.44214 accuracy= 0.61595 time= 0.01567
Epoch: 0251 train_loss= 423.36377 accuracy= 0.61374 time= 0.01569
Epoch: 0261 train_loss= 404.57541 accuracy= 0.61743 time= 0.01562
Epoch: 0271 train_loss= 389.41159 accuracy= 0.61374 time= 0.01559
Epoch: 0281 train_loss= 373.46609 accuracy= 0.61891 time= 0.01564
Epoch: 0291 train_loss= 355.83386 accuracy= 0.61521 time= 0.01563

accuracy 0.61300
auc 0.27077
f1_score 0.12667
Job finished!



Initializing normal twodecoders
Namespace(alpha=1, beta=1, clique_size=20, cuda=False, dataset='cora', device=device(type='cuda'), dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 32
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
Epoch: 0001 train_loss= 3993.13696 feature_loss= 33.28036 structure_loss= 3959.85669 accuracy= 0.61078 accuracy_s= 0.77843 accuracy_f= 0.80502 time= 0.03737
Epoch: 0011 train_loss= 1850.79480 feature_loss= 11.91753 structure_loss= 1838.87732 accuracy= 0.60857 accuracy_s= 0.78065 accuracy_f= 0.83309 time= 0.02725
Epoch: 0021 train_loss= 1197.27356 feature_loss= 8.13021 structure_loss= 1189.14331 accuracy= 0.61300 accuracy_s= 0.78139 accuracy_f= 0.83752 time= 0.02721
Epoch: 0031 train_loss= 1003.38580 feature_loss= 6.28077 structure_loss= 997.10504 accuracy= 0.61004 accuracy_s= 0.77917 accuracy_f= 0.84047 time= 0.02711
Epoch: 0041 train_loss= 925.02148 feature_loss= 5.62935 structure_loss= 919.39215 accuracy= 0.61891 accuracy_s= 0.77843 accuracy_f= 0.83752 time= 0.02698
Epoch: 0051 train_loss= 889.67224 feature_loss= 5.31756 structure_loss= 884.35468 accuracy= 0.64402 accuracy_s= 0.77991 accuracy_f= 0.84047 time= 0.02727
Epoch: 0061 train_loss= 859.35455 feature_loss= 5.16664 structure_loss= 854.18793 accuracy= 0.68095 accuracy_s= 0.78508 accuracy_f= 0.83973 time= 0.02698
Epoch: 0071 train_loss= 823.06854 feature_loss= 5.11243 structure_loss= 817.95612 accuracy= 0.68538 accuracy_s= 0.78951 accuracy_f= 0.84047 time= 0.02724
Epoch: 0081 train_loss= 807.39526 feature_loss= 5.09120 structure_loss= 802.30408 accuracy= 0.69424 accuracy_s= 0.78582 accuracy_f= 0.83973 time= 0.02704
Epoch: 0091 train_loss= 779.23993 feature_loss= 5.08107 structure_loss= 774.15887 accuracy= 0.68390 accuracy_s= 0.78656 accuracy_f= 0.83900 time= 0.02729
Epoch: 0101 train_loss= 751.87122 feature_loss= 5.07369 structure_loss= 746.79755 accuracy= 0.68242 accuracy_s= 0.78213 accuracy_f= 0.83900 time= 0.02707
Epoch: 0111 train_loss= 724.29980 feature_loss= 5.06613 structure_loss= 719.23370 accuracy= 0.68981 accuracy_s= 0.78582 accuracy_f= 0.83900 time= 0.02700
Epoch: 0121 train_loss= 701.76294 feature_loss= 5.05865 structure_loss= 696.70428 accuracy= 0.68242 accuracy_s= 0.78139 accuracy_f= 0.83900 time= 0.02725
Epoch: 0131 train_loss= 672.95123 feature_loss= 5.05147 structure_loss= 667.89978 accuracy= 0.68685 accuracy_s= 0.78360 accuracy_f= 0.83900 time= 0.02701
Epoch: 0141 train_loss= 651.57629 feature_loss= 5.04423 structure_loss= 646.53204 accuracy= 0.68981 accuracy_s= 0.78139 accuracy_f= 0.83973 time= 0.02748
Epoch: 0151 train_loss= 627.12579 feature_loss= 5.03702 structure_loss= 622.08875 accuracy= 0.69350 accuracy_s= 0.78065 accuracy_f= 0.83973 time= 0.02714
Epoch: 0161 train_loss= 603.84717 feature_loss= 5.02974 structure_loss= 598.81744 accuracy= 0.69129 accuracy_s= 0.78213 accuracy_f= 0.83973 time= 0.02698
Epoch: 0171 train_loss= 582.68872 feature_loss= 5.02238 structure_loss= 577.66632 accuracy= 0.69202 accuracy_s= 0.77917 accuracy_f= 0.84121 time= 0.02721
Epoch: 0181 train_loss= 563.42999 feature_loss= 5.01640 structure_loss= 558.41357 accuracy= 0.69276 accuracy_s= 0.78139 accuracy_f= 0.84047 time= 0.02708
Epoch: 0191 train_loss= 543.68359 feature_loss= 5.00832 structure_loss= 538.67529 accuracy= 0.68759 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.02727
Epoch: 0201 train_loss= 517.25867 feature_loss= 5.00345 structure_loss= 512.25519 accuracy= 0.69572 accuracy_s= 0.77917 accuracy_f= 0.84047 time= 0.02691
Epoch: 0211 train_loss= 499.25046 feature_loss= 4.99465 structure_loss= 494.25580 accuracy= 0.69276 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.02697
Epoch: 0221 train_loss= 477.56183 feature_loss= 4.98692 structure_loss= 472.57492 accuracy= 0.68907 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.02711
Epoch: 0231 train_loss= 457.24020 feature_loss= 4.97895 structure_loss= 452.26126 accuracy= 0.68833 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.02703
Epoch: 0241 train_loss= 444.47879 feature_loss= 4.97095 structure_loss= 439.50784 accuracy= 0.68759 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.02751
Epoch: 0251 train_loss= 426.18521 feature_loss= 4.96294 structure_loss= 421.22226 accuracy= 0.68685 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.02700
Epoch: 0261 train_loss= 411.75613 feature_loss= 4.95506 structure_loss= 406.80109 accuracy= 0.68981 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.02797
Epoch: 0271 train_loss= 390.42203 feature_loss= 4.94839 structure_loss= 385.47363 accuracy= 0.68759 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.02707
Epoch: 0281 train_loss= 376.16940 feature_loss= 4.93797 structure_loss= 371.23141 accuracy= 0.68981 accuracy_s= 0.77843 accuracy_f= 0.84269 time= 0.02701
Epoch: 0291 train_loss= 359.98688 feature_loss= 4.92886 structure_loss= 355.05801 accuracy= 0.68685 accuracy_s= 0.77843 accuracy_f= 0.84269 time= 0.02738

accuracy 0.68833
accuracy_s 0.77843
accuracy_f 0.84269
auc 0.57699
f1_score 0.29667
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=1, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 295
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([2708, 358]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 4121.14062 accuracy= 0.72969 time= 0.02167
Epoch: 0011 train_loss= 2587.20020 accuracy= 0.62777 time= 0.01264
Epoch: 0021 train_loss= 2182.81836 accuracy= 0.60709 time= 0.01264
Epoch: 0031 train_loss= 1938.04761 accuracy= 0.60857 time= 0.01081
Epoch: 0041 train_loss= 1761.37512 accuracy= 0.60561 time= 0.01087
Epoch: 0051 train_loss= 1623.29358 accuracy= 0.60709 time= 0.01080
Epoch: 0061 train_loss= 1509.60461 accuracy= 0.61004 time= 0.01079
Epoch: 0071 train_loss= 1410.05884 accuracy= 0.61300 time= 0.01083
Epoch: 0081 train_loss= 1324.02734 accuracy= 0.61152 time= 0.01078
Epoch: 0091 train_loss= 1249.32092 accuracy= 0.61300 time= 0.01083
Epoch: 0101 train_loss= 1181.85986 accuracy= 0.61448 time= 0.01077
Epoch: 0111 train_loss= 1118.08264 accuracy= 0.61595 time= 0.01074
Epoch: 0121 train_loss= 1061.75281 accuracy= 0.61521 time= 0.01078
Epoch: 0131 train_loss= 1010.58051 accuracy= 0.61595 time= 0.01080
Epoch: 0141 train_loss= 963.74908 accuracy= 0.61374 time= 0.01077
Epoch: 0151 train_loss= 917.38586 accuracy= 0.61448 time= 0.01078
Epoch: 0161 train_loss= 874.52777 accuracy= 0.61595 time= 0.01080
Epoch: 0171 train_loss= 833.09680 accuracy= 0.61521 time= 0.01077
Epoch: 0181 train_loss= 797.20496 accuracy= 0.61595 time= 0.01078
Epoch: 0191 train_loss= 762.92975 accuracy= 0.61669 time= 0.01077
Epoch: 0201 train_loss= 729.95825 accuracy= 0.61669 time= 0.01078
Epoch: 0211 train_loss= 697.46637 accuracy= 0.61669 time= 0.01079
Epoch: 0221 train_loss= 668.89014 accuracy= 0.61595 time= 0.01078
Epoch: 0231 train_loss= 638.96844 accuracy= 0.61669 time= 0.01081
Epoch: 0241 train_loss= 610.88678 accuracy= 0.61448 time= 0.01084
Epoch: 0251 train_loss= 585.58453 accuracy= 0.61521 time= 0.01081
Epoch: 0261 train_loss= 559.45209 accuracy= 0.61374 time= 0.01079
Epoch: 0271 train_loss= 536.84863 accuracy= 0.61448 time= 0.01161
Epoch: 0281 train_loss= 511.81683 accuracy= 0.61374 time= 0.01076
Epoch: 0291 train_loss= 489.51413 accuracy= 0.61374 time= 0.01077

accuracy 0.61300
auc 0.30885
f1_score 0.12667
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=0, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 230
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([2708, 358]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 37.07145 accuracy= 0.65879 time= 0.02448
Epoch: 0011 train_loss= 17.97222 accuracy= 0.66617 time= 0.01639
Epoch: 0021 train_loss= 12.37568 accuracy= 0.69424 time= 0.01369
Epoch: 0031 train_loss= 9.90328 accuracy= 0.71787 time= 0.01359
Epoch: 0041 train_loss= 8.83617 accuracy= 0.72969 time= 0.01416
Epoch: 0051 train_loss= 8.13000 accuracy= 0.73855 time= 0.01353
Epoch: 0061 train_loss= 7.61252 accuracy= 0.75037 time= 0.01355
Epoch: 0071 train_loss= 7.14326 accuracy= 0.75185 time= 0.01358
Epoch: 0081 train_loss= 6.69418 accuracy= 0.75923 time= 0.01357
Epoch: 0091 train_loss= 6.14393 accuracy= 0.76071 time= 0.01357
Epoch: 0101 train_loss= 5.55542 accuracy= 0.75997 time= 0.01366
Epoch: 0111 train_loss= 5.12635 accuracy= 0.76219 time= 0.01357
Epoch: 0121 train_loss= 4.80506 accuracy= 0.75628 time= 0.01357
Epoch: 0131 train_loss= 4.65442 accuracy= 0.76219 time= 0.01354
Epoch: 0141 train_loss= 4.58275 accuracy= 0.76071 time= 0.01368
Epoch: 0151 train_loss= 4.50878 accuracy= 0.76809 time= 0.01366
Epoch: 0161 train_loss= 4.46366 accuracy= 0.76219 time= 0.01364
Epoch: 0171 train_loss= 4.44347 accuracy= 0.76514 time= 0.01362
Epoch: 0181 train_loss= 4.42089 accuracy= 0.76071 time= 0.01358
Epoch: 0191 train_loss= 4.39402 accuracy= 0.77105 time= 0.01360
Epoch: 0201 train_loss= 4.37297 accuracy= 0.76588 time= 0.01362
Epoch: 0211 train_loss= 4.35740 accuracy= 0.76588 time= 0.01363
Epoch: 0221 train_loss= 4.34337 accuracy= 0.76662 time= 0.01357
Epoch: 0231 train_loss= 4.33590 accuracy= 0.75997 time= 0.01364
Epoch: 0241 train_loss= 4.31970 accuracy= 0.76809 time= 0.01360
Epoch: 0251 train_loss= 4.31291 accuracy= 0.77253 time= 0.01358
Epoch: 0261 train_loss= 4.31498 accuracy= 0.76809 time= 0.01361
Epoch: 0271 train_loss= 4.29960 accuracy= 0.76662 time= 0.01358
Epoch: 0281 train_loss= 4.30692 accuracy= 0.76219 time= 0.01359
Epoch: 0291 train_loss= 4.27470 accuracy= 0.76366 time= 0.01354

accuracy 0.76662
auc 0.73096
f1_score 0.47333
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=1, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 858
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 46 steps!

y_features shape after FMS torch.Size([2708, 358]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 4041.99512 accuracy= 0.73338 time= 0.02456
Epoch: 0011 train_loss= 2521.87769 accuracy= 0.61300 time= 0.01257
Epoch: 0021 train_loss= 2159.69092 accuracy= 0.60931 time= 0.01147
Epoch: 0031 train_loss= 1933.63074 accuracy= 0.61300 time= 0.01144
Epoch: 0041 train_loss= 1769.68958 accuracy= 0.61374 time= 0.01068
Epoch: 0051 train_loss= 1636.50403 accuracy= 0.60857 time= 0.01070
Epoch: 0061 train_loss= 1528.63684 accuracy= 0.60783 time= 0.01071
Epoch: 0071 train_loss= 1433.75500 accuracy= 0.60709 time= 0.01069
Epoch: 0081 train_loss= 1351.17542 accuracy= 0.60783 time= 0.01075
Epoch: 0091 train_loss= 1274.17798 accuracy= 0.60635 time= 0.01072
Epoch: 0101 train_loss= 1207.06311 accuracy= 0.60635 time= 0.01076
Epoch: 0111 train_loss= 1143.76917 accuracy= 0.60561 time= 0.01071
Epoch: 0121 train_loss= 1083.76941 accuracy= 0.60340 time= 0.01069
Epoch: 0131 train_loss= 1028.86670 accuracy= 0.60414 time= 0.01075
Epoch: 0141 train_loss= 978.80798 accuracy= 0.60414 time= 0.01070
Epoch: 0151 train_loss= 929.38257 accuracy= 0.60561 time= 0.01073
Epoch: 0161 train_loss= 885.07220 accuracy= 0.60487 time= 0.01075
Epoch: 0171 train_loss= 841.43573 accuracy= 0.60709 time= 0.01074
Epoch: 0181 train_loss= 801.92902 accuracy= 0.60857 time= 0.01072
Epoch: 0191 train_loss= 763.46118 accuracy= 0.60783 time= 0.01069
Epoch: 0201 train_loss= 727.22858 accuracy= 0.60931 time= 0.01077
Epoch: 0211 train_loss= 692.18774 accuracy= 0.60857 time= 0.01078
Epoch: 0221 train_loss= 658.70392 accuracy= 0.60857 time= 0.01068
Epoch: 0231 train_loss= 628.44922 accuracy= 0.60783 time= 0.01073
Epoch: 0241 train_loss= 597.86908 accuracy= 0.60635 time= 0.01074
Epoch: 0251 train_loss= 570.55762 accuracy= 0.60857 time= 0.01072
Epoch: 0261 train_loss= 543.09515 accuracy= 0.60931 time= 0.01073
Epoch: 0271 train_loss= 516.29401 accuracy= 0.60709 time= 0.01074
Epoch: 0281 train_loss= 490.44064 accuracy= 0.60635 time= 0.01074
Epoch: 0291 train_loss= 465.01242 accuracy= 0.60931 time= 0.01072

accuracy 0.60709
auc 0.30175
f1_score 0.11333
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=0, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 150
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 61 steps!

y_features shape after FMS torch.Size([2708, 358]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 37.14780 accuracy= 0.67430 time= 0.02371
Epoch: 0011 train_loss= 18.01789 accuracy= 0.67356 time= 0.01626
Epoch: 0021 train_loss= 11.64947 accuracy= 0.69719 time= 0.01386
Epoch: 0031 train_loss= 9.08016 accuracy= 0.72600 time= 0.01369
Epoch: 0041 train_loss= 7.84538 accuracy= 0.73117 time= 0.01345
Epoch: 0051 train_loss= 7.42238 accuracy= 0.74520 time= 0.01343
Epoch: 0061 train_loss= 7.18457 accuracy= 0.75037 time= 0.01348
Epoch: 0071 train_loss= 7.04116 accuracy= 0.75554 time= 0.01340
Epoch: 0081 train_loss= 6.89250 accuracy= 0.75702 time= 0.01349
Epoch: 0091 train_loss= 6.67185 accuracy= 0.76071 time= 0.01349
Epoch: 0101 train_loss= 6.06769 accuracy= 0.76145 time= 0.01343
Epoch: 0111 train_loss= 5.92651 accuracy= 0.76292 time= 0.01350
Epoch: 0121 train_loss= 5.72694 accuracy= 0.75923 time= 0.01347
Epoch: 0131 train_loss= 5.64119 accuracy= 0.74372 time= 0.01352
Epoch: 0141 train_loss= 5.84010 accuracy= 0.72600 time= 0.01350
Epoch: 0151 train_loss= 5.69597 accuracy= 0.72747 time= 0.01343
Epoch: 0161 train_loss= 5.32294 accuracy= 0.72600 time= 0.01344
Epoch: 0171 train_loss= 5.00999 accuracy= 0.72969 time= 0.01346
Epoch: 0181 train_loss= 4.79009 accuracy= 0.72895 time= 0.01346
Epoch: 0191 train_loss= 4.51040 accuracy= 0.72304 time= 0.01343
Epoch: 0201 train_loss= 4.33840 accuracy= 0.72083 time= 0.01345
Epoch: 0211 train_loss= 4.27120 accuracy= 0.71935 time= 0.01344
Epoch: 0221 train_loss= 4.22914 accuracy= 0.71935 time= 0.01344
Epoch: 0231 train_loss= 4.19524 accuracy= 0.71640 time= 0.01346
Epoch: 0241 train_loss= 4.17236 accuracy= 0.72009 time= 0.01346
Epoch: 0251 train_loss= 4.15939 accuracy= 0.72009 time= 0.01342
Epoch: 0261 train_loss= 4.13539 accuracy= 0.71861 time= 0.01343
Epoch: 0271 train_loss= 4.12606 accuracy= 0.71566 time= 0.01347
Epoch: 0281 train_loss= 4.11341 accuracy= 0.71418 time= 0.01344
Epoch: 0291 train_loss= 4.10645 accuracy= 0.71492 time= 0.01347

accuracy 0.71640
auc 0.64402
f1_score 0.36000
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=1, device=device(type='cuda'), dim_reduce=2, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 334
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>

y_features shape after NoReduction torch.Size([2708, 18629]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 3722.40356 accuracy= 0.61448 time= 0.03702
Epoch: 0011 train_loss= 3385.67383 accuracy= 0.64476 time= 0.02405
Epoch: 0021 train_loss= 2016.48352 accuracy= 0.64623 time= 0.02331
Epoch: 0031 train_loss= 1193.71643 accuracy= 0.63442 time= 0.02361
Epoch: 0041 train_loss= 724.69208 accuracy= 0.63072 time= 0.02360
Epoch: 0051 train_loss= 440.03610 accuracy= 0.62260 time= 0.02351
Epoch: 0061 train_loss= 293.54810 accuracy= 0.61078 time= 0.02387
Epoch: 0071 train_loss= 201.71791 accuracy= 0.60783 time= 0.02337
Epoch: 0081 train_loss= 160.43684 accuracy= 0.60709 time= 0.02350
Epoch: 0091 train_loss= 136.37569 accuracy= 0.61152 time= 0.02381
Epoch: 0101 train_loss= 109.67577 accuracy= 0.61817 time= 0.02330
Epoch: 0111 train_loss= 90.78653 accuracy= 0.61743 time= 0.02347
Epoch: 0121 train_loss= 70.53836 accuracy= 0.62999 time= 0.02378
Epoch: 0131 train_loss= 50.16676 accuracy= 0.61595 time= 0.02335
Epoch: 0141 train_loss= 43.03097 accuracy= 0.61817 time= 0.02342
Epoch: 0151 train_loss= 39.88381 accuracy= 0.61448 time= 0.02383
Epoch: 0161 train_loss= 37.22277 accuracy= 0.61374 time= 0.02334
Epoch: 0171 train_loss= 32.21323 accuracy= 0.61078 time= 0.02372
Epoch: 0181 train_loss= 29.15880 accuracy= 0.61078 time= 0.02356
Epoch: 0191 train_loss= 26.05369 accuracy= 0.61078 time= 0.02345
Epoch: 0201 train_loss= 23.27869 accuracy= 0.61669 time= 0.02379
Epoch: 0211 train_loss= 21.67536 accuracy= 0.61448 time= 0.02332
Epoch: 0221 train_loss= 18.72727 accuracy= 0.61374 time= 0.02359
Epoch: 0231 train_loss= 18.33622 accuracy= 0.61226 time= 0.02363
Epoch: 0241 train_loss= 17.12758 accuracy= 0.61078 time= 0.02339
Epoch: 0251 train_loss= 13.19868 accuracy= 0.61152 time= 0.02337
Epoch: 0261 train_loss= 10.90411 accuracy= 0.61152 time= 0.02339
Epoch: 0271 train_loss= 10.75079 accuracy= 0.61152 time= 0.02366
Epoch: 0281 train_loss= 10.59141 accuracy= 0.61152 time= 0.02363
Epoch: 0291 train_loss= 10.49396 accuracy= 0.61152 time= 0.02344

accuracy 0.61152
auc 0.25528
f1_score 0.12333
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=0, device=device(type='cuda'), dim_reduce=2, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 340
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>

y_features shape after NoReduction torch.Size([2708, 18629]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 34.56482 accuracy= 0.61004 time= 0.03665
Epoch: 0011 train_loss= 7.34673 accuracy= 0.70089 time= 0.02629
Epoch: 0021 train_loss= 6.26861 accuracy= 0.70753 time= 0.02656
Epoch: 0031 train_loss= 5.91530 accuracy= 0.71196 time= 0.02652
Epoch: 0041 train_loss= 5.76462 accuracy= 0.71713 time= 0.02615
Epoch: 0051 train_loss= 5.66433 accuracy= 0.72083 time= 0.02632
Epoch: 0061 train_loss= 5.58531 accuracy= 0.72452 time= 0.02653
Epoch: 0071 train_loss= 5.52018 accuracy= 0.72747 time= 0.02699
Epoch: 0081 train_loss= 5.45519 accuracy= 0.73043 time= 0.02707
Epoch: 0091 train_loss= 5.39466 accuracy= 0.73412 time= 0.02690
Epoch: 0101 train_loss= 5.33981 accuracy= 0.72969 time= 0.02689
Epoch: 0111 train_loss= 5.29811 accuracy= 0.73412 time= 0.02668
Epoch: 0121 train_loss= 5.26535 accuracy= 0.73560 time= 0.02661
Epoch: 0131 train_loss= 5.22005 accuracy= 0.74003 time= 0.02649
Epoch: 0141 train_loss= 5.18385 accuracy= 0.74298 time= 0.02648
Epoch: 0151 train_loss= 5.14431 accuracy= 0.74520 time= 0.02642
Epoch: 0161 train_loss= 5.09260 accuracy= 0.74372 time= 0.02640
Epoch: 0171 train_loss= 5.05552 accuracy= 0.74668 time= 0.02674
Epoch: 0181 train_loss= 5.02227 accuracy= 0.75111 time= 0.02600
Epoch: 0191 train_loss= 4.98866 accuracy= 0.74815 time= 0.02599
Epoch: 0201 train_loss= 4.95209 accuracy= 0.74889 time= 0.02643
Epoch: 0211 train_loss= 4.92438 accuracy= 0.75480 time= 0.02657
Epoch: 0221 train_loss= 4.87829 accuracy= 0.74889 time= 0.02688
Epoch: 0231 train_loss= 4.86450 accuracy= 0.75111 time= 0.02692
Epoch: 0241 train_loss= 4.82304 accuracy= 0.75480 time= 0.02704
Epoch: 0251 train_loss= 4.80504 accuracy= 0.75480 time= 0.02664
Epoch: 0261 train_loss= 4.77069 accuracy= 0.75702 time= 0.02663
Epoch: 0271 train_loss= 4.74960 accuracy= 0.75702 time= 0.02639
Epoch: 0281 train_loss= 4.73297 accuracy= 0.75702 time= 0.02643
Epoch: 0291 train_loss= 4.69500 accuracy= 0.75628 time= 0.02620

accuracy 0.75406
auc 0.71527
f1_score 0.44500
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=1, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 485
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([2708, 358]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 1032.58984 accuracy= 0.69572 time= 0.03230
Epoch: 0011 train_loss= 103.01283 accuracy= 0.61669 time= 0.01822
Epoch: 0021 train_loss= 27.75189 accuracy= 0.60857 time= 0.01562
Epoch: 0031 train_loss= 4.77115 accuracy= 0.60931 time= 0.01564
Epoch: 0041 train_loss= 0.47963 accuracy= 0.61595 time= 0.01557
Epoch: 0051 train_loss= 0.44026 accuracy= 0.61595 time= 0.01557
Epoch: 0061 train_loss= 0.42969 accuracy= 0.61521 time= 0.01561
Epoch: 0071 train_loss= 0.42968 accuracy= 0.61448 time= 0.01558
Epoch: 0081 train_loss= 0.42987 accuracy= 0.61521 time= 0.01561
Epoch: 0091 train_loss= 0.42954 accuracy= 0.61521 time= 0.01554
Epoch: 0101 train_loss= 0.42885 accuracy= 0.61374 time= 0.01567
Epoch: 0111 train_loss= 0.42817 accuracy= 0.61300 time= 0.01561
Epoch: 0121 train_loss= 0.42721 accuracy= 0.61300 time= 0.01559
Epoch: 0131 train_loss= 0.42616 accuracy= 0.61374 time= 0.01557
Epoch: 0141 train_loss= 0.42471 accuracy= 0.61521 time= 0.01558
Epoch: 0151 train_loss= 0.42358 accuracy= 0.61374 time= 0.01557
Epoch: 0161 train_loss= 0.42226 accuracy= 0.61448 time= 0.01559
Epoch: 0171 train_loss= 0.42120 accuracy= 0.61448 time= 0.01555
Epoch: 0181 train_loss= 0.41940 accuracy= 0.61300 time= 0.01557
Epoch: 0191 train_loss= 0.41802 accuracy= 0.61374 time= 0.01554
Epoch: 0201 train_loss= 0.41703 accuracy= 0.61226 time= 0.01554
Epoch: 0211 train_loss= 0.41552 accuracy= 0.61374 time= 0.01563
Epoch: 0221 train_loss= 0.41408 accuracy= 0.61448 time= 0.01560
Epoch: 0231 train_loss= 0.41197 accuracy= 0.61448 time= 0.01558
Epoch: 0241 train_loss= 0.40968 accuracy= 0.61226 time= 0.01554
Epoch: 0251 train_loss= 0.40815 accuracy= 0.61374 time= 0.01555
Epoch: 0261 train_loss= 0.40612 accuracy= 0.61300 time= 0.01555
Epoch: 0271 train_loss= 0.40467 accuracy= 0.61152 time= 0.01568
Epoch: 0281 train_loss= 0.40199 accuracy= 0.61078 time= 0.01558
Epoch: 0291 train_loss= 0.40053 accuracy= 0.61004 time= 0.01558

accuracy 0.61152
auc 0.25988
f1_score 0.12333
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=0, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 303
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([2708, 358]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 36.85763 accuracy= 0.66248 time= 0.03175
Epoch: 0011 train_loss= 7.24687 accuracy= 0.69572 time= 0.01839
Epoch: 0021 train_loss= 6.20077 accuracy= 0.70015 time= 0.01837
Epoch: 0031 train_loss= 5.78879 accuracy= 0.70532 time= 0.01834
Epoch: 0041 train_loss= 5.75285 accuracy= 0.70606 time= 0.01848
Epoch: 0051 train_loss= 5.70626 accuracy= 0.70753 time= 0.01831
Epoch: 0061 train_loss= 5.68551 accuracy= 0.70532 time= 0.01837
Epoch: 0071 train_loss= 5.66851 accuracy= 0.70532 time= 0.01832
Epoch: 0081 train_loss= 5.65190 accuracy= 0.70753 time= 0.01833
Epoch: 0091 train_loss= 5.63621 accuracy= 0.70606 time= 0.01831
Epoch: 0101 train_loss= 5.62058 accuracy= 0.70679 time= 0.01833
Epoch: 0111 train_loss= 5.60417 accuracy= 0.70679 time= 0.01833
Epoch: 0121 train_loss= 5.58894 accuracy= 0.70606 time= 0.01831
Epoch: 0131 train_loss= 5.57225 accuracy= 0.70532 time= 0.01835
Epoch: 0141 train_loss= 5.55768 accuracy= 0.70458 time= 0.01831
Epoch: 0151 train_loss= 5.54134 accuracy= 0.70606 time= 0.01833
Epoch: 0161 train_loss= 5.52528 accuracy= 0.70532 time= 0.01829
Epoch: 0171 train_loss= 5.50879 accuracy= 0.70606 time= 0.01832
Epoch: 0181 train_loss= 5.49225 accuracy= 0.70532 time= 0.01829
Epoch: 0191 train_loss= 5.47534 accuracy= 0.70458 time= 0.01832
Epoch: 0201 train_loss= 5.45828 accuracy= 0.70458 time= 0.01835
Epoch: 0211 train_loss= 5.44145 accuracy= 0.70384 time= 0.01835
Epoch: 0221 train_loss= 5.42385 accuracy= 0.70458 time= 0.01833
Epoch: 0231 train_loss= 5.40623 accuracy= 0.70384 time= 0.01837
Epoch: 0241 train_loss= 5.38917 accuracy= 0.70384 time= 0.01840
Epoch: 0251 train_loss= 5.37050 accuracy= 0.70236 time= 0.01840
Epoch: 0261 train_loss= 5.35161 accuracy= 0.70458 time= 0.01842
Epoch: 0271 train_loss= 5.33133 accuracy= 0.70532 time= 0.01838
Epoch: 0281 train_loss= 5.31163 accuracy= 0.70458 time= 0.01842
Epoch: 0291 train_loss= 5.32331 accuracy= 0.69941 time= 0.01844

accuracy 0.70015
auc 0.61521
f1_score 0.32333
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=1, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 575
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 26 steps!

y_features shape after FMS torch.Size([2708, 358]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 2.79982 accuracy= 0.70310 time= 0.02801
Epoch: 0011 train_loss= 0.48150 accuracy= 0.60931 time= 0.01811
Epoch: 0021 train_loss= 0.38679 accuracy= 0.61152 time= 0.01544
Epoch: 0031 train_loss= 0.38102 accuracy= 0.61226 time= 0.01540
Epoch: 0041 train_loss= 0.37892 accuracy= 0.61595 time= 0.01546
Epoch: 0051 train_loss= 0.37701 accuracy= 0.61669 time= 0.01540
Epoch: 0061 train_loss= 0.37516 accuracy= 0.61300 time= 0.01546
Epoch: 0071 train_loss= 0.37380 accuracy= 0.61448 time= 0.01549
Epoch: 0081 train_loss= 0.37298 accuracy= 0.61374 time= 0.01544
Epoch: 0091 train_loss= 0.37255 accuracy= 0.61521 time= 0.01546
Epoch: 0101 train_loss= 0.37235 accuracy= 0.61743 time= 0.01541
Epoch: 0111 train_loss= 0.37215 accuracy= 0.61595 time= 0.01545
Epoch: 0121 train_loss= 0.37197 accuracy= 0.61595 time= 0.01551
Epoch: 0131 train_loss= 0.37232 accuracy= 0.61669 time= 0.01601
Epoch: 0141 train_loss= 0.37231 accuracy= 0.61669 time= 0.01549
Epoch: 0151 train_loss= 0.37255 accuracy= 0.61521 time= 0.01545
Epoch: 0161 train_loss= 0.37302 accuracy= 0.61669 time= 0.01544
Epoch: 0171 train_loss= 0.37314 accuracy= 0.61669 time= 0.01545
Epoch: 0181 train_loss= 0.37351 accuracy= 0.61817 time= 0.01541
Epoch: 0191 train_loss= 0.37377 accuracy= 0.61743 time= 0.01545
Epoch: 0201 train_loss= 0.37419 accuracy= 0.61669 time= 0.01549
Epoch: 0211 train_loss= 0.37437 accuracy= 0.61817 time= 0.01547
Epoch: 0221 train_loss= 0.37485 accuracy= 0.61817 time= 0.01548
Epoch: 0231 train_loss= 0.37531 accuracy= 0.61891 time= 0.01548
Epoch: 0241 train_loss= 0.37575 accuracy= 0.61817 time= 0.01546
Epoch: 0251 train_loss= 0.37639 accuracy= 0.61891 time= 0.01550
Epoch: 0261 train_loss= 0.37661 accuracy= 0.61743 time= 0.01545
Epoch: 0271 train_loss= 0.37712 accuracy= 0.61743 time= 0.01542
Epoch: 0281 train_loss= 0.37765 accuracy= 0.61743 time= 0.01544
Epoch: 0291 train_loss= 0.37811 accuracy= 0.61891 time= 0.01561

accuracy 0.61743
auc 0.26930
f1_score 0.13667
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=0, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 240
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 27 steps!

y_features shape after FMS torch.Size([2708, 358]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 35.38848 accuracy= 0.68981 time= 0.03680
Epoch: 0011 train_loss= 6.69565 accuracy= 0.70236 time= 0.02230
Epoch: 0021 train_loss= 4.75575 accuracy= 0.70606 time= 0.01854
Epoch: 0031 train_loss= 4.26980 accuracy= 0.70679 time= 0.01857
Epoch: 0041 train_loss= 4.19940 accuracy= 0.70679 time= 0.01855
Epoch: 0051 train_loss= 4.15504 accuracy= 0.70679 time= 0.01852
Epoch: 0061 train_loss= 4.14054 accuracy= 0.70753 time= 0.01858
Epoch: 0071 train_loss= 4.13671 accuracy= 0.70753 time= 0.01852
Epoch: 0081 train_loss= 4.13373 accuracy= 0.70753 time= 0.01866
Epoch: 0091 train_loss= 4.13092 accuracy= 0.70679 time= 0.01851
Epoch: 0101 train_loss= 4.12880 accuracy= 0.70532 time= 0.01852
Epoch: 0111 train_loss= 4.12718 accuracy= 0.70532 time= 0.01850
Epoch: 0121 train_loss= 4.12584 accuracy= 0.70532 time= 0.01852
Epoch: 0131 train_loss= 4.12473 accuracy= 0.70458 time= 0.01851
Epoch: 0141 train_loss= 4.12382 accuracy= 0.70532 time= 0.01849
Epoch: 0151 train_loss= 4.12306 accuracy= 0.70532 time= 0.01853
Epoch: 0161 train_loss= 4.12244 accuracy= 0.70532 time= 0.01851
Epoch: 0171 train_loss= 4.12193 accuracy= 0.70458 time= 0.01853
Epoch: 0181 train_loss= 4.12151 accuracy= 0.70458 time= 0.01848
Epoch: 0191 train_loss= 4.12116 accuracy= 0.70458 time= 0.01852
Epoch: 0201 train_loss= 4.12088 accuracy= 0.70310 time= 0.01885
Epoch: 0211 train_loss= 4.12066 accuracy= 0.70310 time= 0.01855
Epoch: 0221 train_loss= 4.12047 accuracy= 0.70236 time= 0.01855
Epoch: 0231 train_loss= 4.12032 accuracy= 0.70310 time= 0.01857
Epoch: 0241 train_loss= 4.12021 accuracy= 0.70310 time= 0.01858
Epoch: 0251 train_loss= 4.12011 accuracy= 0.70384 time= 0.01853
Epoch: 0261 train_loss= 4.12004 accuracy= 0.70310 time= 0.01854
Epoch: 0271 train_loss= 4.11998 accuracy= 0.70310 time= 0.01855
Epoch: 0281 train_loss= 4.11993 accuracy= 0.70310 time= 0.01858
Epoch: 0291 train_loss= 4.11990 accuracy= 0.70310 time= 0.01860

accuracy 0.70310
auc 0.61982
f1_score 0.33000
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=1, device=device(type='cuda'), dim_reduce=2, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 646
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>

y_features shape after NoReduction torch.Size([2708, 18629]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 0.43226 accuracy= 0.61300 time= 0.76597
Epoch: 0011 train_loss= 0.43203 accuracy= 0.61226 time= 0.74513
Epoch: 0021 train_loss= 0.43237 accuracy= 0.61226 time= 0.74867
Epoch: 0031 train_loss= 0.43202 accuracy= 0.61226 time= 0.74647
Epoch: 0041 train_loss= 0.43210 accuracy= 0.61226 time= 0.74678
Epoch: 0051 train_loss= 0.43199 accuracy= 0.61226 time= 0.74459
Epoch: 0061 train_loss= 0.43199 accuracy= 0.61226 time= 0.74418
Epoch: 0071 train_loss= 0.43200 accuracy= 0.61226 time= 0.73767
Epoch: 0081 train_loss= 0.43199 accuracy= 0.61226 time= 0.74997
Epoch: 0091 train_loss= 0.43199 accuracy= 0.61226 time= 0.75743
Epoch: 0101 train_loss= 0.43199 accuracy= 0.61226 time= 0.75583
Epoch: 0111 train_loss= 0.43199 accuracy= 0.61226 time= 0.75464
Epoch: 0121 train_loss= 0.43199 accuracy= 0.61226 time= 0.76183
Epoch: 0131 train_loss= 0.43199 accuracy= 0.61226 time= 0.74678
Epoch: 0141 train_loss= 0.43199 accuracy= 0.61226 time= 0.75263
Epoch: 0151 train_loss= 0.43199 accuracy= 0.61226 time= 0.75656
Epoch: 0161 train_loss= 0.43199 accuracy= 0.61226 time= 0.74760
Epoch: 0171 train_loss= 0.43199 accuracy= 0.61226 time= 0.75760
Epoch: 0181 train_loss= 0.43199 accuracy= 0.61226 time= 0.74522
Epoch: 0191 train_loss= 0.43199 accuracy= 0.61226 time= 0.75507
Epoch: 0201 train_loss= 0.43199 accuracy= 0.61226 time= 0.76216
Epoch: 0211 train_loss= 0.43199 accuracy= 0.61226 time= 0.74561
Epoch: 0221 train_loss= 0.43199 accuracy= 0.61226 time= 0.75801
Epoch: 0231 train_loss= 0.43199 accuracy= 0.61226 time= 0.76603
Epoch: 0241 train_loss= 0.43199 accuracy= 0.61226 time= 0.76053
Epoch: 0251 train_loss= 0.43199 accuracy= 0.61226 time= 0.76470
Epoch: 0261 train_loss= 0.43199 accuracy= 0.61226 time= 0.76179
Epoch: 0271 train_loss= 0.43199 accuracy= 0.61226 time= 0.76484
Epoch: 0281 train_loss= 0.43199 accuracy= 0.61226 time= 0.76099
Epoch: 0291 train_loss= 0.43199 accuracy= 0.61226 time= 0.74474

accuracy 0.61226
auc 0.25584
f1_score 0.12500
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=0, device=device(type='cuda'), dim_reduce=2, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 196
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>

y_features shape after NoReduction torch.Size([2708, 18629]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 4.57765 accuracy= 0.66987 time= 0.76080
Epoch: 0011 train_loss= 4.19001 accuracy= 0.70679 time= 0.71143
Epoch: 0021 train_loss= 4.16999 accuracy= 0.70827 time= 0.64892
Epoch: 0031 train_loss= 4.17938 accuracy= 0.70679 time= 0.76117
Epoch: 0041 train_loss= 4.19081 accuracy= 0.70310 time= 0.76475
Epoch: 0051 train_loss= 4.15169 accuracy= 0.70679 time= 0.75157
Epoch: 0061 train_loss= 4.15727 accuracy= 0.70532 time= 0.74906
Epoch: 0071 train_loss= 4.13696 accuracy= 0.70753 time= 0.74834
Epoch: 0081 train_loss= 4.19770 accuracy= 0.69867 time= 0.75362
Epoch: 0091 train_loss= 4.13159 accuracy= 0.70753 time= 0.74833
Epoch: 0101 train_loss= 4.14329 accuracy= 0.70310 time= 0.75558
Epoch: 0111 train_loss= 4.13469 accuracy= 0.70458 time= 0.75292
Epoch: 0121 train_loss= 4.12989 accuracy= 0.70532 time= 0.75927
Epoch: 0131 train_loss= 4.12560 accuracy= 0.70532 time= 0.75014
Epoch: 0141 train_loss= 4.12619 accuracy= 0.70532 time= 0.75449
Epoch: 0151 train_loss= 4.12356 accuracy= 0.70606 time= 0.75031
Epoch: 0161 train_loss= 4.12288 accuracy= 0.70532 time= 0.74997
Epoch: 0171 train_loss= 4.12343 accuracy= 0.70606 time= 0.74707
Epoch: 0181 train_loss= 4.12561 accuracy= 0.70310 time= 0.74277
Epoch: 0191 train_loss= 4.12178 accuracy= 0.70532 time= 0.75642
Epoch: 0201 train_loss= 4.12114 accuracy= 0.70384 time= 0.76120
Epoch: 0211 train_loss= 4.12114 accuracy= 0.70384 time= 0.75310
Epoch: 0221 train_loss= 4.12067 accuracy= 0.70310 time= 0.76224
Epoch: 0231 train_loss= 4.12049 accuracy= 0.70236 time= 0.75897
Epoch: 0241 train_loss= 4.12048 accuracy= 0.70236 time= 0.77475
Epoch: 0251 train_loss= 4.12026 accuracy= 0.70384 time= 0.75796
Epoch: 0261 train_loss= 4.12013 accuracy= 0.70310 time= 0.75671
Epoch: 0271 train_loss= 4.12006 accuracy= 0.70236 time= 0.75232
Epoch: 0281 train_loss= 4.12000 accuracy= 0.70310 time= 0.72800
Epoch: 0291 train_loss= 4.11995 accuracy= 0.70310 time= 0.75408

accuracy 0.70236
auc 0.61981
f1_score 0.32833
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=1, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 562
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([2708, 358]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 671.43066 accuracy= 0.66617 time= 0.02378
Epoch: 0011 train_loss= 81.09113 accuracy= 0.60931 time= 0.01397
Epoch: 0021 train_loss= 27.79034 accuracy= 0.60340 time= 0.01232
Epoch: 0031 train_loss= 8.53890 accuracy= 0.61448 time= 0.01223
Epoch: 0041 train_loss= 2.19522 accuracy= 0.60931 time= 0.01228
Epoch: 0051 train_loss= 0.56871 accuracy= 0.60783 time= 0.01223
Epoch: 0061 train_loss= 0.38352 accuracy= 0.61743 time= 0.01218
Epoch: 0071 train_loss= 0.37165 accuracy= 0.61521 time= 0.01276
Epoch: 0081 train_loss= 0.36447 accuracy= 0.61300 time= 0.01217
Epoch: 0091 train_loss= 0.35831 accuracy= 0.61004 time= 0.01220
Epoch: 0101 train_loss= 0.35393 accuracy= 0.61226 time= 0.01235
Epoch: 0111 train_loss= 0.35035 accuracy= 0.61226 time= 0.01230
Epoch: 0121 train_loss= 0.34690 accuracy= 0.60931 time= 0.01222
Epoch: 0131 train_loss= 0.34385 accuracy= 0.61595 time= 0.01242
Epoch: 0141 train_loss= 0.34114 accuracy= 0.61521 time= 0.01218
Epoch: 0151 train_loss= 0.33790 accuracy= 0.61669 time= 0.01227
Epoch: 0161 train_loss= 0.33480 accuracy= 0.61669 time= 0.01226
Epoch: 0171 train_loss= 0.33165 accuracy= 0.61521 time= 0.01229
Epoch: 0181 train_loss= 0.32916 accuracy= 0.61669 time= 0.01222
Epoch: 0191 train_loss= 0.32656 accuracy= 0.61669 time= 0.01218
Epoch: 0201 train_loss= 0.32310 accuracy= 0.61448 time= 0.01229
Epoch: 0211 train_loss= 0.32092 accuracy= 0.61595 time= 0.01229
Epoch: 0221 train_loss= 0.31742 accuracy= 0.61448 time= 0.01219
Epoch: 0231 train_loss= 0.31438 accuracy= 0.61595 time= 0.01228
Epoch: 0241 train_loss= 0.31120 accuracy= 0.61152 time= 0.01226
Epoch: 0251 train_loss= 0.30816 accuracy= 0.61448 time= 0.01229
Epoch: 0261 train_loss= 0.30597 accuracy= 0.61817 time= 0.01228
Epoch: 0271 train_loss= 0.30365 accuracy= 0.61521 time= 0.01234
Epoch: 0281 train_loss= 0.29934 accuracy= 0.61226 time= 0.01233
Epoch: 0291 train_loss= 0.29707 accuracy= 0.61669 time= 0.01230

accuracy 0.61817
auc 0.28184
f1_score 0.13833
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=0, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 828
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([2708, 358]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 37.07491 accuracy= 0.67356 time= 0.02573
Epoch: 0011 train_loss= 6.44524 accuracy= 0.69276 time= 0.01538
Epoch: 0021 train_loss= 5.93939 accuracy= 0.70162 time= 0.01511
Epoch: 0031 train_loss= 5.81700 accuracy= 0.70606 time= 0.01513
Epoch: 0041 train_loss= 5.75326 accuracy= 0.70901 time= 0.01516
Epoch: 0051 train_loss= 5.72282 accuracy= 0.71123 time= 0.01511
Epoch: 0061 train_loss= 5.69992 accuracy= 0.71492 time= 0.01513
Epoch: 0071 train_loss= 5.66574 accuracy= 0.70901 time= 0.01513
Epoch: 0081 train_loss= 5.64998 accuracy= 0.70827 time= 0.01513
Epoch: 0091 train_loss= 5.63122 accuracy= 0.70901 time= 0.01512
Epoch: 0101 train_loss= 5.60969 accuracy= 0.71123 time= 0.01511
Epoch: 0111 train_loss= 5.58820 accuracy= 0.71418 time= 0.01513
Epoch: 0121 train_loss= 5.56824 accuracy= 0.71418 time= 0.01509
Epoch: 0131 train_loss= 5.55045 accuracy= 0.71566 time= 0.01520
Epoch: 0141 train_loss= 5.54323 accuracy= 0.71196 time= 0.01508
Epoch: 0151 train_loss= 5.52817 accuracy= 0.71049 time= 0.01519
Epoch: 0161 train_loss= 5.51360 accuracy= 0.70753 time= 0.01511
Epoch: 0171 train_loss= 5.49353 accuracy= 0.70901 time= 0.01510
Epoch: 0181 train_loss= 5.47677 accuracy= 0.70753 time= 0.01513
Epoch: 0191 train_loss= 5.45549 accuracy= 0.71123 time= 0.01512
Epoch: 0201 train_loss= 5.45973 accuracy= 0.70827 time= 0.01572
Epoch: 0211 train_loss= 5.43882 accuracy= 0.70532 time= 0.01514
Epoch: 0221 train_loss= 5.41716 accuracy= 0.70532 time= 0.01510
Epoch: 0231 train_loss= 5.39757 accuracy= 0.70606 time= 0.01511
Epoch: 0241 train_loss= 5.37660 accuracy= 0.71123 time= 0.01512
Epoch: 0251 train_loss= 5.35712 accuracy= 0.71123 time= 0.01506
Epoch: 0261 train_loss= 5.33816 accuracy= 0.71344 time= 0.01518
Epoch: 0271 train_loss= 5.31629 accuracy= 0.71492 time= 0.01509
Epoch: 0281 train_loss= 5.29804 accuracy= 0.71418 time= 0.01535
Epoch: 0291 train_loss= 5.27544 accuracy= 0.71418 time= 0.01520

accuracy 0.71566
auc 0.64064
f1_score 0.35833
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=1, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 997
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 27 steps!

y_features shape after FMS torch.Size([2708, 358]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 1.48596 accuracy= 0.71713 time= 0.02027
Epoch: 0011 train_loss= 0.42474 accuracy= 0.60931 time= 0.01389
Epoch: 0021 train_loss= 0.37438 accuracy= 0.61521 time= 0.01199
Epoch: 0031 train_loss= 0.36995 accuracy= 0.61743 time= 0.01202
Epoch: 0041 train_loss= 0.36637 accuracy= 0.61669 time= 0.01199
Epoch: 0051 train_loss= 0.36292 accuracy= 0.61595 time= 0.01198
Epoch: 0061 train_loss= 0.36011 accuracy= 0.61743 time= 0.01201
Epoch: 0071 train_loss= 0.35807 accuracy= 0.61817 time= 0.01202
Epoch: 0081 train_loss= 0.35671 accuracy= 0.62038 time= 0.01201
Epoch: 0091 train_loss= 0.35564 accuracy= 0.61965 time= 0.01203
Epoch: 0101 train_loss= 0.35481 accuracy= 0.61965 time= 0.01201
Epoch: 0111 train_loss= 0.35430 accuracy= 0.62112 time= 0.01201
Epoch: 0121 train_loss= 0.35370 accuracy= 0.61965 time= 0.01204
Epoch: 0131 train_loss= 0.35342 accuracy= 0.61965 time= 0.01204
Epoch: 0141 train_loss= 0.35320 accuracy= 0.62038 time= 0.01202
Epoch: 0151 train_loss= 0.35309 accuracy= 0.61965 time= 0.01201
Epoch: 0161 train_loss= 0.35297 accuracy= 0.61965 time= 0.01201
Epoch: 0171 train_loss= 0.35295 accuracy= 0.62186 time= 0.01204
Epoch: 0181 train_loss= 0.35323 accuracy= 0.61965 time= 0.01202
Epoch: 0191 train_loss= 0.35316 accuracy= 0.61891 time= 0.01199
Epoch: 0201 train_loss= 0.35353 accuracy= 0.61891 time= 0.01203
Epoch: 0211 train_loss= 0.35376 accuracy= 0.61965 time= 0.01203
Epoch: 0221 train_loss= 0.35407 accuracy= 0.61891 time= 0.01199
Epoch: 0231 train_loss= 0.35449 accuracy= 0.62038 time= 0.01203
Epoch: 0241 train_loss= 0.35470 accuracy= 0.61965 time= 0.01200
Epoch: 0251 train_loss= 0.35513 accuracy= 0.61965 time= 0.01206
Epoch: 0261 train_loss= 0.35532 accuracy= 0.61965 time= 0.01203
Epoch: 0271 train_loss= 0.35593 accuracy= 0.61965 time= 0.01199
Epoch: 0281 train_loss= 0.35647 accuracy= 0.61891 time= 0.01200
Epoch: 0291 train_loss= 0.35686 accuracy= 0.61817 time= 0.01197

accuracy 0.61817
auc 0.27089
f1_score 0.13833
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=0, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 985
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 30 steps!

y_features shape after FMS torch.Size([2708, 358]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 34.44408 accuracy= 0.68538 time= 0.02450
Epoch: 0011 train_loss= 4.20175 accuracy= 0.70753 time= 0.01770
Epoch: 0021 train_loss= 4.17423 accuracy= 0.70753 time= 0.01486
Epoch: 0031 train_loss= 4.15861 accuracy= 0.70753 time= 0.01475
Epoch: 0041 train_loss= 4.15002 accuracy= 0.70679 time= 0.01475
Epoch: 0051 train_loss= 4.14402 accuracy= 0.70679 time= 0.01473
Epoch: 0061 train_loss= 4.13943 accuracy= 0.70753 time= 0.01475
Epoch: 0071 train_loss= 4.13584 accuracy= 0.70753 time= 0.01474
Epoch: 0081 train_loss= 4.13296 accuracy= 0.70679 time= 0.01470
Epoch: 0091 train_loss= 4.13062 accuracy= 0.70679 time= 0.01475
Epoch: 0101 train_loss= 4.12870 accuracy= 0.70532 time= 0.01476
Epoch: 0111 train_loss= 4.12712 accuracy= 0.70532 time= 0.01479
Epoch: 0121 train_loss= 4.12581 accuracy= 0.70532 time= 0.01474
Epoch: 0131 train_loss= 4.12473 accuracy= 0.70458 time= 0.01472
Epoch: 0141 train_loss= 4.12383 accuracy= 0.70606 time= 0.01471
Epoch: 0151 train_loss= 4.12309 accuracy= 0.70532 time= 0.01473
Epoch: 0161 train_loss= 4.12247 accuracy= 0.70458 time= 0.01476
Epoch: 0171 train_loss= 4.12196 accuracy= 0.70532 time= 0.01475
Epoch: 0181 train_loss= 4.12154 accuracy= 0.70458 time= 0.01473
Epoch: 0191 train_loss= 4.12120 accuracy= 0.70458 time= 0.01470
Epoch: 0201 train_loss= 4.12091 accuracy= 0.70384 time= 0.01480
Epoch: 0211 train_loss= 4.12068 accuracy= 0.70310 time= 0.01476
Epoch: 0221 train_loss= 4.12050 accuracy= 0.70236 time= 0.01470
Epoch: 0231 train_loss= 4.12035 accuracy= 0.70236 time= 0.01472
Epoch: 0241 train_loss= 4.12023 accuracy= 0.70310 time= 0.01472
Epoch: 0251 train_loss= 4.12013 accuracy= 0.70310 time= 0.01473
Epoch: 0261 train_loss= 4.12005 accuracy= 0.70236 time= 0.01476
Epoch: 0271 train_loss= 4.11999 accuracy= 0.70310 time= 0.01472
Epoch: 0281 train_loss= 4.11994 accuracy= 0.70310 time= 0.01472
Epoch: 0291 train_loss= 4.11991 accuracy= 0.70236 time= 0.01472

accuracy 0.70310
auc 0.61983
f1_score 0.33000
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=1, device=device(type='cuda'), dim_reduce=2, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 700
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>

y_features shape after NoReduction torch.Size([2708, 18629]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 0.43230 accuracy= 0.61300 time= 0.08655
Epoch: 0011 train_loss= 0.43208 accuracy= 0.61226 time= 0.06926
Epoch: 0021 train_loss= 0.43237 accuracy= 0.61300 time= 0.06924
Epoch: 0031 train_loss= 0.43199 accuracy= 0.61226 time= 0.06954
Epoch: 0041 train_loss= 0.43201 accuracy= 0.61152 time= 0.06943
Epoch: 0051 train_loss= 0.43194 accuracy= 0.61226 time= 0.07038
Epoch: 0061 train_loss= 0.43194 accuracy= 0.61226 time= 0.06974
Epoch: 0071 train_loss= 0.43194 accuracy= 0.61226 time= 0.06927
Epoch: 0081 train_loss= 0.43194 accuracy= 0.61226 time= 0.06981
Epoch: 0091 train_loss= 0.43194 accuracy= 0.61226 time= 0.06930
Epoch: 0101 train_loss= 0.43194 accuracy= 0.61226 time= 0.06938
Epoch: 0111 train_loss= 0.43194 accuracy= 0.61226 time= 0.06981
Epoch: 0121 train_loss= 0.43194 accuracy= 0.61226 time= 0.06957
Epoch: 0131 train_loss= 0.43194 accuracy= 0.61226 time= 0.06987
Epoch: 0141 train_loss= 0.43194 accuracy= 0.61226 time= 0.06917
Epoch: 0151 train_loss= 0.43194 accuracy= 0.61226 time= 0.06988
Epoch: 0161 train_loss= 0.43194 accuracy= 0.61226 time= 0.06968
Epoch: 0171 train_loss= 0.43195 accuracy= 0.61226 time= 0.06959
Epoch: 0181 train_loss= 0.43195 accuracy= 0.61226 time= 0.06976
Epoch: 0191 train_loss= 0.43195 accuracy= 0.61226 time= 0.06977
Epoch: 0201 train_loss= 0.43195 accuracy= 0.61226 time= 0.06983
Epoch: 0211 train_loss= 0.43195 accuracy= 0.61226 time= 0.06974
Epoch: 0221 train_loss= 0.43195 accuracy= 0.61226 time= 0.07274
Epoch: 0231 train_loss= 0.43195 accuracy= 0.61226 time= 0.06988
Epoch: 0241 train_loss= 0.43196 accuracy= 0.61226 time= 0.07023
Epoch: 0251 train_loss= 0.43196 accuracy= 0.61226 time= 0.06985
Epoch: 0261 train_loss= 0.43196 accuracy= 0.61226 time= 0.06997
Epoch: 0271 train_loss= 0.43196 accuracy= 0.61226 time= 0.07032
Epoch: 0281 train_loss= 0.43196 accuracy= 0.61226 time= 0.06970
Epoch: 0291 train_loss= 0.43196 accuracy= 0.61226 time= 0.07044

accuracy 0.61226
auc 0.25589
f1_score 0.12500
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=0, device=device(type='cuda'), dim_reduce=2, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 895
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>

y_features shape after NoReduction torch.Size([2708, 18629]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 4.44910 accuracy= 0.68833 time= 0.08861
Epoch: 0011 train_loss= 4.19003 accuracy= 0.70679 time= 0.07110
Epoch: 0021 train_loss= 4.17009 accuracy= 0.70827 time= 0.07109
Epoch: 0031 train_loss= 4.23603 accuracy= 0.70236 time= 0.07099
Epoch: 0041 train_loss= 4.19358 accuracy= 0.70384 time= 0.07244
Epoch: 0051 train_loss= 4.16822 accuracy= 0.70384 time= 0.07243
Epoch: 0061 train_loss= 4.14589 accuracy= 0.70679 time= 0.07347
Epoch: 0071 train_loss= 4.15082 accuracy= 0.70458 time= 0.07214
Epoch: 0081 train_loss= 4.15674 accuracy= 0.70310 time= 0.07276
Epoch: 0091 train_loss= 4.13589 accuracy= 0.70458 time= 0.07191
Epoch: 0101 train_loss= 4.14588 accuracy= 0.70384 time= 0.07248
Epoch: 0111 train_loss= 4.12791 accuracy= 0.70532 time= 0.07266
Epoch: 0121 train_loss= 4.12953 accuracy= 0.70384 time= 0.07202
Epoch: 0131 train_loss= 4.12541 accuracy= 0.70532 time= 0.07250
Epoch: 0141 train_loss= 4.12466 accuracy= 0.70458 time= 0.07220
Epoch: 0151 train_loss= 4.12356 accuracy= 0.70606 time= 0.07280
Epoch: 0161 train_loss= 4.12347 accuracy= 0.70532 time= 0.07231
Epoch: 0171 train_loss= 4.12248 accuracy= 0.70384 time= 0.07238
Epoch: 0181 train_loss= 4.12185 accuracy= 0.70532 time= 0.07275
Epoch: 0191 train_loss= 4.12146 accuracy= 0.70532 time= 0.07245
Epoch: 0201 train_loss= 4.12114 accuracy= 0.70532 time= 0.07258
Epoch: 0211 train_loss= 4.12088 accuracy= 0.70384 time= 0.07247
Epoch: 0221 train_loss= 4.12131 accuracy= 0.70310 time= 0.07266
Epoch: 0231 train_loss= 4.12051 accuracy= 0.70310 time= 0.07290
Epoch: 0241 train_loss= 4.12034 accuracy= 0.70310 time= 0.07242
Epoch: 0251 train_loss= 4.12022 accuracy= 0.70310 time= 0.07295
Epoch: 0261 train_loss= 4.12013 accuracy= 0.70310 time= 0.07293
Epoch: 0271 train_loss= 4.12006 accuracy= 0.70236 time= 0.07299
Epoch: 0281 train_loss= 4.12000 accuracy= 0.70310 time= 0.07269
Epoch: 0291 train_loss= 4.11995 accuracy= 0.70310 time= 0.07265

accuracy 0.70310
auc 0.61947
f1_score 0.33000
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=3, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=1, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 47
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([2708, 358]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 4601.72949 accuracy= 0.66396 time= 0.02733
Epoch: 0011 train_loss= 2674.70532 accuracy= 0.61374 time= 0.01674
Epoch: 0021 train_loss= 2284.37915 accuracy= 0.61448 time= 0.01482
Epoch: 0031 train_loss= 2037.52563 accuracy= 0.61300 time= 0.01459
Epoch: 0041 train_loss= 1851.53247 accuracy= 0.61004 time= 0.01454
Epoch: 0051 train_loss= 1704.58838 accuracy= 0.60931 time= 0.01458
Epoch: 0061 train_loss= 1577.72815 accuracy= 0.61152 time= 0.01463
Epoch: 0071 train_loss= 1467.99377 accuracy= 0.61226 time= 0.01459
Epoch: 0081 train_loss= 1367.91260 accuracy= 0.61152 time= 0.01461
Epoch: 0091 train_loss= 1282.53650 accuracy= 0.61004 time= 0.01460
Epoch: 0101 train_loss= 1199.29358 accuracy= 0.61152 time= 0.01462
Epoch: 0111 train_loss= 1129.10498 accuracy= 0.61004 time= 0.01459
Epoch: 0121 train_loss= 1060.92590 accuracy= 0.61004 time= 0.01457
Epoch: 0131 train_loss= 1003.72253 accuracy= 0.60931 time= 0.01462
Epoch: 0141 train_loss= 948.35669 accuracy= 0.60783 time= 0.01480
Epoch: 0151 train_loss= 902.33429 accuracy= 0.60635 time= 0.01453
Epoch: 0161 train_loss= 856.20337 accuracy= 0.60635 time= 0.01459
Epoch: 0171 train_loss= 812.86041 accuracy= 0.60635 time= 0.01460
Epoch: 0181 train_loss= 778.24573 accuracy= 0.60783 time= 0.01464
Epoch: 0191 train_loss= 743.82410 accuracy= 0.60783 time= 0.01459
Epoch: 0201 train_loss= 707.76050 accuracy= 0.60709 time= 0.01474
Epoch: 0211 train_loss= 676.92108 accuracy= 0.60783 time= 0.01455
Epoch: 0221 train_loss= 647.04779 accuracy= 0.60709 time= 0.01462
Epoch: 0231 train_loss= 619.12433 accuracy= 0.60783 time= 0.01457
Epoch: 0241 train_loss= 595.47913 accuracy= 0.60709 time= 0.01458
Epoch: 0251 train_loss= 568.63696 accuracy= 0.60709 time= 0.01460
Epoch: 0261 train_loss= 546.45685 accuracy= 0.60857 time= 0.01466
Epoch: 0271 train_loss= 522.84802 accuracy= 0.60635 time= 0.01454
Epoch: 0281 train_loss= 501.30606 accuracy= 0.60931 time= 0.01456
Epoch: 0291 train_loss= 479.97739 accuracy= 0.60857 time= 0.01459

accuracy 0.60635
auc 0.33304
f1_score 0.11167
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=3, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=0, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 719
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([2708, 358]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 36.83816 accuracy= 0.65140 time= 0.02916
Epoch: 0011 train_loss= 13.33315 accuracy= 0.66027 time= 0.02047
Epoch: 0021 train_loss= 8.57937 accuracy= 0.67061 time= 0.01740
Epoch: 0031 train_loss= 6.81849 accuracy= 0.70310 time= 0.01746
Epoch: 0041 train_loss= 6.54399 accuracy= 0.71640 time= 0.01737
Epoch: 0051 train_loss= 6.24404 accuracy= 0.72378 time= 0.01743
Epoch: 0061 train_loss= 5.75672 accuracy= 0.73043 time= 0.01743
Epoch: 0071 train_loss= 5.60754 accuracy= 0.73634 time= 0.01741
Epoch: 0081 train_loss= 5.55748 accuracy= 0.73855 time= 0.01737
Epoch: 0091 train_loss= 5.52955 accuracy= 0.74298 time= 0.01741
Epoch: 0101 train_loss= 5.50280 accuracy= 0.74742 time= 0.01739
Epoch: 0111 train_loss= 5.48369 accuracy= 0.74594 time= 0.01742
Epoch: 0121 train_loss= 5.45834 accuracy= 0.75111 time= 0.01745
Epoch: 0131 train_loss= 5.43403 accuracy= 0.75111 time= 0.01734
Epoch: 0141 train_loss= 5.42024 accuracy= 0.75037 time= 0.01734
Epoch: 0151 train_loss= 5.39604 accuracy= 0.74742 time= 0.01740
Epoch: 0161 train_loss= 5.36915 accuracy= 0.75111 time= 0.01736
Epoch: 0171 train_loss= 5.36651 accuracy= 0.75258 time= 0.01734
Epoch: 0181 train_loss= 5.34170 accuracy= 0.75037 time= 0.01741
Epoch: 0191 train_loss= 5.33093 accuracy= 0.75554 time= 0.01739
Epoch: 0201 train_loss= 5.29113 accuracy= 0.75332 time= 0.01740
Epoch: 0211 train_loss= 5.27817 accuracy= 0.75480 time= 0.01738
Epoch: 0221 train_loss= 5.28059 accuracy= 0.75775 time= 0.01737
Epoch: 0231 train_loss= 5.26593 accuracy= 0.74963 time= 0.01738
Epoch: 0241 train_loss= 5.26505 accuracy= 0.75332 time= 0.01741
Epoch: 0251 train_loss= 5.23363 accuracy= 0.75702 time= 0.01740
Epoch: 0261 train_loss= 5.27229 accuracy= 0.75332 time= 0.01739
Epoch: 0271 train_loss= 5.20840 accuracy= 0.75628 time= 0.01739
Epoch: 0281 train_loss= 5.18520 accuracy= 0.74889 time= 0.01738
Epoch: 0291 train_loss= 5.16087 accuracy= 0.75332 time= 0.01738

accuracy 0.76292
auc 0.71145
f1_score 0.46500
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=3, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=1, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 940
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 109 steps!

y_features shape after FMS torch.Size([2708, 358]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 4482.66846 accuracy= 0.72747 time= 0.03161
Epoch: 0011 train_loss= 2720.56104 accuracy= 0.60709 time= 0.01697
Epoch: 0021 train_loss= 2331.65479 accuracy= 0.60414 time= 0.01468
Epoch: 0031 train_loss= 2076.74487 accuracy= 0.60857 time= 0.01537
Epoch: 0041 train_loss= 1891.46008 accuracy= 0.60561 time= 0.01462
Epoch: 0051 train_loss= 1736.70679 accuracy= 0.60857 time= 0.01673
Epoch: 0061 train_loss= 1608.80164 accuracy= 0.60783 time= 0.01676
Epoch: 0071 train_loss= 1495.81921 accuracy= 0.60783 time= 0.01730
Epoch: 0081 train_loss= 1394.00439 accuracy= 0.60709 time= 0.01736
Epoch: 0091 train_loss= 1302.15564 accuracy= 0.60857 time= 0.01663
Epoch: 0101 train_loss= 1222.51196 accuracy= 0.60783 time= 0.01613
Epoch: 0111 train_loss= 1147.94373 accuracy= 0.60857 time= 0.01466
Epoch: 0121 train_loss= 1082.84436 accuracy= 0.61078 time= 0.01471
Epoch: 0131 train_loss= 1022.01825 accuracy= 0.61152 time= 0.01463
Epoch: 0141 train_loss= 967.60455 accuracy= 0.61300 time= 0.01462
Epoch: 0151 train_loss= 915.34497 accuracy= 0.61374 time= 0.01456
Epoch: 0161 train_loss= 872.04205 accuracy= 0.61152 time= 0.01458
Epoch: 0171 train_loss= 831.19055 accuracy= 0.61226 time= 0.01460
Epoch: 0181 train_loss= 788.05969 accuracy= 0.61300 time= 0.01461
Epoch: 0191 train_loss= 754.54944 accuracy= 0.61152 time= 0.01460
Epoch: 0201 train_loss= 718.98566 accuracy= 0.61226 time= 0.01481
Epoch: 0211 train_loss= 685.70300 accuracy= 0.61300 time= 0.01458
Epoch: 0221 train_loss= 657.22559 accuracy= 0.61448 time= 0.01460
Epoch: 0231 train_loss= 626.48773 accuracy= 0.61448 time= 0.01460
Epoch: 0241 train_loss= 598.54492 accuracy= 0.61374 time= 0.01461
Epoch: 0251 train_loss= 575.42072 accuracy= 0.61521 time= 0.01460
Epoch: 0261 train_loss= 550.43524 accuracy= 0.61226 time= 0.01459
Epoch: 0271 train_loss= 524.35052 accuracy= 0.61078 time= 0.01464
Epoch: 0281 train_loss= 500.62415 accuracy= 0.61300 time= 0.01463
Epoch: 0291 train_loss= 479.10538 accuracy= 0.61374 time= 0.01461

accuracy 0.61152
auc 0.30157
f1_score 0.12333
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=3, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=0, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 615
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 27 steps!

y_features shape after FMS torch.Size([2708, 358]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 36.97004 accuracy= 0.68612 time= 0.03415
Epoch: 0011 train_loss= 13.38385 accuracy= 0.64845 time= 0.01914
Epoch: 0021 train_loss= 8.05595 accuracy= 0.69055 time= 0.01724
Epoch: 0031 train_loss= 6.65961 accuracy= 0.71123 time= 0.01726
Epoch: 0041 train_loss= 6.11367 accuracy= 0.71492 time= 0.01733
Epoch: 0051 train_loss= 6.05639 accuracy= 0.72304 time= 0.01733
Epoch: 0061 train_loss= 6.00334 accuracy= 0.72895 time= 0.01726
Epoch: 0071 train_loss= 5.95370 accuracy= 0.73708 time= 0.01727
Epoch: 0081 train_loss= 5.90189 accuracy= 0.73929 time= 0.01727
Epoch: 0091 train_loss= 5.83799 accuracy= 0.74372 time= 0.01728
Epoch: 0101 train_loss= 5.72894 accuracy= 0.74963 time= 0.01735
Epoch: 0111 train_loss= 5.63607 accuracy= 0.74594 time= 0.01732
Epoch: 0121 train_loss= 5.68647 accuracy= 0.72304 time= 0.01734
Epoch: 0131 train_loss= 5.66578 accuracy= 0.72821 time= 0.01732
Epoch: 0141 train_loss= 5.99039 accuracy= 0.74520 time= 0.01737
Epoch: 0151 train_loss= 5.71404 accuracy= 0.68833 time= 0.01731
Epoch: 0161 train_loss= 5.32696 accuracy= 0.69867 time= 0.01729
Epoch: 0171 train_loss= 5.09779 accuracy= 0.70606 time= 0.01730
Epoch: 0181 train_loss= 4.97635 accuracy= 0.70975 time= 0.01732
Epoch: 0191 train_loss= 4.89291 accuracy= 0.71123 time= 0.01736
Epoch: 0201 train_loss= 4.85118 accuracy= 0.70901 time= 0.01734
Epoch: 0211 train_loss= 4.78474 accuracy= 0.70901 time= 0.01731
Epoch: 0221 train_loss= 4.74304 accuracy= 0.71196 time= 0.01727
Epoch: 0231 train_loss= 4.71153 accuracy= 0.71270 time= 0.01732
Epoch: 0241 train_loss= 4.68737 accuracy= 0.71418 time= 0.01730
Epoch: 0251 train_loss= 4.65615 accuracy= 0.71418 time= 0.01731
Epoch: 0261 train_loss= 4.62904 accuracy= 0.71196 time= 0.01734
Epoch: 0271 train_loss= 4.60450 accuracy= 0.71344 time= 0.01739
Epoch: 0281 train_loss= 4.58090 accuracy= 0.71344 time= 0.01732
Epoch: 0291 train_loss= 4.56577 accuracy= 0.71344 time= 0.01733

accuracy 0.71935
auc 0.63910
f1_score 0.36667
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=3, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=1, device=device(type='cuda'), dim_reduce=2, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 241
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>

y_features shape after NoReduction torch.Size([2708, 18629]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 3880.90308 accuracy= 0.60857 time= 0.45771
Epoch: 0011 train_loss= 33.33690 accuracy= 0.64845 time= 0.45055
Epoch: 0021 train_loss= 30.56227 accuracy= 0.66691 time= 0.45062
Epoch: 0031 train_loss= 28.66231 accuracy= 0.66987 time= 0.45152
Epoch: 0041 train_loss= 27.21519 accuracy= 0.67799 time= 0.45256
Epoch: 0051 train_loss= 25.15784 accuracy= 0.67651 time= 0.45565
Epoch: 0061 train_loss= 24.78719 accuracy= 0.67356 time= 0.45520
Epoch: 0071 train_loss= 23.33671 accuracy= 0.69055 time= 0.44710
Epoch: 0081 train_loss= 21.96314 accuracy= 0.67873 time= 0.45574
Epoch: 0091 train_loss= 20.97540 accuracy= 0.68612 time= 0.45784
Epoch: 0101 train_loss= 20.32151 accuracy= 0.68390 time= 0.45735
Epoch: 0111 train_loss= 19.67912 accuracy= 0.68833 time= 0.45838
Epoch: 0121 train_loss= 18.01411 accuracy= 0.67356 time= 0.45871
Epoch: 0131 train_loss= 17.80876 accuracy= 0.67799 time= 0.46032
Epoch: 0141 train_loss= 16.64103 accuracy= 0.67799 time= 0.45884
Epoch: 0151 train_loss= 16.19740 accuracy= 0.68685 time= 0.46188
Epoch: 0161 train_loss= 15.11806 accuracy= 0.68759 time= 0.46246
Epoch: 0171 train_loss= 14.45656 accuracy= 0.68390 time= 0.46115
Epoch: 0181 train_loss= 13.75686 accuracy= 0.67578 time= 0.46256
Epoch: 0191 train_loss= 13.16969 accuracy= 0.68316 time= 0.46276
Epoch: 0201 train_loss= 12.49596 accuracy= 0.68095 time= 0.46101
Epoch: 0211 train_loss= 11.88726 accuracy= 0.68021 time= 0.46286
Epoch: 0221 train_loss= 11.26730 accuracy= 0.68095 time= 0.46326
Epoch: 0231 train_loss= 10.75723 accuracy= 0.67504 time= 0.46340
Epoch: 0241 train_loss= 10.09953 accuracy= 0.67725 time= 0.46526
Epoch: 0251 train_loss= 9.60142 accuracy= 0.67356 time= 0.46544
Epoch: 0261 train_loss= 9.21197 accuracy= 0.66470 time= 0.46651
Epoch: 0271 train_loss= 8.70670 accuracy= 0.67578 time= 0.46566
Epoch: 0281 train_loss= 8.43998 accuracy= 0.67061 time= 0.46606
Epoch: 0291 train_loss= 7.65005 accuracy= 0.67578 time= 0.46569

accuracy 0.66839
auc 0.55468
f1_score 0.25167
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=3, beta=1, clique_size=20, cuda=False, dataset='cora', decoder=0, device=device(type='cuda'), dim_reduce=2, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 335
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>

y_features shape after NoReduction torch.Size([2708, 18629]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 33.59059 accuracy= 0.60931 time= 0.48788
Epoch: 0011 train_loss= 6.45621 accuracy= 0.70384 time= 0.46834
Epoch: 0021 train_loss= 5.93239 accuracy= 0.70384 time= 0.46795
Epoch: 0031 train_loss= 5.66821 accuracy= 0.70827 time= 0.47183
Epoch: 0041 train_loss= 5.47984 accuracy= 0.70606 time= 0.47000
Epoch: 0051 train_loss= 5.39962 accuracy= 0.70901 time= 0.47077
Epoch: 0061 train_loss= 5.36838 accuracy= 0.71123 time= 0.47242
Epoch: 0071 train_loss= 5.34940 accuracy= 0.71196 time= 0.47148
Epoch: 0081 train_loss= 5.33219 accuracy= 0.71049 time= 0.47287
Epoch: 0091 train_loss= 5.31740 accuracy= 0.71196 time= 0.47484
Epoch: 0101 train_loss= 5.30080 accuracy= 0.71344 time= 0.47287
Epoch: 0111 train_loss= 5.28351 accuracy= 0.71492 time= 0.47457
Epoch: 0121 train_loss= 5.26615 accuracy= 0.71566 time= 0.47542
Epoch: 0131 train_loss= 5.25088 accuracy= 0.71713 time= 0.47651
Epoch: 0141 train_loss= 5.23541 accuracy= 0.71640 time= 0.46352
Epoch: 0151 train_loss= 5.21639 accuracy= 0.71492 time= 0.47382
Epoch: 0161 train_loss= 5.20194 accuracy= 0.71713 time= 0.47845
Epoch: 0171 train_loss= 5.18518 accuracy= 0.71492 time= 0.47743
Epoch: 0181 train_loss= 5.23868 accuracy= 0.70606 time= 0.47732
Epoch: 0191 train_loss= 5.15472 accuracy= 0.71196 time= 0.47590
Epoch: 0201 train_loss= 5.13538 accuracy= 0.70975 time= 0.47537
Epoch: 0211 train_loss= 5.11130 accuracy= 0.71640 time= 0.47728
Epoch: 0221 train_loss= 5.09156 accuracy= 0.71640 time= 0.47763
Epoch: 0231 train_loss= 5.07787 accuracy= 0.71270 time= 0.48197
Epoch: 0241 train_loss= 5.05742 accuracy= 0.71492 time= 0.48086
Epoch: 0251 train_loss= 5.03721 accuracy= 0.71861 time= 0.47987
Epoch: 0261 train_loss= 5.01828 accuracy= 0.71935 time= 0.48126
Epoch: 0271 train_loss= 5.00674 accuracy= 0.71566 time= 0.48177
Epoch: 0281 train_loss= 4.98154 accuracy= 0.71787 time= 0.47938
Epoch: 0291 train_loss= 4.96038 accuracy= 0.72009 time= 0.48171

accuracy 0.72230
auc 0.65460
f1_score 0.37333
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=0, beta=1, clique_size=20, cuda=False, dataset='cora', device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 136
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
using PCA to reduce dim
y_features shape after PCA torch.Size([2708, 358]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 4188.47119 feature_loss= 37.03064 structure_loss= 4151.44043 accuracy= 0.69941 accuracy_s= 0.82496 accuracy_f= 0.79911 time= 0.03194
Epoch: 0011 train_loss= 2608.73193 feature_loss= 17.78426 structure_loss= 2590.94775 accuracy= 0.62482 accuracy_s= 0.78213 accuracy_f= 0.80576 time= 0.02464
Epoch: 0021 train_loss= 2202.55518 feature_loss= 12.31494 structure_loss= 2190.24023 accuracy= 0.60931 accuracy_s= 0.77917 accuracy_f= 0.82275 time= 0.02297
Epoch: 0031 train_loss= 1951.14868 feature_loss= 10.26022 structure_loss= 1940.88843 accuracy= 0.60635 accuracy_s= 0.77917 accuracy_f= 0.83456 time= 0.02283
Epoch: 0041 train_loss= 1773.29993 feature_loss= 9.19723 structure_loss= 1764.10266 accuracy= 0.60487 accuracy_s= 0.77917 accuracy_f= 0.85007 time= 0.02269
Epoch: 0051 train_loss= 1634.90662 feature_loss= 8.40737 structure_loss= 1626.49927 accuracy= 0.60709 accuracy_s= 0.77917 accuracy_f= 0.85746 time= 0.02298
Epoch: 0061 train_loss= 1518.55151 feature_loss= 6.96051 structure_loss= 1511.59106 accuracy= 0.61078 accuracy_s= 0.77917 accuracy_f= 0.85894 time= 0.02286
Epoch: 0071 train_loss= 1420.01685 feature_loss= 6.27970 structure_loss= 1413.73718 accuracy= 0.61152 accuracy_s= 0.77917 accuracy_f= 0.86484 time= 0.02262
Epoch: 0081 train_loss= 1334.29053 feature_loss= 5.99953 structure_loss= 1328.29102 accuracy= 0.61300 accuracy_s= 0.77917 accuracy_f= 0.86854 time= 0.02275
Epoch: 0091 train_loss= 1256.42090 feature_loss= 5.68741 structure_loss= 1250.73352 accuracy= 0.61374 accuracy_s= 0.77917 accuracy_f= 0.86854 time= 0.02302
Epoch: 0101 train_loss= 1187.45740 feature_loss= 5.37711 structure_loss= 1182.08032 accuracy= 0.61374 accuracy_s= 0.77917 accuracy_f= 0.86854 time= 0.02273
Epoch: 0111 train_loss= 1126.78137 feature_loss= 5.27708 structure_loss= 1121.50427 accuracy= 0.61300 accuracy_s= 0.77917 accuracy_f= 0.86780 time= 0.02271
Epoch: 0121 train_loss= 1068.45776 feature_loss= 5.22342 structure_loss= 1063.23438 accuracy= 0.61374 accuracy_s= 0.77917 accuracy_f= 0.87075 time= 0.02304
Epoch: 0131 train_loss= 1016.71130 feature_loss= 5.17151 structure_loss= 1011.53979 accuracy= 0.61448 accuracy_s= 0.77917 accuracy_f= 0.87223 time= 0.02291
Epoch: 0141 train_loss= 968.03314 feature_loss= 5.11831 structure_loss= 962.91486 accuracy= 0.61595 accuracy_s= 0.77917 accuracy_f= 0.87075 time= 0.02274
Epoch: 0151 train_loss= 921.64581 feature_loss= 5.06314 structure_loss= 916.58270 accuracy= 0.61595 accuracy_s= 0.77917 accuracy_f= 0.86928 time= 0.02293
Epoch: 0161 train_loss= 879.33563 feature_loss= 5.02179 structure_loss= 874.31384 accuracy= 0.61669 accuracy_s= 0.77917 accuracy_f= 0.86706 time= 0.02283
Epoch: 0171 train_loss= 840.63806 feature_loss= 4.98839 structure_loss= 835.64966 accuracy= 0.61743 accuracy_s= 0.77917 accuracy_f= 0.86558 time= 0.02265
Epoch: 0181 train_loss= 804.62964 feature_loss= 4.95304 structure_loss= 799.67657 accuracy= 0.61743 accuracy_s= 0.77917 accuracy_f= 0.86780 time= 0.02283
Epoch: 0191 train_loss= 768.77484 feature_loss= 4.92520 structure_loss= 763.84967 accuracy= 0.61669 accuracy_s= 0.77917 accuracy_f= 0.86337 time= 0.02309
Epoch: 0201 train_loss= 733.58075 feature_loss= 4.89966 structure_loss= 728.68109 accuracy= 0.61669 accuracy_s= 0.77917 accuracy_f= 0.86189 time= 0.02272
Epoch: 0211 train_loss= 702.75818 feature_loss= 4.87640 structure_loss= 697.88177 accuracy= 0.61669 accuracy_s= 0.77917 accuracy_f= 0.86189 time= 0.02281
Epoch: 0221 train_loss= 672.64880 feature_loss= 4.85120 structure_loss= 667.79761 accuracy= 0.61669 accuracy_s= 0.77917 accuracy_f= 0.86189 time= 0.02302
Epoch: 0231 train_loss= 644.29852 feature_loss= 4.84810 structure_loss= 639.45044 accuracy= 0.61595 accuracy_s= 0.77917 accuracy_f= 0.86115 time= 0.02269
Epoch: 0241 train_loss= 616.27698 feature_loss= 4.82344 structure_loss= 611.45355 accuracy= 0.61595 accuracy_s= 0.77917 accuracy_f= 0.86115 time= 0.02290
Epoch: 0251 train_loss= 591.36920 feature_loss= 4.80368 structure_loss= 586.56555 accuracy= 0.61521 accuracy_s= 0.77917 accuracy_f= 0.85894 time= 0.02291
Epoch: 0261 train_loss= 564.83844 feature_loss= 4.79525 structure_loss= 560.04321 accuracy= 0.61595 accuracy_s= 0.77917 accuracy_f= 0.85672 time= 0.02277
Epoch: 0271 train_loss= 541.62067 feature_loss= 4.77425 structure_loss= 536.84644 accuracy= 0.61669 accuracy_s= 0.77917 accuracy_f= 0.85672 time= 0.02265
Epoch: 0281 train_loss= 515.55341 feature_loss= 4.82381 structure_loss= 510.72958 accuracy= 0.61300 accuracy_s= 0.77917 accuracy_f= 0.85451 time= 0.02294
Epoch: 0291 train_loss= 496.28741 feature_loss= 4.74030 structure_loss= 491.54712 accuracy= 0.61521 accuracy_s= 0.77917 accuracy_f= 0.85672 time= 0.02288

accuracy 0.61226
accuracy_s 0.77917
accuracy_f 0.85451
auc 0.31129
f1_score 0.12500
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=0, beta=1, clique_size=20, cuda=False, dataset='cora', device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 761
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 28 steps!
y_features shape after FMS torch.Size([2708, 358]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 4119.04883 feature_loss= 37.12271 structure_loss= 4081.92603 accuracy= 0.71935 accuracy_s= 0.83235 accuracy_f= 0.79394 time= 0.03273
Epoch: 0011 train_loss= 2539.60767 feature_loss= 18.36840 structure_loss= 2521.23926 accuracy= 0.61669 accuracy_s= 0.77991 accuracy_f= 0.79838 time= 0.02241
Epoch: 0021 train_loss= 2172.45898 feature_loss= 12.58742 structure_loss= 2159.87158 accuracy= 0.60635 accuracy_s= 0.77917 accuracy_f= 0.81093 time= 0.02225
Epoch: 0031 train_loss= 1940.32043 feature_loss= 10.06063 structure_loss= 1930.25977 accuracy= 0.60931 accuracy_s= 0.77917 accuracy_f= 0.82644 time= 0.02257
Epoch: 0041 train_loss= 1772.92346 feature_loss= 8.90005 structure_loss= 1764.02344 accuracy= 0.60931 accuracy_s= 0.77917 accuracy_f= 0.84121 time= 0.02244
Epoch: 0051 train_loss= 1640.53845 feature_loss= 7.80857 structure_loss= 1632.72986 accuracy= 0.60931 accuracy_s= 0.77917 accuracy_f= 0.85155 time= 0.02254
Epoch: 0061 train_loss= 1529.72363 feature_loss= 7.39911 structure_loss= 1522.32458 accuracy= 0.60931 accuracy_s= 0.77917 accuracy_f= 0.85598 time= 0.02235
Epoch: 0071 train_loss= 1435.29419 feature_loss= 7.09565 structure_loss= 1428.19849 accuracy= 0.61152 accuracy_s= 0.77917 accuracy_f= 0.85672 time= 0.02237
Epoch: 0081 train_loss= 1351.47534 feature_loss= 6.73172 structure_loss= 1344.74365 accuracy= 0.61152 accuracy_s= 0.77917 accuracy_f= 0.86041 time= 0.02238
Epoch: 0091 train_loss= 1275.68054 feature_loss= 6.17410 structure_loss= 1269.50647 accuracy= 0.61078 accuracy_s= 0.77917 accuracy_f= 0.85746 time= 0.02247
Epoch: 0101 train_loss= 1210.44983 feature_loss= 6.07803 structure_loss= 1204.37183 accuracy= 0.60931 accuracy_s= 0.77917 accuracy_f= 0.85894 time= 0.02241
Epoch: 0111 train_loss= 1145.62207 feature_loss= 5.97160 structure_loss= 1139.65051 accuracy= 0.60857 accuracy_s= 0.77917 accuracy_f= 0.85968 time= 0.02235
Epoch: 0121 train_loss= 1087.24561 feature_loss= 5.90595 structure_loss= 1081.33960 accuracy= 0.60783 accuracy_s= 0.77917 accuracy_f= 0.85820 time= 0.02247
Epoch: 0131 train_loss= 1033.00598 feature_loss= 5.83650 structure_loss= 1027.16943 accuracy= 0.61004 accuracy_s= 0.77917 accuracy_f= 0.85894 time= 0.02243
Epoch: 0141 train_loss= 983.61877 feature_loss= 5.76010 structure_loss= 977.85870 accuracy= 0.60709 accuracy_s= 0.77917 accuracy_f= 0.86484 time= 0.02232
Epoch: 0151 train_loss= 933.74585 feature_loss= 5.68164 structure_loss= 928.06421 accuracy= 0.60857 accuracy_s= 0.77917 accuracy_f= 0.86041 time= 0.02243
Epoch: 0161 train_loss= 889.81982 feature_loss= 5.63944 structure_loss= 884.18036 accuracy= 0.60783 accuracy_s= 0.77917 accuracy_f= 0.86189 time= 0.02397
Epoch: 0171 train_loss= 848.01489 feature_loss= 5.60542 structure_loss= 842.40948 accuracy= 0.60857 accuracy_s= 0.77917 accuracy_f= 0.85894 time= 0.02252
Epoch: 0181 train_loss= 808.11389 feature_loss= 5.58429 structure_loss= 802.52960 accuracy= 0.61078 accuracy_s= 0.77917 accuracy_f= 0.85377 time= 0.02244
Epoch: 0191 train_loss= 769.27814 feature_loss= 5.56685 structure_loss= 763.71130 accuracy= 0.61004 accuracy_s= 0.77917 accuracy_f= 0.85524 time= 0.02235
Epoch: 0201 train_loss= 732.28564 feature_loss= 5.82022 structure_loss= 726.46539 accuracy= 0.61078 accuracy_s= 0.77917 accuracy_f= 0.84712 time= 0.02234
Epoch: 0211 train_loss= 697.20721 feature_loss= 5.88104 structure_loss= 691.32617 accuracy= 0.61004 accuracy_s= 0.77843 accuracy_f= 0.84490 time= 0.02243
Epoch: 0221 train_loss= 667.31915 feature_loss= 5.67727 structure_loss= 661.64191 accuracy= 0.60931 accuracy_s= 0.77843 accuracy_f= 0.85968 time= 0.02429
Epoch: 0231 train_loss= 634.77283 feature_loss= 5.54732 structure_loss= 629.22552 accuracy= 0.61078 accuracy_s= 0.77917 accuracy_f= 0.85894 time= 0.02241
Epoch: 0241 train_loss= 604.48102 feature_loss= 5.40788 structure_loss= 599.07312 accuracy= 0.60931 accuracy_s= 0.77843 accuracy_f= 0.85968 time= 0.02240
Epoch: 0251 train_loss= 576.85315 feature_loss= 5.19459 structure_loss= 571.65857 accuracy= 0.60931 accuracy_s= 0.77843 accuracy_f= 0.86115 time= 0.02249
Epoch: 0261 train_loss= 546.78564 feature_loss= 4.93223 structure_loss= 541.85339 accuracy= 0.61004 accuracy_s= 0.77917 accuracy_f= 0.85746 time= 0.02245
Epoch: 0271 train_loss= 519.22723 feature_loss= 4.66383 structure_loss= 514.56342 accuracy= 0.60931 accuracy_s= 0.77917 accuracy_f= 0.85598 time= 0.02238
Epoch: 0281 train_loss= 493.92105 feature_loss= 4.46525 structure_loss= 489.45581 accuracy= 0.60709 accuracy_s= 0.77917 accuracy_f= 0.85524 time= 0.02232
Epoch: 0291 train_loss= 470.45392 feature_loss= 4.37258 structure_loss= 466.08133 accuracy= 0.60783 accuracy_s= 0.77917 accuracy_f= 0.85451 time= 0.02232

accuracy 0.60709
accuracy_s 0.77917
accuracy_f 0.85303
auc 0.30487
f1_score 0.11333
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=0, beta=1, clique_size=20, cuda=False, dataset='cora', device=device(type='cuda'), dim_reduce=2, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 623
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
18629
y_features shape after NoReduction torch.Size([2708, 18629]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 3696.62573 feature_loss= 34.41527 structure_loss= 3662.21045 accuracy= 0.61374 accuracy_s= 0.78065 accuracy_f= 0.81019 time= 0.06153
Epoch: 0011 train_loss= 3224.41479 feature_loss= 7.40005 structure_loss= 3217.01465 accuracy= 0.64697 accuracy_s= 0.80281 accuracy_f= 0.83087 time= 0.04994
Epoch: 0021 train_loss= 1710.92175 feature_loss= 6.37729 structure_loss= 1704.54443 accuracy= 0.63442 accuracy_s= 0.79542 accuracy_f= 0.84121 time= 0.04921
Epoch: 0031 train_loss= 1003.35687 feature_loss= 6.05711 structure_loss= 997.29974 accuracy= 0.63811 accuracy_s= 0.79099 accuracy_f= 0.84343 time= 0.04975
Epoch: 0041 train_loss= 607.51208 feature_loss= 5.87980 structure_loss= 601.63226 accuracy= 0.63368 accuracy_s= 0.78730 accuracy_f= 0.84269 time= 0.04933
Epoch: 0051 train_loss= 393.60434 feature_loss= 5.77039 structure_loss= 387.83395 accuracy= 0.62112 accuracy_s= 0.78508 accuracy_f= 0.84269 time= 0.04934
Epoch: 0061 train_loss= 263.59140 feature_loss= 5.66332 structure_loss= 257.92807 accuracy= 0.61743 accuracy_s= 0.78213 accuracy_f= 0.84417 time= 0.04988
Epoch: 0071 train_loss= 188.15689 feature_loss= 5.56639 structure_loss= 182.59050 accuracy= 0.61595 accuracy_s= 0.78065 accuracy_f= 0.84786 time= 0.04917
Epoch: 0081 train_loss= 133.61673 feature_loss= 5.45994 structure_loss= 128.15680 accuracy= 0.61152 accuracy_s= 0.77991 accuracy_f= 0.84860 time= 0.04974
Epoch: 0091 train_loss= 104.98836 feature_loss= 5.36997 structure_loss= 99.61839 accuracy= 0.60487 accuracy_s= 0.77917 accuracy_f= 0.85229 time= 0.04976
Epoch: 0101 train_loss= 76.97546 feature_loss= 5.31209 structure_loss= 71.66337 accuracy= 0.62186 accuracy_s= 0.78065 accuracy_f= 0.85155 time= 0.04925
Epoch: 0111 train_loss= 64.95966 feature_loss= 5.26184 structure_loss= 59.69782 accuracy= 0.63072 accuracy_s= 0.78656 accuracy_f= 0.84786 time= 0.04990
Epoch: 0121 train_loss= 59.59517 feature_loss= 5.22968 structure_loss= 54.36549 accuracy= 0.62925 accuracy_s= 0.78877 accuracy_f= 0.84860 time= 0.04923
Epoch: 0131 train_loss= 51.76394 feature_loss= 5.19658 structure_loss= 46.56736 accuracy= 0.63516 accuracy_s= 0.79173 accuracy_f= 0.85155 time= 0.05026
Epoch: 0141 train_loss= 45.22866 feature_loss= 5.16300 structure_loss= 40.06566 accuracy= 0.63737 accuracy_s= 0.79025 accuracy_f= 0.85155 time= 0.04942
Epoch: 0151 train_loss= 43.20616 feature_loss= 5.13202 structure_loss= 38.07414 accuracy= 0.63959 accuracy_s= 0.79025 accuracy_f= 0.85672 time= 0.04947
Epoch: 0161 train_loss= 39.84858 feature_loss= 5.09802 structure_loss= 34.75056 accuracy= 0.64032 accuracy_s= 0.79099 accuracy_f= 0.85746 time= 0.04994
Epoch: 0171 train_loss= 36.83751 feature_loss= 5.05012 structure_loss= 31.78739 accuracy= 0.63663 accuracy_s= 0.79542 accuracy_f= 0.86115 time= 0.04936
Epoch: 0181 train_loss= 28.17747 feature_loss= 5.02516 structure_loss= 23.15231 accuracy= 0.68464 accuracy_s= 0.79394 accuracy_f= 0.86189 time= 0.04983
Epoch: 0191 train_loss= 27.05544 feature_loss= 5.00453 structure_loss= 22.05091 accuracy= 0.70901 accuracy_s= 0.78287 accuracy_f= 0.86041 time= 0.04936
Epoch: 0201 train_loss= 25.49973 feature_loss= 4.97888 structure_loss= 20.52085 accuracy= 0.70458 accuracy_s= 0.77843 accuracy_f= 0.86041 time= 0.04991
Epoch: 0211 train_loss= 24.69213 feature_loss= 4.95694 structure_loss= 19.73520 accuracy= 0.70753 accuracy_s= 0.77843 accuracy_f= 0.86263 time= 0.04932
Epoch: 0221 train_loss= 21.63862 feature_loss= 4.93071 structure_loss= 16.70791 accuracy= 0.71640 accuracy_s= 0.77843 accuracy_f= 0.86484 time= 0.04969
Epoch: 0231 train_loss= 20.88166 feature_loss= 4.90127 structure_loss= 15.98039 accuracy= 0.71270 accuracy_s= 0.77843 accuracy_f= 0.86041 time= 0.04935
Epoch: 0241 train_loss= 18.38921 feature_loss= 4.88034 structure_loss= 13.50887 accuracy= 0.71418 accuracy_s= 0.77843 accuracy_f= 0.86041 time= 0.04996
Epoch: 0251 train_loss= 18.57682 feature_loss= 4.84822 structure_loss= 13.72860 accuracy= 0.71861 accuracy_s= 0.77843 accuracy_f= 0.86411 time= 0.04934
Epoch: 0261 train_loss= 15.56746 feature_loss= 4.82650 structure_loss= 10.74096 accuracy= 0.72526 accuracy_s= 0.77843 accuracy_f= 0.86484 time= 0.04980
Epoch: 0271 train_loss= 15.38210 feature_loss= 4.80920 structure_loss= 10.57290 accuracy= 0.72378 accuracy_s= 0.77843 accuracy_f= 0.86411 time= 0.04939
Epoch: 0281 train_loss= 14.90406 feature_loss= 4.84469 structure_loss= 10.05937 accuracy= 0.71640 accuracy_s= 0.77843 accuracy_f= 0.85451 time= 0.04980
Epoch: 0291 train_loss= 13.94623 feature_loss= 4.79302 structure_loss= 9.15321 accuracy= 0.71861 accuracy_s= 0.77843 accuracy_f= 0.86189 time= 0.04923

accuracy 0.72009
accuracy_s 0.77843
accuracy_f 0.86263
auc 0.64895
f1_score 0.36833
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=1, beta=1, clique_size=20, cuda=False, dataset='cora', device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 900
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
using PCA to reduce dim
y_features shape after PCA torch.Size([2708, 358]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 1039.90125 feature_loss= 36.87211 structure_loss= 1003.02917 accuracy= 0.69645 accuracy_s= 0.82866 accuracy_f= 0.79985 time= 0.04244
Epoch: 0011 train_loss= 121.27389 feature_loss= 13.92301 structure_loss= 107.35088 accuracy= 0.61891 accuracy_s= 0.77917 accuracy_f= 0.80798 time= 0.02761
Epoch: 0021 train_loss= 38.85438 feature_loss= 6.50950 structure_loss= 32.34488 accuracy= 0.61448 accuracy_s= 0.77917 accuracy_f= 0.83383 time= 0.02795
Epoch: 0031 train_loss= 14.27730 feature_loss= 5.68263 structure_loss= 8.59467 accuracy= 0.61743 accuracy_s= 0.77917 accuracy_f= 0.83678 time= 0.02750
Epoch: 0041 train_loss= 6.53897 feature_loss= 5.57283 structure_loss= 0.96613 accuracy= 0.63589 accuracy_s= 0.77843 accuracy_f= 0.83826 time= 0.02755
Epoch: 0051 train_loss= 5.95472 feature_loss= 5.53972 structure_loss= 0.41500 accuracy= 0.68316 accuracy_s= 0.77843 accuracy_f= 0.83826 time= 0.02789
Epoch: 0061 train_loss= 5.94492 feature_loss= 5.51663 structure_loss= 0.42829 accuracy= 0.68538 accuracy_s= 0.77843 accuracy_f= 0.83826 time= 0.02740
Epoch: 0071 train_loss= 5.92938 feature_loss= 5.50298 structure_loss= 0.42640 accuracy= 0.68612 accuracy_s= 0.77843 accuracy_f= 0.83900 time= 0.02795
Epoch: 0081 train_loss= 5.91860 feature_loss= 5.49218 structure_loss= 0.42642 accuracy= 0.68538 accuracy_s= 0.77843 accuracy_f= 0.83900 time= 0.02782
Epoch: 0091 train_loss= 5.90720 feature_loss= 5.48164 structure_loss= 0.42556 accuracy= 0.68464 accuracy_s= 0.77843 accuracy_f= 0.83973 time= 0.02747
Epoch: 0101 train_loss= 5.89568 feature_loss= 5.47142 structure_loss= 0.42426 accuracy= 0.68612 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.02757
Epoch: 0111 train_loss= 5.88374 feature_loss= 5.46090 structure_loss= 0.42285 accuracy= 0.68612 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.02783
Epoch: 0121 train_loss= 5.87139 feature_loss= 5.45120 structure_loss= 0.42019 accuracy= 0.68538 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.02747
Epoch: 0131 train_loss= 5.85829 feature_loss= 5.44137 structure_loss= 0.41692 accuracy= 0.68464 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.02756
Epoch: 0141 train_loss= 5.84505 feature_loss= 5.43080 structure_loss= 0.41425 accuracy= 0.68685 accuracy_s= 0.77843 accuracy_f= 0.83973 time= 0.02782
Epoch: 0151 train_loss= 5.83270 feature_loss= 5.42209 structure_loss= 0.41061 accuracy= 0.68538 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.02752
Epoch: 0161 train_loss= 5.81965 feature_loss= 5.41221 structure_loss= 0.40744 accuracy= 0.68390 accuracy_s= 0.77843 accuracy_f= 0.84269 time= 0.02760
Epoch: 0171 train_loss= 5.80686 feature_loss= 5.40273 structure_loss= 0.40412 accuracy= 0.68538 accuracy_s= 0.77843 accuracy_f= 0.84269 time= 0.02773
Epoch: 0181 train_loss= 5.79544 feature_loss= 5.39445 structure_loss= 0.40099 accuracy= 0.68390 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.02738
Epoch: 0191 train_loss= 5.77885 feature_loss= 5.38143 structure_loss= 0.39741 accuracy= 0.68685 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.02752
Epoch: 0201 train_loss= 5.76518 feature_loss= 5.37071 structure_loss= 0.39447 accuracy= 0.68538 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.02882
Epoch: 0211 train_loss= 5.75050 feature_loss= 5.35934 structure_loss= 0.39115 accuracy= 0.68538 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.02742
Epoch: 0221 train_loss= 5.73901 feature_loss= 5.34979 structure_loss= 0.38923 accuracy= 0.68612 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.02763
Epoch: 0231 train_loss= 5.72312 feature_loss= 5.33808 structure_loss= 0.38505 accuracy= 0.68464 accuracy_s= 0.77843 accuracy_f= 0.84269 time= 0.02767
Epoch: 0241 train_loss= 5.70910 feature_loss= 5.32609 structure_loss= 0.38300 accuracy= 0.68612 accuracy_s= 0.77843 accuracy_f= 0.84269 time= 0.02745
Epoch: 0251 train_loss= 5.69507 feature_loss= 5.31458 structure_loss= 0.38049 accuracy= 0.68612 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.02767
Epoch: 0261 train_loss= 5.68292 feature_loss= 5.30553 structure_loss= 0.37739 accuracy= 0.68390 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.02767
Epoch: 0271 train_loss= 5.66937 feature_loss= 5.29434 structure_loss= 0.37504 accuracy= 0.68390 accuracy_s= 0.77843 accuracy_f= 0.84269 time= 0.02755
Epoch: 0281 train_loss= 5.65488 feature_loss= 5.28217 structure_loss= 0.37271 accuracy= 0.68685 accuracy_s= 0.77843 accuracy_f= 0.84343 time= 0.02790
Epoch: 0291 train_loss= 5.63881 feature_loss= 5.26857 structure_loss= 0.37024 accuracy= 0.68685 accuracy_s= 0.77843 accuracy_f= 0.84417 time= 0.02745

accuracy 0.68538
accuracy_s 0.77843
accuracy_f 0.84195
auc 0.56369
f1_score 0.29000
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=1, beta=1, clique_size=20, cuda=False, dataset='cora', device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 969
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 37 steps!
y_features shape after FMS torch.Size([2708, 358]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 38.39590 feature_loss= 35.40356 structure_loss= 2.99234 accuracy= 0.69941 accuracy_s= 0.81758 accuracy_f= 0.79025 time= 0.04683
Epoch: 0011 train_loss= 8.32545 feature_loss= 7.75707 structure_loss= 0.56838 accuracy= 0.68168 accuracy_s= 0.77917 accuracy_f= 0.83309 time= 0.02728
Epoch: 0021 train_loss= 6.61498 feature_loss= 6.20530 structure_loss= 0.40968 accuracy= 0.67799 accuracy_s= 0.77917 accuracy_f= 0.83752 time= 0.02742
Epoch: 0031 train_loss= 6.28142 feature_loss= 5.89670 structure_loss= 0.38472 accuracy= 0.68168 accuracy_s= 0.77917 accuracy_f= 0.83826 time= 0.02732
Epoch: 0041 train_loss= 4.79885 feature_loss= 4.41924 structure_loss= 0.37961 accuracy= 0.67873 accuracy_s= 0.77917 accuracy_f= 0.83900 time= 0.02743
Epoch: 0051 train_loss= 4.65486 feature_loss= 4.27806 structure_loss= 0.37680 accuracy= 0.67873 accuracy_s= 0.77917 accuracy_f= 0.83826 time= 0.02721
Epoch: 0061 train_loss= 4.55207 feature_loss= 4.17740 structure_loss= 0.37468 accuracy= 0.68168 accuracy_s= 0.77917 accuracy_f= 0.83973 time= 0.02734
Epoch: 0071 train_loss= 4.51345 feature_loss= 4.14075 structure_loss= 0.37270 accuracy= 0.68021 accuracy_s= 0.77917 accuracy_f= 0.84047 time= 0.02748
Epoch: 0081 train_loss= 4.50530 feature_loss= 4.13394 structure_loss= 0.37136 accuracy= 0.68095 accuracy_s= 0.77917 accuracy_f= 0.83900 time= 0.02730
Epoch: 0091 train_loss= 4.50209 feature_loss= 4.13197 structure_loss= 0.37012 accuracy= 0.68168 accuracy_s= 0.77917 accuracy_f= 0.83973 time= 0.02757
Epoch: 0101 train_loss= 4.49876 feature_loss= 4.12946 structure_loss= 0.36931 accuracy= 0.68021 accuracy_s= 0.77917 accuracy_f= 0.83973 time= 0.02734
Epoch: 0111 train_loss= 4.49592 feature_loss= 4.12733 structure_loss= 0.36859 accuracy= 0.67947 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.02746
Epoch: 0121 train_loss= 4.49407 feature_loss= 4.12587 structure_loss= 0.36820 accuracy= 0.67947 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.02714
Epoch: 0131 train_loss= 4.49255 feature_loss= 4.12477 structure_loss= 0.36778 accuracy= 0.68095 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.02724
Epoch: 0141 train_loss= 4.49149 feature_loss= 4.12385 structure_loss= 0.36763 accuracy= 0.68095 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.02728
Epoch: 0151 train_loss= 4.49051 feature_loss= 4.12310 structure_loss= 0.36741 accuracy= 0.68021 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.02732
Epoch: 0161 train_loss= 4.48987 feature_loss= 4.12248 structure_loss= 0.36739 accuracy= 0.68021 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.02730
Epoch: 0171 train_loss= 4.48931 feature_loss= 4.12196 structure_loss= 0.36735 accuracy= 0.68095 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.02722
Epoch: 0181 train_loss= 4.48897 feature_loss= 4.12154 structure_loss= 0.36742 accuracy= 0.68095 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.02728
Epoch: 0191 train_loss= 4.48886 feature_loss= 4.12120 structure_loss= 0.36766 accuracy= 0.68168 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.02735
Epoch: 0201 train_loss= 4.48860 feature_loss= 4.12091 structure_loss= 0.36768 accuracy= 0.68242 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.02760
Epoch: 0211 train_loss= 4.48858 feature_loss= 4.12068 structure_loss= 0.36789 accuracy= 0.68242 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.02727
Epoch: 0221 train_loss= 4.48852 feature_loss= 4.12050 structure_loss= 0.36802 accuracy= 0.68168 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.02748
Epoch: 0231 train_loss= 4.48883 feature_loss= 4.12035 structure_loss= 0.36849 accuracy= 0.68168 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.02738
Epoch: 0241 train_loss= 4.48898 feature_loss= 4.12023 structure_loss= 0.36875 accuracy= 0.68242 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.02739
Epoch: 0251 train_loss= 4.48903 feature_loss= 4.12013 structure_loss= 0.36890 accuracy= 0.68168 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.02767
Epoch: 0261 train_loss= 4.48933 feature_loss= 4.12005 structure_loss= 0.36928 accuracy= 0.68316 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.02744
Epoch: 0271 train_loss= 4.48967 feature_loss= 4.11999 structure_loss= 0.36968 accuracy= 0.68168 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.02751
Epoch: 0281 train_loss= 4.48995 feature_loss= 4.11994 structure_loss= 0.37001 accuracy= 0.68168 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.02748
Epoch: 0291 train_loss= 4.49050 feature_loss= 4.11991 structure_loss= 0.37060 accuracy= 0.68168 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.02752

accuracy 0.68168
accuracy_s 0.77843
accuracy_f 0.84195
auc 0.54657
f1_score 0.28167
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=1, beta=1, clique_size=20, cuda=False, dataset='cora', device=device(type='cuda'), dim_reduce=2, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 683
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
18629
y_features shape after NoReduction torch.Size([2708, 18629]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 5.00258 feature_loss= 4.57033 structure_loss= 0.43226 accuracy= 0.65288 accuracy_s= 0.77843 accuracy_f= 0.82496 time= 0.79733
Epoch: 0011 train_loss= 4.62578 feature_loss= 4.19361 structure_loss= 0.43218 accuracy= 0.68168 accuracy_s= 0.77843 accuracy_f= 0.83678 time= 0.77397
Epoch: 0021 train_loss= 4.60252 feature_loss= 4.17002 structure_loss= 0.43250 accuracy= 0.68316 accuracy_s= 0.77843 accuracy_f= 0.83973 time= 0.78065
Epoch: 0031 train_loss= 4.59054 feature_loss= 4.15850 structure_loss= 0.43203 accuracy= 0.68464 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.78522
Epoch: 0041 train_loss= 4.61922 feature_loss= 4.18723 structure_loss= 0.43199 accuracy= 0.68242 accuracy_s= 0.77843 accuracy_f= 0.83383 time= 0.78726
Epoch: 0051 train_loss= 4.57698 feature_loss= 4.14501 structure_loss= 0.43197 accuracy= 0.68612 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.78631
Epoch: 0061 train_loss= 4.60424 feature_loss= 4.17230 structure_loss= 0.43195 accuracy= 0.68168 accuracy_s= 0.77843 accuracy_f= 0.83530 time= 0.79695
Epoch: 0071 train_loss= 4.59682 feature_loss= 4.16489 structure_loss= 0.43193 accuracy= 0.68538 accuracy_s= 0.77843 accuracy_f= 0.83530 time= 0.79388
Epoch: 0081 train_loss= 4.56595 feature_loss= 4.13402 structure_loss= 0.43193 accuracy= 0.68685 accuracy_s= 0.77843 accuracy_f= 0.83973 time= 0.79340
Epoch: 0091 train_loss= 4.57298 feature_loss= 4.14105 structure_loss= 0.43193 accuracy= 0.68538 accuracy_s= 0.77843 accuracy_f= 0.83826 time= 0.77679
Epoch: 0101 train_loss= 4.57349 feature_loss= 4.14156 structure_loss= 0.43193 accuracy= 0.68464 accuracy_s= 0.77843 accuracy_f= 0.83826 time= 0.77737
Epoch: 0111 train_loss= 4.57151 feature_loss= 4.13958 structure_loss= 0.43194 accuracy= 0.68464 accuracy_s= 0.77843 accuracy_f= 0.83900 time= 0.77897
Epoch: 0121 train_loss= 4.56155 feature_loss= 4.12960 structure_loss= 0.43195 accuracy= 0.68464 accuracy_s= 0.77843 accuracy_f= 0.83973 time= 0.79661
Epoch: 0131 train_loss= 4.55830 feature_loss= 4.12634 structure_loss= 0.43196 accuracy= 0.68390 accuracy_s= 0.77843 accuracy_f= 0.83973 time= 0.79178
Epoch: 0141 train_loss= 4.55725 feature_loss= 4.12528 structure_loss= 0.43197 accuracy= 0.68464 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.78619
Epoch: 0151 train_loss= 4.55554 feature_loss= 4.12356 structure_loss= 0.43197 accuracy= 0.68316 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.78097
Epoch: 0161 train_loss= 4.55522 feature_loss= 4.12324 structure_loss= 0.43198 accuracy= 0.68316 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.79566
Epoch: 0171 train_loss= 4.55512 feature_loss= 4.12314 structure_loss= 0.43198 accuracy= 0.68242 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.79845
Epoch: 0181 train_loss= 4.55384 feature_loss= 4.12185 structure_loss= 0.43199 accuracy= 0.68390 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.78779
Epoch: 0191 train_loss= 4.55480 feature_loss= 4.12282 structure_loss= 0.43199 accuracy= 0.68390 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.79388
Epoch: 0201 train_loss= 4.55331 feature_loss= 4.12132 structure_loss= 0.43199 accuracy= 0.68538 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.79642
Epoch: 0211 train_loss= 4.55341 feature_loss= 4.12142 structure_loss= 0.43199 accuracy= 0.68464 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.79210
Epoch: 0221 train_loss= 4.55265 feature_loss= 4.12066 structure_loss= 0.43199 accuracy= 0.68464 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.79782
Epoch: 0231 train_loss= 4.55281 feature_loss= 4.12082 structure_loss= 0.43199 accuracy= 0.68538 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.79131
Epoch: 0241 train_loss= 4.55238 feature_loss= 4.12039 structure_loss= 0.43199 accuracy= 0.68464 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.78413
Epoch: 0251 train_loss= 4.55222 feature_loss= 4.12023 structure_loss= 0.43199 accuracy= 0.68464 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.78617
Epoch: 0261 train_loss= 4.55226 feature_loss= 4.12027 structure_loss= 0.43199 accuracy= 0.68464 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.79556
Epoch: 0271 train_loss= 4.55205 feature_loss= 4.12006 structure_loss= 0.43199 accuracy= 0.68464 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.78119
Epoch: 0281 train_loss= 4.55206 feature_loss= 4.12007 structure_loss= 0.43199 accuracy= 0.68464 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.78674
Epoch: 0291 train_loss= 4.55194 feature_loss= 4.11995 structure_loss= 0.43199 accuracy= 0.68390 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.79163

accuracy 0.68390
accuracy_s 0.77843
accuracy_f 0.84195
auc 0.55228
f1_score 0.28667
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=2, beta=1, clique_size=20, cuda=False, dataset='cora', device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 742
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
using PCA to reduce dim
y_features shape after PCA torch.Size([2708, 358]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 611.70825 feature_loss= 37.06444 structure_loss= 574.64380 accuracy= 0.67504 accuracy_s= 0.81905 accuracy_f= 0.79985 time= 0.03768
Epoch: 0011 train_loss= 90.52891 feature_loss= 15.04347 structure_loss= 75.48544 accuracy= 0.63516 accuracy_s= 0.77917 accuracy_f= 0.80945 time= 0.02428
Epoch: 0021 train_loss= 32.03216 feature_loss= 7.63095 structure_loss= 24.40121 accuracy= 0.60783 accuracy_s= 0.77917 accuracy_f= 0.83013 time= 0.02427
Epoch: 0031 train_loss= 13.47167 feature_loss= 6.48715 structure_loss= 6.98452 accuracy= 0.61004 accuracy_s= 0.77917 accuracy_f= 0.83973 time= 0.02429
Epoch: 0041 train_loss= 7.67888 feature_loss= 6.03036 structure_loss= 1.64852 accuracy= 0.63220 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.02419
Epoch: 0051 train_loss= 6.18039 feature_loss= 5.71466 structure_loss= 0.46573 accuracy= 0.67504 accuracy_s= 0.77843 accuracy_f= 0.83973 time= 0.02415
Epoch: 0061 train_loss= 5.92234 feature_loss= 5.54360 structure_loss= 0.37875 accuracy= 0.68612 accuracy_s= 0.77843 accuracy_f= 0.83973 time= 0.02412
Epoch: 0071 train_loss= 5.87122 feature_loss= 5.49886 structure_loss= 0.37236 accuracy= 0.68612 accuracy_s= 0.77843 accuracy_f= 0.83973 time= 0.02413
Epoch: 0081 train_loss= 5.84380 feature_loss= 5.47941 structure_loss= 0.36438 accuracy= 0.68612 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.02415
Epoch: 0091 train_loss= 5.82344 feature_loss= 5.46589 structure_loss= 0.35755 accuracy= 0.68612 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.02406
Epoch: 0101 train_loss= 5.80504 feature_loss= 5.45192 structure_loss= 0.35313 accuracy= 0.68685 accuracy_s= 0.77843 accuracy_f= 0.84343 time= 0.02408
Epoch: 0111 train_loss= 5.78608 feature_loss= 5.43773 structure_loss= 0.34836 accuracy= 0.68759 accuracy_s= 0.77843 accuracy_f= 0.84490 time= 0.02399
Epoch: 0121 train_loss= 5.76731 feature_loss= 5.42291 structure_loss= 0.34439 accuracy= 0.68981 accuracy_s= 0.77843 accuracy_f= 0.84490 time= 0.02409
Epoch: 0131 train_loss= 5.74875 feature_loss= 5.40789 structure_loss= 0.34086 accuracy= 0.68907 accuracy_s= 0.77843 accuracy_f= 0.84638 time= 0.02412
Epoch: 0141 train_loss= 5.72937 feature_loss= 5.39205 structure_loss= 0.33732 accuracy= 0.69129 accuracy_s= 0.77843 accuracy_f= 0.84712 time= 0.02406
Epoch: 0151 train_loss= 5.71227 feature_loss= 5.37774 structure_loss= 0.33454 accuracy= 0.68907 accuracy_s= 0.77843 accuracy_f= 0.84860 time= 0.02402
Epoch: 0161 train_loss= 5.69605 feature_loss= 5.36482 structure_loss= 0.33123 accuracy= 0.69498 accuracy_s= 0.77843 accuracy_f= 0.84712 time= 0.02406
Epoch: 0171 train_loss= 5.67422 feature_loss= 5.34601 structure_loss= 0.32821 accuracy= 0.69645 accuracy_s= 0.77843 accuracy_f= 0.84860 time= 0.02533
Epoch: 0181 train_loss= 5.65293 feature_loss= 5.32921 structure_loss= 0.32372 accuracy= 0.69719 accuracy_s= 0.77843 accuracy_f= 0.84860 time= 0.02410
Epoch: 0191 train_loss= 5.63964 feature_loss= 5.31741 structure_loss= 0.32223 accuracy= 0.69719 accuracy_s= 0.77843 accuracy_f= 0.84860 time= 0.02412
Epoch: 0201 train_loss= 5.61549 feature_loss= 5.29717 structure_loss= 0.31831 accuracy= 0.70089 accuracy_s= 0.77843 accuracy_f= 0.84934 time= 0.02400
Epoch: 0211 train_loss= 5.59552 feature_loss= 5.28075 structure_loss= 0.31477 accuracy= 0.69572 accuracy_s= 0.77843 accuracy_f= 0.84786 time= 0.02404
Epoch: 0221 train_loss= 5.57444 feature_loss= 5.26447 structure_loss= 0.30997 accuracy= 0.69867 accuracy_s= 0.77843 accuracy_f= 0.85007 time= 0.02477
Epoch: 0231 train_loss= 5.55959 feature_loss= 5.25299 structure_loss= 0.30660 accuracy= 0.69645 accuracy_s= 0.77843 accuracy_f= 0.85229 time= 0.02417
Epoch: 0241 train_loss= 5.55246 feature_loss= 5.24797 structure_loss= 0.30449 accuracy= 0.70089 accuracy_s= 0.77843 accuracy_f= 0.85007 time= 0.02413
Epoch: 0251 train_loss= 5.53833 feature_loss= 5.23842 structure_loss= 0.29991 accuracy= 0.69719 accuracy_s= 0.77843 accuracy_f= 0.84564 time= 0.02413
Epoch: 0261 train_loss= 5.52004 feature_loss= 5.22325 structure_loss= 0.29679 accuracy= 0.69498 accuracy_s= 0.77843 accuracy_f= 0.84786 time= 0.02409
Epoch: 0271 train_loss= 5.50993 feature_loss= 5.21640 structure_loss= 0.29353 accuracy= 0.69572 accuracy_s= 0.77843 accuracy_f= 0.85155 time= 0.02445
Epoch: 0281 train_loss= 5.50229 feature_loss= 5.21042 structure_loss= 0.29187 accuracy= 0.69645 accuracy_s= 0.77843 accuracy_f= 0.84860 time= 0.02403
Epoch: 0291 train_loss= 5.47543 feature_loss= 5.18643 structure_loss= 0.28900 accuracy= 0.69645 accuracy_s= 0.77843 accuracy_f= 0.84860 time= 0.02405

accuracy 0.69645
accuracy_s 0.77843
accuracy_f 0.84786
auc 0.60147
f1_score 0.31500
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=2, beta=1, clique_size=20, cuda=False, dataset='cora', device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 959
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 28 steps!
y_features shape after FMS torch.Size([2708, 358]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 35.91897 feature_loss= 34.41691 structure_loss= 1.50206 accuracy= 0.70901 accuracy_s= 0.84343 accuracy_f= 0.79321 time= 0.03928
Epoch: 0011 train_loss= 4.62667 feature_loss= 4.20063 structure_loss= 0.42605 accuracy= 0.67873 accuracy_s= 0.77991 accuracy_f= 0.83973 time= 0.02384
Epoch: 0021 train_loss= 4.54783 feature_loss= 4.17291 structure_loss= 0.37493 accuracy= 0.68095 accuracy_s= 0.77917 accuracy_f= 0.83973 time= 0.02383
Epoch: 0031 train_loss= 4.52893 feature_loss= 4.15833 structure_loss= 0.37060 accuracy= 0.68242 accuracy_s= 0.77917 accuracy_f= 0.84047 time= 0.02363
Epoch: 0041 train_loss= 4.51732 feature_loss= 4.14995 structure_loss= 0.36737 accuracy= 0.68242 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.02382
Epoch: 0051 train_loss= 4.50785 feature_loss= 4.14396 structure_loss= 0.36389 accuracy= 0.68242 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.02392
Epoch: 0061 train_loss= 4.50051 feature_loss= 4.13942 structure_loss= 0.36109 accuracy= 0.68242 accuracy_s= 0.77843 accuracy_f= 0.83973 time= 0.02390
Epoch: 0071 train_loss= 4.49519 feature_loss= 4.13584 structure_loss= 0.35935 accuracy= 0.68316 accuracy_s= 0.77843 accuracy_f= 0.83973 time= 0.02368
Epoch: 0081 train_loss= 4.49063 feature_loss= 4.13297 structure_loss= 0.35765 accuracy= 0.68390 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.02379
Epoch: 0091 train_loss= 4.48706 feature_loss= 4.13063 structure_loss= 0.35643 accuracy= 0.68538 accuracy_s= 0.77843 accuracy_f= 0.83973 time= 0.02381
Epoch: 0101 train_loss= 4.48417 feature_loss= 4.12872 structure_loss= 0.35545 accuracy= 0.68464 accuracy_s= 0.77843 accuracy_f= 0.83973 time= 0.02386
Epoch: 0111 train_loss= 4.48203 feature_loss= 4.12713 structure_loss= 0.35490 accuracy= 0.68095 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.02376
Epoch: 0121 train_loss= 4.48020 feature_loss= 4.12583 structure_loss= 0.35437 accuracy= 0.68095 accuracy_s= 0.77843 accuracy_f= 0.83973 time= 0.02382
Epoch: 0131 train_loss= 4.47871 feature_loss= 4.12474 structure_loss= 0.35397 accuracy= 0.68095 accuracy_s= 0.77843 accuracy_f= 0.83973 time= 0.02386
Epoch: 0141 train_loss= 4.47761 feature_loss= 4.12384 structure_loss= 0.35377 accuracy= 0.68168 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.02369
Epoch: 0151 train_loss= 4.47654 feature_loss= 4.12310 structure_loss= 0.35345 accuracy= 0.68095 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.02374
Epoch: 0161 train_loss= 4.47585 feature_loss= 4.12248 structure_loss= 0.35337 accuracy= 0.68168 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.02379
Epoch: 0171 train_loss= 4.47559 feature_loss= 4.12197 structure_loss= 0.35362 accuracy= 0.68168 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.02384
Epoch: 0181 train_loss= 4.47511 feature_loss= 4.12155 structure_loss= 0.35357 accuracy= 0.68168 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.02377
Epoch: 0191 train_loss= 4.47505 feature_loss= 4.12120 structure_loss= 0.35385 accuracy= 0.68168 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.02376
Epoch: 0201 train_loss= 4.47483 feature_loss= 4.12092 structure_loss= 0.35392 accuracy= 0.68316 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.02387
Epoch: 0211 train_loss= 4.47457 feature_loss= 4.12069 structure_loss= 0.35388 accuracy= 0.68242 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.02382
Epoch: 0221 train_loss= 4.47466 feature_loss= 4.12050 structure_loss= 0.35416 accuracy= 0.68316 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.02379
Epoch: 0231 train_loss= 4.47492 feature_loss= 4.12035 structure_loss= 0.35457 accuracy= 0.68242 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.02379
Epoch: 0241 train_loss= 4.47512 feature_loss= 4.12023 structure_loss= 0.35489 accuracy= 0.68168 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.02388
Epoch: 0251 train_loss= 4.47549 feature_loss= 4.12013 structure_loss= 0.35536 accuracy= 0.68168 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.02378
Epoch: 0261 train_loss= 4.47563 feature_loss= 4.12005 structure_loss= 0.35557 accuracy= 0.68168 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.02387
Epoch: 0271 train_loss= 4.47607 feature_loss= 4.11999 structure_loss= 0.35608 accuracy= 0.68168 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.02381
Epoch: 0281 train_loss= 4.47638 feature_loss= 4.11994 structure_loss= 0.35644 accuracy= 0.68168 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.02366
Epoch: 0291 train_loss= 4.47709 feature_loss= 4.11991 structure_loss= 0.35718 accuracy= 0.68168 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.02381

accuracy 0.68168
accuracy_s 0.77843
accuracy_f 0.84195
auc 0.54805
f1_score 0.28167
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=2, beta=1, clique_size=20, cuda=False, dataset='cora', device=device(type='cuda'), dim_reduce=2, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 675
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
18629
y_features shape after NoReduction torch.Size([2708, 18629]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 4.88912 feature_loss= 4.45682 structure_loss= 0.43230 accuracy= 0.66174 accuracy_s= 0.77843 accuracy_f= 0.82939 time= 0.12075
Epoch: 0011 train_loss= 4.62211 feature_loss= 4.19004 structure_loss= 0.43208 accuracy= 0.68242 accuracy_s= 0.77843 accuracy_f= 0.83752 time= 0.10275
Epoch: 0021 train_loss= 4.60965 feature_loss= 4.17728 structure_loss= 0.43237 accuracy= 0.68464 accuracy_s= 0.77843 accuracy_f= 0.83826 time= 0.10206
Epoch: 0031 train_loss= 4.64876 feature_loss= 4.21677 structure_loss= 0.43199 accuracy= 0.68095 accuracy_s= 0.77843 accuracy_f= 0.83161 time= 0.10255
Epoch: 0041 train_loss= 4.59541 feature_loss= 4.16340 structure_loss= 0.43201 accuracy= 0.68612 accuracy_s= 0.77843 accuracy_f= 0.83752 time= 0.10260
Epoch: 0051 train_loss= 4.57700 feature_loss= 4.14506 structure_loss= 0.43194 accuracy= 0.68612 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.10326
Epoch: 0061 train_loss= 4.61574 feature_loss= 4.18380 structure_loss= 0.43194 accuracy= 0.68390 accuracy_s= 0.77843 accuracy_f= 0.83309 time= 0.10370
Epoch: 0071 train_loss= 4.58037 feature_loss= 4.14843 structure_loss= 0.43194 accuracy= 0.68538 accuracy_s= 0.77843 accuracy_f= 0.83826 time= 0.10233
Epoch: 0081 train_loss= 4.57475 feature_loss= 4.14281 structure_loss= 0.43194 accuracy= 0.68685 accuracy_s= 0.77843 accuracy_f= 0.83973 time= 0.10313
Epoch: 0091 train_loss= 4.57018 feature_loss= 4.13824 structure_loss= 0.43194 accuracy= 0.68612 accuracy_s= 0.77843 accuracy_f= 0.83900 time= 0.10237
Epoch: 0101 train_loss= 4.56737 feature_loss= 4.13543 structure_loss= 0.43194 accuracy= 0.68538 accuracy_s= 0.77843 accuracy_f= 0.83900 time= 0.10248
Epoch: 0111 train_loss= 4.56329 feature_loss= 4.13135 structure_loss= 0.43194 accuracy= 0.68538 accuracy_s= 0.77843 accuracy_f= 0.83900 time= 0.10333
Epoch: 0121 train_loss= 4.55896 feature_loss= 4.12702 structure_loss= 0.43194 accuracy= 0.68538 accuracy_s= 0.77843 accuracy_f= 0.83973 time= 0.10269
Epoch: 0131 train_loss= 4.55771 feature_loss= 4.12577 structure_loss= 0.43194 accuracy= 0.68390 accuracy_s= 0.77843 accuracy_f= 0.83973 time= 0.10264
Epoch: 0141 train_loss= 4.55756 feature_loss= 4.12562 structure_loss= 0.43194 accuracy= 0.68316 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.10260
Epoch: 0151 train_loss= 4.55676 feature_loss= 4.12482 structure_loss= 0.43194 accuracy= 0.68316 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.10379
Epoch: 0161 train_loss= 4.55483 feature_loss= 4.12289 structure_loss= 0.43194 accuracy= 0.68316 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.10326
Epoch: 0171 train_loss= 4.55427 feature_loss= 4.12232 structure_loss= 0.43195 accuracy= 0.68316 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.10271
Epoch: 0181 train_loss= 4.55380 feature_loss= 4.12185 structure_loss= 0.43195 accuracy= 0.68390 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.10263
Epoch: 0191 train_loss= 4.55394 feature_loss= 4.12199 structure_loss= 0.43195 accuracy= 0.68464 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.10299
Epoch: 0201 train_loss= 4.55309 feature_loss= 4.12114 structure_loss= 0.43195 accuracy= 0.68538 accuracy_s= 0.77843 accuracy_f= 0.84047 time= 0.10262
Epoch: 0211 train_loss= 4.55292 feature_loss= 4.12097 structure_loss= 0.43195 accuracy= 0.68464 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.10342
Epoch: 0221 train_loss= 4.55261 feature_loss= 4.12066 structure_loss= 0.43195 accuracy= 0.68464 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.10303
Epoch: 0231 train_loss= 4.55348 feature_loss= 4.12152 structure_loss= 0.43195 accuracy= 0.68464 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.10297
Epoch: 0241 train_loss= 4.55229 feature_loss= 4.12034 structure_loss= 0.43196 accuracy= 0.68464 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.10329
Epoch: 0251 train_loss= 4.55222 feature_loss= 4.12027 structure_loss= 0.43196 accuracy= 0.68390 accuracy_s= 0.77843 accuracy_f= 0.84121 time= 0.10346
Epoch: 0261 train_loss= 4.55233 feature_loss= 4.12037 structure_loss= 0.43196 accuracy= 0.68464 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.10344
Epoch: 0271 train_loss= 4.55203 feature_loss= 4.12007 structure_loss= 0.43196 accuracy= 0.68464 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.10357
Epoch: 0281 train_loss= 4.55196 feature_loss= 4.12000 structure_loss= 0.43196 accuracy= 0.68464 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.10402
Epoch: 0291 train_loss= 4.55270 feature_loss= 4.12074 structure_loss= 0.43196 accuracy= 0.68390 accuracy_s= 0.77843 accuracy_f= 0.84195 time= 0.10411

accuracy 0.68390
accuracy_s 0.77843
accuracy_f 0.84195
auc 0.55224
f1_score 0.28667
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=3, beta=1, clique_size=20, cuda=False, dataset='cora', device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 62
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
using PCA to reduce dim
y_features shape after PCA torch.Size([2708, 358]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 4487.95312 feature_loss= 36.84204 structure_loss= 4451.11084 accuracy= 0.68759 accuracy_s= 0.82939 accuracy_f= 0.79911 time= 0.04298
Epoch: 0011 train_loss= 2684.20947 feature_loss= 16.53577 structure_loss= 2667.67383 accuracy= 0.60857 accuracy_s= 0.77991 accuracy_f= 0.80871 time= 0.02914
Epoch: 0021 train_loss= 2289.20630 feature_loss= 10.03565 structure_loss= 2279.17065 accuracy= 0.61078 accuracy_s= 0.77917 accuracy_f= 0.82644 time= 0.02641
Epoch: 0031 train_loss= 2039.13452 feature_loss= 7.66296 structure_loss= 2031.47156 accuracy= 0.61078 accuracy_s= 0.77917 accuracy_f= 0.83973 time= 0.02689
Epoch: 0041 train_loss= 1855.51416 feature_loss= 6.43149 structure_loss= 1849.08264 accuracy= 0.60709 accuracy_s= 0.77917 accuracy_f= 0.84417 time= 0.02666
Epoch: 0051 train_loss= 1706.10144 feature_loss= 6.12845 structure_loss= 1699.97302 accuracy= 0.60857 accuracy_s= 0.77917 accuracy_f= 0.85155 time= 0.02656
Epoch: 0061 train_loss= 1578.97729 feature_loss= 6.02310 structure_loss= 1572.95422 accuracy= 0.60931 accuracy_s= 0.77917 accuracy_f= 0.85524 time= 0.02648
Epoch: 0071 train_loss= 1466.24670 feature_loss= 5.92388 structure_loss= 1460.32288 accuracy= 0.61152 accuracy_s= 0.77843 accuracy_f= 0.85820 time= 0.02648
Epoch: 0081 train_loss= 1364.40002 feature_loss= 5.84205 structure_loss= 1358.55798 accuracy= 0.61078 accuracy_s= 0.77843 accuracy_f= 0.85968 time= 0.02757
Epoch: 0091 train_loss= 1277.53284 feature_loss= 5.76259 structure_loss= 1271.77026 accuracy= 0.61226 accuracy_s= 0.77843 accuracy_f= 0.86115 time= 0.02729
Epoch: 0101 train_loss= 1198.83337 feature_loss= 5.65217 structure_loss= 1193.18115 accuracy= 0.61374 accuracy_s= 0.77843 accuracy_f= 0.86558 time= 0.02771
Epoch: 0111 train_loss= 1123.66370 feature_loss= 5.55951 structure_loss= 1118.10425 accuracy= 0.60931 accuracy_s= 0.77843 accuracy_f= 0.86411 time= 0.02710
Epoch: 0121 train_loss= 1059.70459 feature_loss= 5.48044 structure_loss= 1054.22412 accuracy= 0.61004 accuracy_s= 0.77843 accuracy_f= 0.86558 time= 0.02761
Epoch: 0131 train_loss= 1004.05951 feature_loss= 5.40585 structure_loss= 998.65369 accuracy= 0.60857 accuracy_s= 0.77843 accuracy_f= 0.86706 time= 0.02720
Epoch: 0141 train_loss= 953.63831 feature_loss= 5.34056 structure_loss= 948.29773 accuracy= 0.60857 accuracy_s= 0.77843 accuracy_f= 0.87001 time= 0.02755
Epoch: 0151 train_loss= 902.36206 feature_loss= 5.23797 structure_loss= 897.12408 accuracy= 0.61004 accuracy_s= 0.77843 accuracy_f= 0.86337 time= 0.02728
Epoch: 0161 train_loss= 864.57239 feature_loss= 5.31462 structure_loss= 859.25775 accuracy= 0.60931 accuracy_s= 0.77843 accuracy_f= 0.84269 time= 0.02725
Epoch: 0171 train_loss= 824.41583 feature_loss= 5.37588 structure_loss= 819.03998 accuracy= 0.60857 accuracy_s= 0.77843 accuracy_f= 0.84934 time= 0.02750
Epoch: 0181 train_loss= 783.96594 feature_loss= 5.40429 structure_loss= 778.56165 accuracy= 0.61152 accuracy_s= 0.77917 accuracy_f= 0.84417 time= 0.02714
Epoch: 0191 train_loss= 751.12329 feature_loss= 5.34408 structure_loss= 745.77924 accuracy= 0.60931 accuracy_s= 0.77917 accuracy_f= 0.85451 time= 0.02744
Epoch: 0201 train_loss= 713.49817 feature_loss= 5.36610 structure_loss= 708.13208 accuracy= 0.60857 accuracy_s= 0.77843 accuracy_f= 0.84712 time= 0.02726
Epoch: 0211 train_loss= 683.80957 feature_loss= 5.23327 structure_loss= 678.57629 accuracy= 0.60783 accuracy_s= 0.77991 accuracy_f= 0.85155 time= 0.02732
Epoch: 0221 train_loss= 659.32037 feature_loss= 5.11117 structure_loss= 654.20923 accuracy= 0.60931 accuracy_s= 0.77991 accuracy_f= 0.85081 time= 0.02647
Epoch: 0231 train_loss= 626.95996 feature_loss= 5.04158 structure_loss= 621.91840 accuracy= 0.60709 accuracy_s= 0.77991 accuracy_f= 0.85007 time= 0.02644
Epoch: 0241 train_loss= 598.79785 feature_loss= 4.86544 structure_loss= 593.93243 accuracy= 0.60709 accuracy_s= 0.77991 accuracy_f= 0.85672 time= 0.02667
Epoch: 0251 train_loss= 578.42450 feature_loss= 4.73128 structure_loss= 573.69324 accuracy= 0.60561 accuracy_s= 0.77991 accuracy_f= 0.85820 time= 0.02671
Epoch: 0261 train_loss= 551.53430 feature_loss= 4.58181 structure_loss= 546.95251 accuracy= 0.60783 accuracy_s= 0.77991 accuracy_f= 0.85155 time= 0.02675
Epoch: 0271 train_loss= 529.03461 feature_loss= 4.39344 structure_loss= 524.64117 accuracy= 0.60783 accuracy_s= 0.77991 accuracy_f= 0.85155 time= 0.02657
Epoch: 0281 train_loss= 509.78046 feature_loss= 4.28753 structure_loss= 505.49292 accuracy= 0.60635 accuracy_s= 0.77991 accuracy_f= 0.84860 time= 0.02662
Epoch: 0291 train_loss= 486.65289 feature_loss= 4.25930 structure_loss= 482.39359 accuracy= 0.60709 accuracy_s= 0.77991 accuracy_f= 0.84490 time= 0.02673

accuracy 0.60635
accuracy_s 0.77917
accuracy_f 0.85155
auc 0.34251
f1_score 0.11167
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=3, beta=1, clique_size=20, cuda=False, dataset='cora', device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 620
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 31 steps!
y_features shape after FMS torch.Size([2708, 358]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 4444.17090 feature_loss= 36.91158 structure_loss= 4407.25928 accuracy= 0.72157 accuracy_s= 0.84638 accuracy_f= 0.79542 time= 0.04540
Epoch: 0011 train_loss= 2708.21606 feature_loss= 14.71531 structure_loss= 2693.50073 accuracy= 0.60709 accuracy_s= 0.77991 accuracy_f= 0.79985 time= 0.02707
Epoch: 0021 train_loss= 2314.57202 feature_loss= 8.57607 structure_loss= 2305.99585 accuracy= 0.60857 accuracy_s= 0.77917 accuracy_f= 0.81684 time= 0.02675
Epoch: 0031 train_loss= 2062.85376 feature_loss= 6.81730 structure_loss= 2056.03638 accuracy= 0.60783 accuracy_s= 0.77917 accuracy_f= 0.83900 time= 0.02664
Epoch: 0041 train_loss= 1876.86597 feature_loss= 6.24209 structure_loss= 1870.62390 accuracy= 0.60783 accuracy_s= 0.77917 accuracy_f= 0.84343 time= 0.02687
Epoch: 0051 train_loss= 1725.24194 feature_loss= 6.18030 structure_loss= 1719.06165 accuracy= 0.60709 accuracy_s= 0.77917 accuracy_f= 0.84860 time= 0.02653
Epoch: 0061 train_loss= 1595.80115 feature_loss= 6.10249 structure_loss= 1589.69861 accuracy= 0.60635 accuracy_s= 0.77917 accuracy_f= 0.85155 time= 0.02647
Epoch: 0071 train_loss= 1479.26318 feature_loss= 6.04269 structure_loss= 1473.22046 accuracy= 0.60487 accuracy_s= 0.77917 accuracy_f= 0.85672 time= 0.02659
Epoch: 0081 train_loss= 1377.41882 feature_loss= 6.00554 structure_loss= 1371.41333 accuracy= 0.60709 accuracy_s= 0.77917 accuracy_f= 0.86115 time= 0.02640
Epoch: 0091 train_loss= 1287.55164 feature_loss= 5.97012 structure_loss= 1281.58154 accuracy= 0.61078 accuracy_s= 0.77917 accuracy_f= 0.86041 time= 0.02642
Epoch: 0101 train_loss= 1206.32239 feature_loss= 5.93551 structure_loss= 1200.38684 accuracy= 0.60931 accuracy_s= 0.77917 accuracy_f= 0.85820 time= 0.02645
Epoch: 0111 train_loss= 1131.55554 feature_loss= 5.90215 structure_loss= 1125.65344 accuracy= 0.61300 accuracy_s= 0.77917 accuracy_f= 0.86189 time= 0.02652
Epoch: 0121 train_loss= 1067.40491 feature_loss= 5.86728 structure_loss= 1061.53760 accuracy= 0.61300 accuracy_s= 0.77991 accuracy_f= 0.86263 time= 0.02671
Epoch: 0131 train_loss= 1012.72522 feature_loss= 5.83272 structure_loss= 1006.89252 accuracy= 0.61300 accuracy_s= 0.77991 accuracy_f= 0.86411 time= 0.02666
Epoch: 0141 train_loss= 957.22443 feature_loss= 5.79808 structure_loss= 951.42633 accuracy= 0.61448 accuracy_s= 0.77991 accuracy_f= 0.86411 time= 0.02928
Epoch: 0151 train_loss= 914.04425 feature_loss= 5.76041 structure_loss= 908.28387 accuracy= 0.61226 accuracy_s= 0.77991 accuracy_f= 0.86484 time= 0.02638
Epoch: 0161 train_loss= 868.75781 feature_loss= 5.72480 structure_loss= 863.03302 accuracy= 0.61300 accuracy_s= 0.77991 accuracy_f= 0.86337 time= 0.02646
Epoch: 0171 train_loss= 830.52240 feature_loss= 5.69494 structure_loss= 824.82745 accuracy= 0.61078 accuracy_s= 0.77991 accuracy_f= 0.86854 time= 0.02668
Epoch: 0181 train_loss= 788.63513 feature_loss= 5.66749 structure_loss= 782.96765 accuracy= 0.61300 accuracy_s= 0.78065 accuracy_f= 0.86484 time= 0.02650
Epoch: 0191 train_loss= 754.61517 feature_loss= 5.63303 structure_loss= 748.98212 accuracy= 0.61448 accuracy_s= 0.77991 accuracy_f= 0.86558 time= 0.02687
Epoch: 0201 train_loss= 722.43671 feature_loss= 5.61776 structure_loss= 716.81897 accuracy= 0.60931 accuracy_s= 0.78065 accuracy_f= 0.86484 time= 0.02641
Epoch: 0211 train_loss= 686.18268 feature_loss= 5.58130 structure_loss= 680.60138 accuracy= 0.60857 accuracy_s= 0.78065 accuracy_f= 0.86411 time= 0.02647
Epoch: 0221 train_loss= 658.45135 feature_loss= 5.56582 structure_loss= 652.88556 accuracy= 0.60931 accuracy_s= 0.78065 accuracy_f= 0.86263 time= 0.02638
Epoch: 0231 train_loss= 630.74817 feature_loss= 5.54281 structure_loss= 625.20538 accuracy= 0.60931 accuracy_s= 0.78065 accuracy_f= 0.86484 time= 0.02640
Epoch: 0241 train_loss= 602.19049 feature_loss= 5.51920 structure_loss= 596.67126 accuracy= 0.60709 accuracy_s= 0.78139 accuracy_f= 0.86041 time= 0.02641
Epoch: 0251 train_loss= 578.52393 feature_loss= 5.49606 structure_loss= 573.02789 accuracy= 0.60783 accuracy_s= 0.78139 accuracy_f= 0.86115 time= 0.02643
Epoch: 0261 train_loss= 545.27759 feature_loss= 5.47157 structure_loss= 539.80603 accuracy= 0.60857 accuracy_s= 0.78065 accuracy_f= 0.86263 time= 0.02675
Epoch: 0271 train_loss= 526.43372 feature_loss= 5.45405 structure_loss= 520.97968 accuracy= 0.60635 accuracy_s= 0.78065 accuracy_f= 0.86041 time= 0.02670
Epoch: 0281 train_loss= 503.69434 feature_loss= 5.43476 structure_loss= 498.25958 accuracy= 0.60931 accuracy_s= 0.78065 accuracy_f= 0.86041 time= 0.02652
Epoch: 0291 train_loss= 480.06573 feature_loss= 5.40996 structure_loss= 474.65579 accuracy= 0.60931 accuracy_s= 0.78065 accuracy_f= 0.86337 time= 0.02648

accuracy 0.61004
accuracy_s 0.78065
accuracy_f 0.86189
auc 0.30757
f1_score 0.12000
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=3, beta=1, clique_size=20, cuda=False, dataset='cora', device=device(type='cuda'), dim_reduce=2, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=15, weight_decay=0.0005)
random seed: 389
Found existing ad data, loading...
feature_dim: 1433 hidden1_dim: 716 hidden2_dim: 358 nodes_num: 2708 anomaly_num: 600

Start modeling
using wavelet scattering transform
y_features shape after scatting (2708, 18629) <class 'numpy.ndarray'>
18629
y_features shape after NoReduction torch.Size([2708, 18629]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 3864.91943 feature_loss= 33.79625 structure_loss= 3831.12329 accuracy= 0.61374 accuracy_s= 0.78065 accuracy_f= 0.80798 time= 0.50476
Epoch: 0011 train_loss= 63.12211 feature_loss= 25.66239 structure_loss= 37.45972 accuracy= 0.64476 accuracy_s= 0.80207 accuracy_f= 0.80650 time= 0.48787
Epoch: 0021 train_loss= 53.65096 feature_loss= 19.57837 structure_loss= 34.07259 accuracy= 0.64032 accuracy_s= 0.80576 accuracy_f= 0.80133 time= 0.48936
Epoch: 0031 train_loss= 46.11080 feature_loss= 13.74469 structure_loss= 32.36611 accuracy= 0.65436 accuracy_s= 0.80798 accuracy_f= 0.79985 time= 0.48989
Epoch: 0041 train_loss= 39.64167 feature_loss= 9.05084 structure_loss= 30.59082 accuracy= 0.65731 accuracy_s= 0.80945 accuracy_f= 0.80059 time= 0.48991
Epoch: 0051 train_loss= 36.52698 feature_loss= 7.15243 structure_loss= 29.37455 accuracy= 0.66470 accuracy_s= 0.81093 accuracy_f= 0.80945 time= 0.49171
Epoch: 0061 train_loss= 34.14473 feature_loss= 6.44035 structure_loss= 27.70439 accuracy= 0.67061 accuracy_s= 0.80798 accuracy_f= 0.83087 time= 0.49128
Epoch: 0071 train_loss= 32.73579 feature_loss= 6.17919 structure_loss= 26.55660 accuracy= 0.67134 accuracy_s= 0.81167 accuracy_f= 0.83013 time= 0.49461
Epoch: 0081 train_loss= 30.61388 feature_loss= 5.81598 structure_loss= 24.79791 accuracy= 0.66987 accuracy_s= 0.81241 accuracy_f= 0.83530 time= 0.49397
Epoch: 0091 train_loss= 29.39928 feature_loss= 5.66247 structure_loss= 23.73681 accuracy= 0.66913 accuracy_s= 0.81536 accuracy_f= 0.83309 time= 0.49443
Epoch: 0101 train_loss= 26.54618 feature_loss= 5.58704 structure_loss= 20.95914 accuracy= 0.67356 accuracy_s= 0.81832 accuracy_f= 0.83087 time= 0.49521
Epoch: 0111 train_loss= 26.32223 feature_loss= 5.45057 structure_loss= 20.87166 accuracy= 0.67799 accuracy_s= 0.82127 accuracy_f= 0.83161 time= 0.49678
Epoch: 0121 train_loss= 25.03459 feature_loss= 5.46415 structure_loss= 19.57044 accuracy= 0.68464 accuracy_s= 0.82053 accuracy_f= 0.83087 time= 0.49443
Epoch: 0131 train_loss= 23.81568 feature_loss= 5.34767 structure_loss= 18.46801 accuracy= 0.68464 accuracy_s= 0.82496 accuracy_f= 0.83530 time= 0.49502
Epoch: 0141 train_loss= 23.06260 feature_loss= 5.41958 structure_loss= 17.64302 accuracy= 0.69424 accuracy_s= 0.82349 accuracy_f= 0.82792 time= 0.49799
Epoch: 0151 train_loss= 21.98406 feature_loss= 5.31981 structure_loss= 16.66426 accuracy= 0.69424 accuracy_s= 0.82496 accuracy_f= 0.83161 time= 0.49534
Epoch: 0161 train_loss= 21.11651 feature_loss= 5.22428 structure_loss= 15.89223 accuracy= 0.69719 accuracy_s= 0.82792 accuracy_f= 0.83826 time= 0.49590
Epoch: 0171 train_loss= 20.21106 feature_loss= 5.29622 structure_loss= 14.91485 accuracy= 0.69276 accuracy_s= 0.82792 accuracy_f= 0.83456 time= 0.49900
Epoch: 0181 train_loss= 19.68625 feature_loss= 5.26050 structure_loss= 14.42576 accuracy= 0.69055 accuracy_s= 0.82718 accuracy_f= 0.83309 time= 0.49932
Epoch: 0191 train_loss= 18.87555 feature_loss= 5.24529 structure_loss= 13.63026 accuracy= 0.68685 accuracy_s= 0.82644 accuracy_f= 0.83530 time= 0.50169
Epoch: 0201 train_loss= 18.26039 feature_loss= 5.18549 structure_loss= 13.07489 accuracy= 0.69202 accuracy_s= 0.82792 accuracy_f= 0.84047 time= 0.49890
Epoch: 0211 train_loss= 17.44435 feature_loss= 5.16633 structure_loss= 12.27802 accuracy= 0.68538 accuracy_s= 0.82053 accuracy_f= 0.83826 time= 0.50327
Epoch: 0221 train_loss= 16.90553 feature_loss= 5.17484 structure_loss= 11.73068 accuracy= 0.68685 accuracy_s= 0.82718 accuracy_f= 0.83604 time= 0.50089
Epoch: 0231 train_loss= 16.05825 feature_loss= 5.13092 structure_loss= 10.92733 accuracy= 0.69202 accuracy_s= 0.82570 accuracy_f= 0.83973 time= 0.50112
Epoch: 0241 train_loss= 15.82388 feature_loss= 5.14999 structure_loss= 10.67389 accuracy= 0.68981 accuracy_s= 0.82422 accuracy_f= 0.83900 time= 0.50115
Epoch: 0251 train_loss= 15.06908 feature_loss= 5.16886 structure_loss= 9.90022 accuracy= 0.68538 accuracy_s= 0.82496 accuracy_f= 0.83456 time= 0.50042
Epoch: 0261 train_loss= 14.81522 feature_loss= 5.22821 structure_loss= 9.58700 accuracy= 0.68538 accuracy_s= 0.82053 accuracy_f= 0.83235 time= 0.50221
Epoch: 0271 train_loss= 14.29342 feature_loss= 5.18971 structure_loss= 9.10372 accuracy= 0.68390 accuracy_s= 0.82275 accuracy_f= 0.83383 time= 0.50152
Epoch: 0281 train_loss= 13.88288 feature_loss= 5.20363 structure_loss= 8.67925 accuracy= 0.69055 accuracy_s= 0.82422 accuracy_f= 0.83013 time= 0.50348
Epoch: 0291 train_loss= 13.37089 feature_loss= 5.20039 structure_loss= 8.17050 accuracy= 0.68833 accuracy_s= 0.82422 accuracy_f= 0.83530 time= 0.50402

accuracy 0.69350
accuracy_s 0.82496
accuracy_f 0.83678
auc 0.60397
f1_score 0.30833
Job finished!
cora job finished!
citeseer experiment with fixed GCN hidden layer



Initializing normal onedecoder
Namespace(alpha=1, beta=1, clique_size=20, cuda=False, dataset='citeseer', decoder=0, device=device(type='cuda'), dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 282
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
Epoch: 0001 train_loss= 54.50385 accuracy= 0.58161 time= 0.07697
Epoch: 0011 train_loss= 7.76488 accuracy= 0.68200 time= 0.06374
Epoch: 0021 train_loss= 6.62263 accuracy= 0.68680 time= 0.06352
Epoch: 0031 train_loss= 6.57896 accuracy= 0.68921 time= 0.06339
Epoch: 0041 train_loss= 6.52239 accuracy= 0.69342 time= 0.06324
Epoch: 0051 train_loss= 6.48186 accuracy= 0.70183 time= 0.06397
Epoch: 0061 train_loss= 6.45141 accuracy= 0.70784 time= 0.06350
Epoch: 0071 train_loss= 6.41542 accuracy= 0.71265 time= 0.06371
Epoch: 0081 train_loss= 6.37480 accuracy= 0.70905 time= 0.06308
Epoch: 0091 train_loss= 6.33407 accuracy= 0.71566 time= 0.06329
Epoch: 0101 train_loss= 6.29891 accuracy= 0.72167 time= 0.06325
Epoch: 0111 train_loss= 6.27269 accuracy= 0.72287 time= 0.06367
Epoch: 0121 train_loss= 6.25080 accuracy= 0.72167 time= 0.06300
Epoch: 0131 train_loss= 6.23444 accuracy= 0.72047 time= 0.06322
Epoch: 0141 train_loss= 6.23011 accuracy= 0.72227 time= 0.06372
Epoch: 0151 train_loss= 6.27677 accuracy= 0.71746 time= 0.06361
Epoch: 0161 train_loss= 6.26092 accuracy= 0.72047 time= 0.06315
Epoch: 0171 train_loss= 6.22470 accuracy= 0.72347 time= 0.06341
Epoch: 0181 train_loss= 6.18859 accuracy= 0.72227 time= 0.06346
Epoch: 0191 train_loss= 6.25483 accuracy= 0.72287 time= 0.06343
Epoch: 0201 train_loss= 6.26214 accuracy= 0.72408 time= 0.06303
Epoch: 0211 train_loss= 6.18871 accuracy= 0.72468 time= 0.06321
Epoch: 0221 train_loss= 6.19354 accuracy= 0.71987 time= 0.06342
Epoch: 0231 train_loss= 6.16630 accuracy= 0.72347 time= 0.06327
Epoch: 0241 train_loss= 6.14577 accuracy= 0.72287 time= 0.06316
Epoch: 0251 train_loss= 6.08289 accuracy= 0.72287 time= 0.06324
Epoch: 0261 train_loss= 6.04361 accuracy= 0.72648 time= 0.06316
Epoch: 0271 train_loss= 6.05029 accuracy= 0.72287 time= 0.06325
Epoch: 0281 train_loss= 6.00344 accuracy= 0.72468 time= 0.06304
Epoch: 0291 train_loss= 5.99206 accuracy= 0.72828 time= 0.06326

accuracy 0.72528
auc 0.68847
f1_score 0.42875
Job finished!



Initializing normal onedecoder
Namespace(alpha=1, beta=1, clique_size=20, cuda=False, dataset='citeseer', decoder=1, device=device(type='cuda'), dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 278
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
Epoch: 0001 train_loss= 11021.24121 accuracy= 0.60625 time= 0.05877
Epoch: 0011 train_loss= 4962.67920 accuracy= 0.58100 time= 0.04801
Epoch: 0021 train_loss= 3841.42261 accuracy= 0.57800 time= 0.04818
Epoch: 0031 train_loss= 2996.48828 accuracy= 0.58161 time= 0.04812
Epoch: 0041 train_loss= 2305.92139 accuracy= 0.57619 time= 0.04816
Epoch: 0051 train_loss= 1870.97778 accuracy= 0.57800 time= 0.04771
Epoch: 0061 train_loss= 1713.98230 accuracy= 0.58521 time= 0.04836
Epoch: 0071 train_loss= 1639.59424 accuracy= 0.57800 time= 0.04755
Epoch: 0081 train_loss= 1590.70068 accuracy= 0.60505 time= 0.04731
Epoch: 0091 train_loss= 1537.51855 accuracy= 0.61948 time= 0.04744
Epoch: 0101 train_loss= 1496.30933 accuracy= 0.62008 time= 0.04735
Epoch: 0111 train_loss= 1455.17017 accuracy= 0.61707 time= 0.04708
Epoch: 0121 train_loss= 1414.84583 accuracy= 0.61767 time= 0.04756
Epoch: 0131 train_loss= 1382.53809 accuracy= 0.62008 time= 0.04770
Epoch: 0141 train_loss= 1339.37769 accuracy= 0.60625 time= 0.04716
Epoch: 0151 train_loss= 1308.44080 accuracy= 0.61347 time= 0.04746
Epoch: 0161 train_loss= 1276.41211 accuracy= 0.61226 time= 0.04697
Epoch: 0171 train_loss= 1237.17773 accuracy= 0.61347 time= 0.04744
Epoch: 0181 train_loss= 1203.01233 accuracy= 0.60505 time= 0.04730
Epoch: 0191 train_loss= 1160.82080 accuracy= 0.60685 time= 0.04704
Epoch: 0201 train_loss= 1132.48547 accuracy= 0.60806 time= 0.04739
Epoch: 0211 train_loss= 1094.89673 accuracy= 0.60024 time= 0.04729
Epoch: 0221 train_loss= 1059.23254 accuracy= 0.60325 time= 0.04710
Epoch: 0231 train_loss= 1024.91357 accuracy= 0.59363 time= 0.04737
Epoch: 0241 train_loss= 996.43457 accuracy= 0.60204 time= 0.04730
Epoch: 0251 train_loss= 963.28809 accuracy= 0.59844 time= 0.04725
Epoch: 0261 train_loss= 933.21375 accuracy= 0.60144 time= 0.04753
Epoch: 0271 train_loss= 901.38544 accuracy= 0.60625 time= 0.04700
Epoch: 0281 train_loss= 874.50104 accuracy= 0.59423 time= 0.04742
Epoch: 0291 train_loss= 847.34418 accuracy= 0.58822 time= 0.04747

accuracy 0.59663
auc 0.35400
f1_score 0.16125
Job finished!



Initializing normal twodecoders
Namespace(alpha=1, beta=1, clique_size=20, cuda=False, dataset='citeseer', device=device(type='cuda'), dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 280
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
Epoch: 0001 train_loss= 11851.12891 feature_loss= 54.29833 structure_loss= 11796.83105 accuracy= 0.60866 accuracy_s= 0.75954 accuracy_f= 0.78720 time= 0.10471
Epoch: 0011 train_loss= 4652.85791 feature_loss= 21.92698 structure_loss= 4630.93115 accuracy= 0.57920 accuracy_s= 0.76014 accuracy_f= 0.79561 time= 0.08875
Epoch: 0021 train_loss= 3493.25513 feature_loss= 11.74446 structure_loss= 3481.51074 accuracy= 0.58281 accuracy_s= 0.76014 accuracy_f= 0.81966 time= 0.08870
Epoch: 0031 train_loss= 2522.94092 feature_loss= 9.25583 structure_loss= 2513.68506 accuracy= 0.58221 accuracy_s= 0.76075 accuracy_f= 0.82567 time= 0.08845
Epoch: 0041 train_loss= 1917.59644 feature_loss= 8.40754 structure_loss= 1909.18884 accuracy= 0.58221 accuracy_s= 0.76135 accuracy_f= 0.82507 time= 0.08842
Epoch: 0051 train_loss= 1747.81616 feature_loss= 7.98872 structure_loss= 1739.82739 accuracy= 0.58100 accuracy_s= 0.76075 accuracy_f= 0.82627 time= 0.08719
Epoch: 0061 train_loss= 1675.26306 feature_loss= 7.65522 structure_loss= 1667.60779 accuracy= 0.59303 accuracy_s= 0.76014 accuracy_f= 0.82567 time= 0.08768
Epoch: 0071 train_loss= 1617.40344 feature_loss= 7.45607 structure_loss= 1609.94739 accuracy= 0.66216 accuracy_s= 0.77577 accuracy_f= 0.82627 time= 0.08800
Epoch: 0081 train_loss= 1571.67310 feature_loss= 7.32387 structure_loss= 1564.34924 accuracy= 0.65555 accuracy_s= 0.77036 accuracy_f= 0.82687 time= 0.08766
Epoch: 0091 train_loss= 1530.65186 feature_loss= 7.18187 structure_loss= 1523.46997 accuracy= 0.66817 accuracy_s= 0.77998 accuracy_f= 0.82627 time= 0.08721
Epoch: 0101 train_loss= 1493.77515 feature_loss= 7.04972 structure_loss= 1486.72546 accuracy= 0.67418 accuracy_s= 0.77517 accuracy_f= 0.82687 time= 0.08825
Epoch: 0111 train_loss= 1449.90454 feature_loss= 6.87985 structure_loss= 1443.02466 accuracy= 0.67178 accuracy_s= 0.77577 accuracy_f= 0.82627 time= 0.08746
Epoch: 0121 train_loss= 1410.27271 feature_loss= 6.64150 structure_loss= 1403.63123 accuracy= 0.66997 accuracy_s= 0.77397 accuracy_f= 0.82507 time= 0.08804
Epoch: 0131 train_loss= 1375.34583 feature_loss= 6.47024 structure_loss= 1368.87561 accuracy= 0.67598 accuracy_s= 0.77096 accuracy_f= 0.82627 time= 0.08790
Epoch: 0141 train_loss= 1337.34888 feature_loss= 6.45795 structure_loss= 1330.89087 accuracy= 0.66757 accuracy_s= 0.77277 accuracy_f= 0.82687 time= 0.08714
Epoch: 0151 train_loss= 1305.78162 feature_loss= 6.45357 structure_loss= 1299.32800 accuracy= 0.67358 accuracy_s= 0.77036 accuracy_f= 0.82567 time= 0.08730
Epoch: 0161 train_loss= 1265.02942 feature_loss= 6.45374 structure_loss= 1258.57568 accuracy= 0.66336 accuracy_s= 0.76916 accuracy_f= 0.82567 time= 0.08944
Epoch: 0171 train_loss= 1230.50476 feature_loss= 6.52229 structure_loss= 1223.98242 accuracy= 0.66276 accuracy_s= 0.76736 accuracy_f= 0.82086 time= 0.08750
Epoch: 0181 train_loss= 1193.67664 feature_loss= 6.45358 structure_loss= 1187.22302 accuracy= 0.66997 accuracy_s= 0.76736 accuracy_f= 0.82567 time= 0.08807
Epoch: 0191 train_loss= 1157.18384 feature_loss= 6.46026 structure_loss= 1150.72363 accuracy= 0.67478 accuracy_s= 0.77337 accuracy_f= 0.82567 time= 0.08739
Epoch: 0201 train_loss= 1118.36230 feature_loss= 6.54690 structure_loss= 1111.81543 accuracy= 0.66576 accuracy_s= 0.76736 accuracy_f= 0.81725 time= 0.08711
Epoch: 0211 train_loss= 1086.47791 feature_loss= 6.65682 structure_loss= 1079.82104 accuracy= 0.66576 accuracy_s= 0.76495 accuracy_f= 0.80884 time= 0.08751
Epoch: 0221 train_loss= 1052.84937 feature_loss= 6.62610 structure_loss= 1046.22327 accuracy= 0.67057 accuracy_s= 0.76916 accuracy_f= 0.81304 time= 0.08776
Epoch: 0231 train_loss= 1016.73364 feature_loss= 6.69887 structure_loss= 1010.03479 accuracy= 0.67418 accuracy_s= 0.76555 accuracy_f= 0.80703 time= 0.08790
Epoch: 0241 train_loss= 979.30487 feature_loss= 6.67517 structure_loss= 972.62970 accuracy= 0.66637 accuracy_s= 0.76195 accuracy_f= 0.81184 time= 0.08720
Epoch: 0251 train_loss= 947.67963 feature_loss= 6.52046 structure_loss= 941.15918 accuracy= 0.66637 accuracy_s= 0.76375 accuracy_f= 0.82266 time= 0.08713
Epoch: 0261 train_loss= 917.43976 feature_loss= 6.50412 structure_loss= 910.93567 accuracy= 0.66937 accuracy_s= 0.76495 accuracy_f= 0.82627 time= 0.08788
Epoch: 0271 train_loss= 885.13800 feature_loss= 6.60207 structure_loss= 878.53595 accuracy= 0.66637 accuracy_s= 0.76075 accuracy_f= 0.81906 time= 0.08787
Epoch: 0281 train_loss= 855.45935 feature_loss= 6.67355 structure_loss= 848.78583 accuracy= 0.65795 accuracy_s= 0.76315 accuracy_f= 0.81605 time= 0.08730
Epoch: 0291 train_loss= 829.72919 feature_loss= 6.65900 structure_loss= 823.07019 accuracy= 0.65735 accuracy_s= 0.76014 accuracy_f= 0.81906 time= 0.08768

accuracy 0.66336
accuracy_s 0.76135
accuracy_f 0.81906
auc 0.55884
f1_score 0.30000
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=20, cuda=False, dataset='citeseer', decoder=1, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 900
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([3327, 925]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 7677.95996 accuracy= 0.79381 time= 0.03366
Epoch: 0011 train_loss= 4371.70410 accuracy= 0.59002 time= 0.02518
Epoch: 0021 train_loss= 3700.30249 accuracy= 0.58401 time= 0.02532
Epoch: 0031 train_loss= 3313.11572 accuracy= 0.58281 time= 0.02496
Epoch: 0041 train_loss= 3040.13843 accuracy= 0.58281 time= 0.02495
Epoch: 0051 train_loss= 2828.76733 accuracy= 0.58281 time= 0.02429
Epoch: 0061 train_loss= 2650.87769 accuracy= 0.58281 time= 0.02465
Epoch: 0071 train_loss= 2499.18555 accuracy= 0.58341 time= 0.02539
Epoch: 0081 train_loss= 2365.86304 accuracy= 0.58401 time= 0.02493
Epoch: 0091 train_loss= 2245.63232 accuracy= 0.58461 time= 0.02444
Epoch: 0101 train_loss= 2135.32251 accuracy= 0.58341 time= 0.02420
Epoch: 0111 train_loss= 2033.65613 accuracy= 0.58401 time= 0.02468
Epoch: 0121 train_loss= 1935.73389 accuracy= 0.58281 time= 0.02529
Epoch: 0131 train_loss= 1846.11963 accuracy= 0.58281 time= 0.02486
Epoch: 0141 train_loss= 1764.49231 accuracy= 0.58461 time= 0.02472
Epoch: 0151 train_loss= 1685.88037 accuracy= 0.58521 time= 0.02417
Epoch: 0161 train_loss= 1610.35828 accuracy= 0.58641 time= 0.02490
Epoch: 0171 train_loss= 1536.97778 accuracy= 0.58762 time= 0.02508
Epoch: 0181 train_loss= 1470.01855 accuracy= 0.58702 time= 0.02489
Epoch: 0191 train_loss= 1405.86121 accuracy= 0.58581 time= 0.02598
Epoch: 0201 train_loss= 1342.18933 accuracy= 0.58581 time= 0.02439
Epoch: 0211 train_loss= 1285.17090 accuracy= 0.58581 time= 0.02526
Epoch: 0221 train_loss= 1227.00244 accuracy= 0.58521 time= 0.02489
Epoch: 0231 train_loss= 1171.63208 accuracy= 0.58641 time= 0.02442
Epoch: 0241 train_loss= 1122.25415 accuracy= 0.58521 time= 0.02438
Epoch: 0251 train_loss= 1071.81665 accuracy= 0.58401 time= 0.02527
Epoch: 0261 train_loss= 1023.82135 accuracy= 0.58341 time= 0.02509
Epoch: 0271 train_loss= 977.29248 accuracy= 0.58281 time= 0.02469
Epoch: 0281 train_loss= 934.82233 accuracy= 0.58221 time= 0.02429
Epoch: 0291 train_loss= 891.48462 accuracy= 0.58281 time= 0.02475

accuracy 0.58100
auc 0.30736
f1_score 0.12875
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=20, cuda=False, dataset='citeseer', decoder=0, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 514
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([3327, 925]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 59.70675 accuracy= 0.60986 time= 0.05192
Epoch: 0011 train_loss= 24.42258 accuracy= 0.62549 time= 0.04017
Epoch: 0021 train_loss= 15.03915 accuracy= 0.65675 time= 0.03983
Epoch: 0031 train_loss= 11.34914 accuracy= 0.71326 time= 0.03999
Epoch: 0041 train_loss= 9.54940 accuracy= 0.74271 time= 0.03982
Epoch: 0051 train_loss= 8.80786 accuracy= 0.74932 time= 0.03975
Epoch: 0061 train_loss= 8.10707 accuracy= 0.75594 time= 0.03958
Epoch: 0071 train_loss= 7.42913 accuracy= 0.76135 time= 0.03969
Epoch: 0081 train_loss= 6.76394 accuracy= 0.76135 time= 0.03927
Epoch: 0091 train_loss= 6.34087 accuracy= 0.75894 time= 0.03972
Epoch: 0101 train_loss= 6.13663 accuracy= 0.76315 time= 0.03945
Epoch: 0111 train_loss= 5.97138 accuracy= 0.76856 time= 0.03942
Epoch: 0121 train_loss= 5.40609 accuracy= 0.77457 time= 0.03960
Epoch: 0131 train_loss= 5.26273 accuracy= 0.77577 time= 0.03954
Epoch: 0141 train_loss= 5.21484 accuracy= 0.77577 time= 0.03934
Epoch: 0151 train_loss= 5.15286 accuracy= 0.77337 time= 0.03987
Epoch: 0161 train_loss= 5.11500 accuracy= 0.77577 time= 0.03966
Epoch: 0171 train_loss= 5.17202 accuracy= 0.76916 time= 0.03952
Epoch: 0181 train_loss= 5.11289 accuracy= 0.76856 time= 0.03958
Epoch: 0191 train_loss= 5.06501 accuracy= 0.77036 time= 0.04003
Epoch: 0201 train_loss= 5.04156 accuracy= 0.77157 time= 0.03938
Epoch: 0211 train_loss= 5.01840 accuracy= 0.76916 time= 0.04128
Epoch: 0221 train_loss= 5.02965 accuracy= 0.76976 time= 0.03969
Epoch: 0231 train_loss= 5.03964 accuracy= 0.76676 time= 0.03919
Epoch: 0241 train_loss= 5.05010 accuracy= 0.77337 time= 0.03965
Epoch: 0251 train_loss= 5.00576 accuracy= 0.76255 time= 0.03923
Epoch: 0261 train_loss= 4.99808 accuracy= 0.77337 time= 0.03967
Epoch: 0271 train_loss= 4.99036 accuracy= 0.77096 time= 0.03927
Epoch: 0281 train_loss= 4.97950 accuracy= 0.77397 time= 0.03971
Epoch: 0291 train_loss= 5.07375 accuracy= 0.77397 time= 0.03931

accuracy 0.75894
auc 0.72615
f1_score 0.49875
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=20, cuda=False, dataset='citeseer', decoder=1, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 614
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 238 steps!

y_features shape after FMS torch.Size([3327, 925]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 7412.57910 accuracy= 0.79621 time= 0.03912
Epoch: 0011 train_loss= 4441.48047 accuracy= 0.59182 time= 0.02333
Epoch: 0021 train_loss= 3761.19995 accuracy= 0.58341 time= 0.02364
Epoch: 0031 train_loss= 3348.48877 accuracy= 0.58461 time= 0.02328
Epoch: 0041 train_loss= 3050.95825 accuracy= 0.58100 time= 0.02338
Epoch: 0051 train_loss= 2819.91821 accuracy= 0.58040 time= 0.02392
Epoch: 0061 train_loss= 2629.01367 accuracy= 0.58221 time= 0.02324
Epoch: 0071 train_loss= 2463.30493 accuracy= 0.58401 time= 0.02369
Epoch: 0081 train_loss= 2315.74219 accuracy= 0.58461 time= 0.02336
Epoch: 0091 train_loss= 2181.86353 accuracy= 0.58581 time= 0.02335
Epoch: 0101 train_loss= 2056.81299 accuracy= 0.58581 time= 0.02395
Epoch: 0111 train_loss= 1945.88733 accuracy= 0.58822 time= 0.02322
Epoch: 0121 train_loss= 1942.82434 accuracy= 0.58942 time= 0.02358
Epoch: 0131 train_loss= 1824.31665 accuracy= 0.58762 time= 0.02371
Epoch: 0141 train_loss= 1697.70459 accuracy= 0.58822 time= 0.02321
Epoch: 0151 train_loss= 1593.70996 accuracy= 0.58641 time= 0.02389
Epoch: 0161 train_loss= 1502.44971 accuracy= 0.58762 time= 0.02306
Epoch: 0171 train_loss= 1414.97144 accuracy= 0.58641 time= 0.02355
Epoch: 0181 train_loss= 1322.67358 accuracy= 0.58521 time= 0.02389
Epoch: 0191 train_loss= 1803.16248 accuracy= 0.64472 time= 0.02321
Epoch: 0201 train_loss= 1380.89807 accuracy= 0.60325 time= 0.02365
Epoch: 0211 train_loss= 1210.64648 accuracy= 0.58401 time= 0.02358
Epoch: 0221 train_loss= 1092.71130 accuracy= 0.58341 time= 0.02335
Epoch: 0231 train_loss= 993.05328 accuracy= 0.58461 time= 0.02407
Epoch: 0241 train_loss= 901.27777 accuracy= 0.58461 time= 0.02329
Epoch: 0251 train_loss= 1263.28857 accuracy= 0.66697 time= 0.02368
Epoch: 0261 train_loss= 1000.74561 accuracy= 0.58521 time= 0.02357
Epoch: 0271 train_loss= 846.67706 accuracy= 0.58401 time= 0.02334
Epoch: 0281 train_loss= 733.19446 accuracy= 0.58221 time= 0.02390
Epoch: 0291 train_loss= 639.30029 accuracy= 0.58221 time= 0.02339

accuracy 0.59603
auc 0.40089
f1_score 0.16000
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=20, cuda=False, dataset='citeseer', decoder=0, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 853
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 283 steps!

y_features shape after FMS torch.Size([3327, 925]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 59.70331 accuracy= 0.61286 time= 0.05169
Epoch: 0011 train_loss= 29.12969 accuracy= 0.63090 time= 0.03873
Epoch: 0021 train_loss= 17.44403 accuracy= 0.65374 time= 0.03914
Epoch: 0031 train_loss= 12.39044 accuracy= 0.70243 time= 0.03918
Epoch: 0041 train_loss= 10.11388 accuracy= 0.73189 time= 0.03875
Epoch: 0051 train_loss= 8.92447 accuracy= 0.74391 time= 0.03850
Epoch: 0061 train_loss= 8.29367 accuracy= 0.74151 time= 0.03869
Epoch: 0071 train_loss= 7.67380 accuracy= 0.75413 time= 0.03831
Epoch: 0081 train_loss= 7.06897 accuracy= 0.74932 time= 0.03833
Epoch: 0091 train_loss= 6.39157 accuracy= 0.76195 time= 0.03784
Epoch: 0101 train_loss= 5.75925 accuracy= 0.75834 time= 0.03781
Epoch: 0111 train_loss= 5.47797 accuracy= 0.76495 time= 0.03804
Epoch: 0121 train_loss= 5.34191 accuracy= 0.76555 time= 0.03834
Epoch: 0131 train_loss= 5.23470 accuracy= 0.76555 time= 0.03821
Epoch: 0141 train_loss= 5.18054 accuracy= 0.76916 time= 0.03803
Epoch: 0151 train_loss= 5.15887 accuracy= 0.76495 time= 0.03774
Epoch: 0161 train_loss= 5.12956 accuracy= 0.76976 time= 0.03836
Epoch: 0171 train_loss= 5.12468 accuracy= 0.76255 time= 0.03819
Epoch: 0181 train_loss= 5.07014 accuracy= 0.76676 time= 0.03819
Epoch: 0191 train_loss= 5.06616 accuracy= 0.76555 time= 0.03781
Epoch: 0201 train_loss= 5.12250 accuracy= 0.75894 time= 0.03820
Epoch: 0211 train_loss= 5.11021 accuracy= 0.75834 time= 0.03822
Epoch: 0221 train_loss= 5.00721 accuracy= 0.76616 time= 0.03832
Epoch: 0231 train_loss= 5.00485 accuracy= 0.76555 time= 0.03802
Epoch: 0241 train_loss= 4.99064 accuracy= 0.76495 time= 0.03763
Epoch: 0251 train_loss= 4.98792 accuracy= 0.76976 time= 0.03839
Epoch: 0261 train_loss= 4.96908 accuracy= 0.76796 time= 0.03827
Epoch: 0271 train_loss= 4.96951 accuracy= 0.76856 time= 0.03814
Epoch: 0281 train_loss= 4.95621 accuracy= 0.77277 time= 0.03774
Epoch: 0291 train_loss= 4.94238 accuracy= 0.76435 time= 0.03808

accuracy 0.76976
auc 0.74009
f1_score 0.52125
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=20, cuda=False, dataset='citeseer', decoder=1, device=device(type='cuda'), dim_reduce=2, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 21
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>

y_features shape after NoReduction torch.Size([3327, 48139]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 7683.79932 accuracy= 0.58942 time= 0.14915
Epoch: 0011 train_loss= 17535.52148 accuracy= 0.60866 time= 0.12822
Epoch: 0021 train_loss= 11754.87891 accuracy= 0.60144 time= 0.13032
Epoch: 0031 train_loss= 8303.24219 accuracy= 0.59483 time= 0.12927
Epoch: 0041 train_loss= 5924.93799 accuracy= 0.59363 time= 0.12756
Epoch: 0051 train_loss= 4285.28906 accuracy= 0.59002 time= 0.12929
Epoch: 0061 train_loss= 3187.22778 accuracy= 0.58942 time= 0.12721
Epoch: 0071 train_loss= 2416.68750 accuracy= 0.58762 time= 0.12852
Epoch: 0081 train_loss= 1870.25342 accuracy= 0.59844 time= 0.12717
Epoch: 0091 train_loss= 1497.75500 accuracy= 0.59363 time= 0.12806
Epoch: 0101 train_loss= 1234.30798 accuracy= 0.59182 time= 0.12882
Epoch: 0111 train_loss= 1049.46960 accuracy= 0.59303 time= 0.12869
Epoch: 0121 train_loss= 919.90326 accuracy= 0.59062 time= 0.13029
Epoch: 0131 train_loss= 824.30719 accuracy= 0.59122 time= 0.12887
Epoch: 0141 train_loss= 747.12506 accuracy= 0.59122 time= 0.12791
Epoch: 0151 train_loss= 684.56567 accuracy= 0.59243 time= 0.12966
Epoch: 0161 train_loss= 625.94055 accuracy= 0.59543 time= 0.12914
Epoch: 0171 train_loss= 577.10431 accuracy= 0.59423 time= 0.12759
Epoch: 0181 train_loss= 533.45520 accuracy= 0.59423 time= 0.12957
Epoch: 0191 train_loss= 501.53653 accuracy= 0.59303 time= 0.12875
Epoch: 0201 train_loss= 462.86484 accuracy= 0.59122 time= 0.13007
Epoch: 0211 train_loss= 431.80164 accuracy= 0.59122 time= 0.12868
Epoch: 0221 train_loss= 398.64999 accuracy= 0.59303 time= 0.12809
Epoch: 0231 train_loss= 366.61093 accuracy= 0.59603 time= 0.12925
Epoch: 0241 train_loss= 341.30356 accuracy= 0.59663 time= 0.12987
Epoch: 0251 train_loss= 309.24451 accuracy= 0.59964 time= 0.12875
Epoch: 0261 train_loss= 285.79865 accuracy= 0.60024 time= 0.12711
Epoch: 0271 train_loss= 272.22183 accuracy= 0.59122 time= 0.12945
Epoch: 0281 train_loss= 257.40503 accuracy= 0.59062 time= 0.12711
Epoch: 0291 train_loss= 242.75212 accuracy= 0.58762 time= 0.13094

accuracy 0.58822
auc 0.26592
f1_score 0.14375
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=20, cuda=False, dataset='citeseer', decoder=0, device=device(type='cuda'), dim_reduce=2, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 710
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>

y_features shape after NoReduction torch.Size([3327, 48139]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 57.26781 accuracy= 0.58401 time= 0.16636
Epoch: 0011 train_loss= 12.11159 accuracy= 0.68440 time= 0.14670
Epoch: 0021 train_loss= 9.57331 accuracy= 0.68680 time= 0.14417
Epoch: 0031 train_loss= 8.80381 accuracy= 0.68921 time= 0.14565
Epoch: 0041 train_loss= 8.49625 accuracy= 0.69823 time= 0.14517
Epoch: 0051 train_loss= 8.26990 accuracy= 0.70664 time= 0.14533
Epoch: 0061 train_loss= 8.10150 accuracy= 0.72047 time= 0.14567
Epoch: 0071 train_loss= 7.95078 accuracy= 0.72528 time= 0.14437
Epoch: 0081 train_loss= 7.82853 accuracy= 0.73309 time= 0.14549
Epoch: 0091 train_loss= 7.70272 accuracy= 0.73850 time= 0.14498
Epoch: 0101 train_loss= 7.52889 accuracy= 0.73069 time= 0.14729
Epoch: 0111 train_loss= 7.30489 accuracy= 0.73790 time= 0.14660
Epoch: 0121 train_loss= 7.06318 accuracy= 0.73430 time= 0.14617
Epoch: 0131 train_loss= 7.08986 accuracy= 0.70664 time= 0.14755
Epoch: 0141 train_loss= 7.05241 accuracy= 0.72588 time= 0.14670
Epoch: 0151 train_loss= 7.13540 accuracy= 0.71506 time= 0.14648
Epoch: 0161 train_loss= 6.90935 accuracy= 0.71746 time= 0.14660
Epoch: 0171 train_loss= 6.58684 accuracy= 0.72408 time= 0.14671
Epoch: 0181 train_loss= 6.28148 accuracy= 0.72828 time= 0.14595
Epoch: 0191 train_loss= 6.07292 accuracy= 0.72768 time= 0.14787
Epoch: 0201 train_loss= 5.88379 accuracy= 0.72167 time= 0.14698
Epoch: 0211 train_loss= 5.77136 accuracy= 0.72047 time= 0.14886
Epoch: 0221 train_loss= 5.67399 accuracy= 0.72167 time= 0.14881
Epoch: 0231 train_loss= 5.62304 accuracy= 0.71987 time= 0.14710
Epoch: 0241 train_loss= 5.55720 accuracy= 0.72468 time= 0.14750
Epoch: 0251 train_loss= 5.54343 accuracy= 0.71806 time= 0.14820
Epoch: 0261 train_loss= 5.53580 accuracy= 0.72287 time= 0.14816
Epoch: 0271 train_loss= 5.60845 accuracy= 0.71265 time= 0.14732
Epoch: 0281 train_loss= 5.51980 accuracy= 0.72347 time= 0.14908
Epoch: 0291 train_loss= 5.48466 accuracy= 0.71987 time= 0.14833

accuracy 0.72167
auc 0.68028
f1_score 0.42125
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=20, cuda=False, dataset='citeseer', decoder=1, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 125
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([3327, 925]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 360.98138 accuracy= 0.78299 time= 0.05398
Epoch: 0011 train_loss= 17.89653 accuracy= 0.58521 time= 0.04055
Epoch: 0021 train_loss= 1.26829 accuracy= 0.58942 time= 0.04022
Epoch: 0031 train_loss= 0.53166 accuracy= 0.58702 time= 0.03972
Epoch: 0041 train_loss= 0.50167 accuracy= 0.58581 time= 0.03996
Epoch: 0051 train_loss= 0.50009 accuracy= 0.59243 time= 0.04001
Epoch: 0061 train_loss= 0.49322 accuracy= 0.58581 time= 0.04008
Epoch: 0071 train_loss= 0.48626 accuracy= 0.58281 time= 0.04013
Epoch: 0081 train_loss= 0.48017 accuracy= 0.58641 time= 0.04036
Epoch: 0091 train_loss= 0.47384 accuracy= 0.58581 time= 0.03994
Epoch: 0101 train_loss= 0.46874 accuracy= 0.58521 time= 0.03989
Epoch: 0111 train_loss= 0.46498 accuracy= 0.58762 time= 0.04009
Epoch: 0121 train_loss= 0.46067 accuracy= 0.58942 time= 0.04038
Epoch: 0131 train_loss= 0.45731 accuracy= 0.59002 time= 0.04037
Epoch: 0141 train_loss= 0.45474 accuracy= 0.58942 time= 0.04002
Epoch: 0151 train_loss= 0.45204 accuracy= 0.59002 time= 0.03993
Epoch: 0161 train_loss= 0.45010 accuracy= 0.58942 time= 0.03996
Epoch: 0171 train_loss= 0.44654 accuracy= 0.58882 time= 0.04029
Epoch: 0181 train_loss= 0.44545 accuracy= 0.58882 time= 0.04035
Epoch: 0191 train_loss= 0.44317 accuracy= 0.58762 time= 0.04019
Epoch: 0201 train_loss= 0.44105 accuracy= 0.58581 time= 0.04011
Epoch: 0211 train_loss= 0.43934 accuracy= 0.58641 time= 0.04007
Epoch: 0221 train_loss= 0.43759 accuracy= 0.58581 time= 0.04002
Epoch: 0231 train_loss= 0.43416 accuracy= 0.58581 time= 0.04029
Epoch: 0241 train_loss= 0.43336 accuracy= 0.58702 time= 0.04038
Epoch: 0251 train_loss= 0.43137 accuracy= 0.58581 time= 0.04017
Epoch: 0261 train_loss= 0.42972 accuracy= 0.58641 time= 0.03991
Epoch: 0271 train_loss= 0.42784 accuracy= 0.58461 time= 0.04034
Epoch: 0281 train_loss= 0.42524 accuracy= 0.58461 time= 0.04038
Epoch: 0291 train_loss= 0.42399 accuracy= 0.58521 time= 0.04036

accuracy 0.58401
auc 0.29970
f1_score 0.13500
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=20, cuda=False, dataset='citeseer', decoder=0, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 187
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([3327, 925]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 59.30234 accuracy= 0.62489 time= 0.07666
Epoch: 0011 train_loss= 7.87902 accuracy= 0.68380 time= 0.05482
Epoch: 0021 train_loss= 7.63500 accuracy= 0.68440 time= 0.05482
Epoch: 0031 train_loss= 7.62222 accuracy= 0.68500 time= 0.05508
Epoch: 0041 train_loss= 7.58385 accuracy= 0.68500 time= 0.05527
Epoch: 0051 train_loss= 7.54935 accuracy= 0.68500 time= 0.05510
Epoch: 0061 train_loss= 7.52574 accuracy= 0.68500 time= 0.05487
Epoch: 0071 train_loss= 7.50185 accuracy= 0.68440 time= 0.05503
Epoch: 0081 train_loss= 7.47984 accuracy= 0.68380 time= 0.05539
Epoch: 0091 train_loss= 7.45751 accuracy= 0.68440 time= 0.05527
Epoch: 0101 train_loss= 7.43561 accuracy= 0.68500 time= 0.05510
Epoch: 0111 train_loss= 7.41503 accuracy= 0.68560 time= 0.05535
Epoch: 0121 train_loss= 6.90787 accuracy= 0.67659 time= 0.05532
Epoch: 0131 train_loss= 6.83551 accuracy= 0.68380 time= 0.05501
Epoch: 0141 train_loss= 6.82056 accuracy= 0.68380 time= 0.05524
Epoch: 0151 train_loss= 6.80375 accuracy= 0.68440 time= 0.05533
Epoch: 0161 train_loss= 6.78904 accuracy= 0.68440 time= 0.05502
Epoch: 0171 train_loss= 6.77452 accuracy= 0.68500 time= 0.05527
Epoch: 0181 train_loss= 6.76456 accuracy= 0.68440 time= 0.05558
Epoch: 0191 train_loss= 6.91809 accuracy= 0.68320 time= 0.05498
Epoch: 0201 train_loss= 7.11442 accuracy= 0.67659 time= 0.05537
Epoch: 0211 train_loss= 7.06351 accuracy= 0.67779 time= 0.05535
Epoch: 0221 train_loss= 23.36937 accuracy= 0.65795 time= 0.05546
Epoch: 0231 train_loss= 6.44527 accuracy= 0.68500 time= 0.05585
Epoch: 0241 train_loss= 6.34243 accuracy= 0.68320 time= 0.05543
Epoch: 0251 train_loss= 6.33358 accuracy= 0.68440 time= 0.05576
Epoch: 0261 train_loss= 6.30720 accuracy= 0.68680 time= 0.05542
Epoch: 0271 train_loss= 6.28459 accuracy= 0.68440 time= 0.05578
Epoch: 0281 train_loss= 6.26763 accuracy= 0.68440 time= 0.05570
Epoch: 0291 train_loss= 6.25108 accuracy= 0.68440 time= 0.05545

accuracy 0.68380
auc 0.60558
f1_score 0.34250
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=20, cuda=False, dataset='citeseer', decoder=1, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 548
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 292 steps!

y_features shape after FMS torch.Size([3327, 925]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 0.53703 accuracy= 0.61286 time= 0.05494
Epoch: 0011 train_loss= 0.47652 accuracy= 0.58641 time= 0.03959
Epoch: 0021 train_loss= 0.48257 accuracy= 0.58702 time= 0.03917
Epoch: 0031 train_loss= 0.48474 accuracy= 0.58702 time= 0.03900
Epoch: 0041 train_loss= 0.48760 accuracy= 0.58641 time= 0.03932
Epoch: 0051 train_loss= 0.48912 accuracy= 0.58702 time= 0.03916
Epoch: 0061 train_loss= 0.49033 accuracy= 0.58641 time= 0.03882
Epoch: 0071 train_loss= 0.49118 accuracy= 0.58641 time= 0.03938
Epoch: 0081 train_loss= 0.49184 accuracy= 0.58702 time= 0.03917
Epoch: 0091 train_loss= 0.49233 accuracy= 0.58581 time= 0.04069
Epoch: 0101 train_loss= 0.49274 accuracy= 0.58641 time= 0.03933
Epoch: 0111 train_loss= 0.49310 accuracy= 0.58641 time= 0.04034
Epoch: 0121 train_loss= 0.49336 accuracy= 0.58702 time= 0.03946
Epoch: 0131 train_loss= 0.49361 accuracy= 0.58702 time= 0.04019
Epoch: 0141 train_loss= 0.49382 accuracy= 0.58702 time= 0.03898
Epoch: 0151 train_loss= 0.49401 accuracy= 0.58702 time= 0.03950
Epoch: 0161 train_loss= 0.49422 accuracy= 0.58641 time= 0.03935
Epoch: 0171 train_loss= 0.49442 accuracy= 0.58641 time= 0.03923
Epoch: 0181 train_loss= 0.49460 accuracy= 0.58641 time= 0.03947
Epoch: 0191 train_loss= 0.49478 accuracy= 0.58641 time= 0.03923
Epoch: 0201 train_loss= 0.49497 accuracy= 0.58702 time= 0.03943
Epoch: 0211 train_loss= 0.49510 accuracy= 0.58702 time= 0.03959
Epoch: 0221 train_loss= 0.49524 accuracy= 0.58702 time= 0.03906
Epoch: 0231 train_loss= 0.49539 accuracy= 0.58702 time= 0.03949
Epoch: 0241 train_loss= 0.49557 accuracy= 0.58702 time= 0.03912
Epoch: 0251 train_loss= 0.49572 accuracy= 0.58762 time= 0.03955
Epoch: 0261 train_loss= 0.49584 accuracy= 0.58702 time= 0.03936
Epoch: 0271 train_loss= 0.49603 accuracy= 0.58702 time= 0.03936
Epoch: 0281 train_loss= 0.49618 accuracy= 0.58702 time= 0.03969
Epoch: 0291 train_loss= 0.49637 accuracy= 0.58702 time= 0.03909

accuracy 0.58702
auc 0.25461
f1_score 0.14125
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=20, cuda=False, dataset='citeseer', decoder=0, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 40
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 245 steps!

y_features shape after FMS torch.Size([3327, 925]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 40.06550 accuracy= 0.62609 time= 0.07155
Epoch: 0011 train_loss= 5.58163 accuracy= 0.68320 time= 0.05272
Epoch: 0021 train_loss= 5.55414 accuracy= 0.68500 time= 0.05610
Epoch: 0031 train_loss= 5.53850 accuracy= 0.68500 time= 0.05231
Epoch: 0041 train_loss= 5.52896 accuracy= 0.68500 time= 0.05247
Epoch: 0051 train_loss= 5.52302 accuracy= 0.68440 time= 0.05313
Epoch: 0061 train_loss= 5.51929 accuracy= 0.68500 time= 0.05305
Epoch: 0071 train_loss= 5.51692 accuracy= 0.68560 time= 0.05302
Epoch: 0081 train_loss= 5.51540 accuracy= 0.68560 time= 0.05292
Epoch: 0091 train_loss= 5.51444 accuracy= 0.68560 time= 0.05302
Epoch: 0101 train_loss= 5.51383 accuracy= 0.68560 time= 0.05266
Epoch: 0111 train_loss= 5.51345 accuracy= 0.68620 time= 0.05301
Epoch: 0121 train_loss= 5.51321 accuracy= 0.68620 time= 0.05290
Epoch: 0131 train_loss= 5.51307 accuracy= 0.68560 time= 0.05302
Epoch: 0141 train_loss= 5.51298 accuracy= 0.68560 time= 0.05306
Epoch: 0151 train_loss= 5.51293 accuracy= 0.68440 time= 0.05290
Epoch: 0161 train_loss= 5.51290 accuracy= 0.68440 time= 0.05279
Epoch: 0171 train_loss= 5.51288 accuracy= 0.68440 time= 0.05302
Epoch: 0181 train_loss= 5.51287 accuracy= 0.68440 time= 0.05292
Epoch: 0191 train_loss= 5.51286 accuracy= 0.68440 time= 0.05293
Epoch: 0201 train_loss= 5.51286 accuracy= 0.68440 time= 0.05281
Epoch: 0211 train_loss= 5.51286 accuracy= 0.68440 time= 0.05376
Epoch: 0221 train_loss= 5.51286 accuracy= 0.68440 time= 0.05276
Epoch: 0231 train_loss= 5.51286 accuracy= 0.68440 time= 0.05314
Epoch: 0241 train_loss= 5.51286 accuracy= 0.68440 time= 0.05306
Epoch: 0251 train_loss= 5.51286 accuracy= 0.68440 time= 0.05298
Epoch: 0261 train_loss= 5.51286 accuracy= 0.68440 time= 0.05327
Epoch: 0271 train_loss= 5.51286 accuracy= 0.68440 time= 0.05300
Epoch: 0281 train_loss= 5.51286 accuracy= 0.68440 time= 0.05305
Epoch: 0291 train_loss= 5.51286 accuracy= 0.68440 time= 0.05289

accuracy 0.68440
auc 0.60814
f1_score 0.34375
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=20, cuda=False, dataset='citeseer', decoder=1, device=device(type='cuda'), dim_reduce=2, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 821
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>

y_features shape after NoReduction torch.Size([3327, 48139]) <class 'torch.Tensor'>
Traceback (most recent call last):
  File "train_scat_onedecoder.py", line 247, in <module>
    gae_ad(args)
  File "train_scat_onedecoder.py", line 173, in gae_ad
    optimizer.step()
  File "/home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 15, in decorate_context
    return func(*args, **kwargs)
  File "/home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/optim/adam.py", line 107, in step
    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])
RuntimeError: CUDA out of memory. Tried to allocate 340.00 MiB (GPU 0; 23.65 GiB total capacity; 20.98 GiB already allocated; 112.00 MiB free; 22.58 GiB reserved in total by PyTorch)



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=20, cuda=False, dataset='citeseer', decoder=0, device=device(type='cuda'), dim_reduce=2, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 810
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>

y_features shape after NoReduction torch.Size([3327, 48139]) <class 'torch.Tensor'>
Traceback (most recent call last):
  File "train_scat_onedecoder.py", line 247, in <module>
    gae_ad(args)
  File "train_scat_onedecoder.py", line 171, in gae_ad
    total_loss.backward()
  File "/home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/tensor.py", line 185, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/autograd/__init__.py", line 125, in backward
    Variable._execution_engine.run_backward(
RuntimeError: CUDA out of memory. Tried to allocate 612.00 MiB (GPU 0; 23.65 GiB total capacity; 21.81 GiB already allocated; 596.00 MiB free; 22.11 GiB reserved in total by PyTorch)
Exception raised from malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:272 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7f144d09f1e2 in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1e64b (0x7f144d2f564b in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libc10_cuda.so)
frame #2: <unknown function> + 0x1f464 (0x7f144d2f6464 in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0x1faa1 (0x7f144d2f6aa1 in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libc10_cuda.so)
frame #4: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x11e (0x7f145001d52e in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xf51329 (0x7f144e459329 in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xf6b157 (0x7f144e473157 in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0x10e9c7d (0x7f14851ddc7d in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #8: <unknown function> + 0x10e9f97 (0x7f14851ddf97 in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #9: at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0xfa (0x7f14852e8a1a in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #10: at::native::mm_cuda(at::Tensor const&, at::Tensor const&) + 0x6c (0x7f144f50e8ac in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xf40400 (0x7f144e448400 in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so)
frame #12: <unknown function> + 0xa56530 (0x7f1484b4a530 in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #13: at::Tensor c10::Dispatcher::call<at::Tensor, at::Tensor const&, at::Tensor const&>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&)> const&, at::Tensor const&, at::Tensor const&) const + 0xbc (0x7f148533281c in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #14: at::mm(at::Tensor const&, at::Tensor const&) + 0x4b (0x7f14852836ab in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #15: <unknown function> + 0x2ed0a2f (0x7f1486fc4a2f in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0xa56530 (0x7f1484b4a530 in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #17: at::Tensor c10::Dispatcher::call<at::Tensor, at::Tensor const&, at::Tensor const&>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&)> const&, at::Tensor const&, at::Tensor const&) const + 0xbc (0x7f148533281c in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #18: at::Tensor::mm(at::Tensor const&) const + 0x4b (0x7f1485418cab in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0x2d11c34 (0x7f1486e05c34 in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #20: torch::autograd::generated::MmBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x294 (0x7f1486e21814 in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #21: <unknown function> + 0x3375bb7 (0x7f1487469bb7 in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #22: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x1400 (0x7f1487465400 in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #23: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7f1487465fa1 in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #24: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x89 (0x7f148745e119 in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #25: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x4a (0x7f1494bf970a in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libtorch_python.so)
frame #26: <unknown function> + 0xc70f (0x7f14942c170f in /home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/lib/libtorch.so)
frame #27: <unknown function> + 0x76ba (0x7f149805d6ba in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #28: clone + 0x6d (0x7f1497d9341d in /lib/x86_64-linux-gnu/libc.so.6)




Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=20, cuda=False, dataset='citeseer', decoder=1, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 191
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([3327, 925]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 171.13731 accuracy= 0.79140 time= 0.04466
Epoch: 0011 train_loss= 13.15789 accuracy= 0.58161 time= 0.03030
Epoch: 0021 train_loss= 2.03570 accuracy= 0.59243 time= 0.03029
Epoch: 0031 train_loss= 0.42919 accuracy= 0.58581 time= 0.03055
Epoch: 0041 train_loss= 0.42410 accuracy= 0.58461 time= 0.03103
Epoch: 0051 train_loss= 0.40463 accuracy= 0.58762 time= 0.03099
Epoch: 0061 train_loss= 0.38760 accuracy= 0.58521 time= 0.03089
Epoch: 0071 train_loss= 0.37502 accuracy= 0.58521 time= 0.03071
Epoch: 0081 train_loss= 0.36464 accuracy= 0.58581 time= 0.03092
Epoch: 0091 train_loss= 0.35522 accuracy= 0.58762 time= 0.03065
Epoch: 0101 train_loss= 0.34636 accuracy= 0.59002 time= 0.03074
Epoch: 0111 train_loss= 0.33708 accuracy= 0.59182 time= 0.03099
Epoch: 0121 train_loss= 0.32773 accuracy= 0.59122 time= 0.03094
Epoch: 0131 train_loss= 0.31764 accuracy= 0.58942 time= 0.03037
Epoch: 0141 train_loss= 0.30685 accuracy= 0.58822 time= 0.03062
Epoch: 0151 train_loss= 0.29712 accuracy= 0.59062 time= 0.03094
Epoch: 0161 train_loss= 0.28829 accuracy= 0.58882 time= 0.03059
Epoch: 0171 train_loss= 0.27942 accuracy= 0.58641 time= 0.03071
Epoch: 0181 train_loss= 0.27219 accuracy= 0.58702 time= 0.03094
Epoch: 0191 train_loss= 0.26275 accuracy= 0.59002 time= 0.03099
Epoch: 0201 train_loss= 0.25559 accuracy= 0.58762 time= 0.03088
Epoch: 0211 train_loss= 0.24828 accuracy= 0.59002 time= 0.03101
Epoch: 0221 train_loss= 0.24294 accuracy= 0.58521 time= 0.03091
Epoch: 0231 train_loss= 0.23611 accuracy= 0.58641 time= 0.03092
Epoch: 0241 train_loss= 0.22994 accuracy= 0.58521 time= 0.03086
Epoch: 0251 train_loss= 0.22610 accuracy= 0.58762 time= 0.03094
Epoch: 0261 train_loss= 0.22009 accuracy= 0.58461 time= 0.03074
Epoch: 0271 train_loss= 0.21829 accuracy= 0.58762 time= 0.03077
Epoch: 0281 train_loss= 0.21313 accuracy= 0.58401 time= 0.03059
Epoch: 0291 train_loss= 0.20962 accuracy= 0.58341 time= 0.03079

accuracy 0.58521
auc 0.37994
f1_score 0.13750
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=20, cuda=False, dataset='citeseer', decoder=0, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 583
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([3327, 925]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 59.61777 accuracy= 0.60926 time= 0.05836
Epoch: 0011 train_loss= 8.89448 accuracy= 0.68200 time= 0.04480
Epoch: 0021 train_loss= 7.66837 accuracy= 0.68260 time= 0.04558
Epoch: 0031 train_loss= 7.06565 accuracy= 0.68500 time= 0.04725
Epoch: 0041 train_loss= 7.00674 accuracy= 0.68500 time= 0.04499
Epoch: 0051 train_loss= 6.96289 accuracy= 0.68500 time= 0.04519
Epoch: 0061 train_loss= 6.94494 accuracy= 0.68500 time= 0.04537
Epoch: 0071 train_loss= 6.93064 accuracy= 0.68440 time= 0.04511
Epoch: 0081 train_loss= 6.91755 accuracy= 0.68440 time= 0.04483
Epoch: 0091 train_loss= 7.04727 accuracy= 0.66997 time= 0.04499
Epoch: 0101 train_loss= 6.90302 accuracy= 0.68139 time= 0.04537
Epoch: 0111 train_loss= 6.88268 accuracy= 0.68500 time= 0.04619
Epoch: 0121 train_loss= 6.86990 accuracy= 0.68560 time= 0.04472
Epoch: 0131 train_loss= 6.85634 accuracy= 0.68560 time= 0.04517
Epoch: 0141 train_loss= 6.84266 accuracy= 0.68620 time= 0.04502
Epoch: 0151 train_loss= 6.82926 accuracy= 0.68560 time= 0.04542
Epoch: 0161 train_loss= 6.90028 accuracy= 0.68680 time= 0.04466
Epoch: 0171 train_loss= 6.82789 accuracy= 0.68440 time= 0.04482
Epoch: 0181 train_loss= 6.90122 accuracy= 0.68680 time= 0.04520
Epoch: 0191 train_loss= 6.79265 accuracy= 0.68440 time= 0.04639
Epoch: 0201 train_loss= 6.76205 accuracy= 0.68500 time= 0.04511
Epoch: 0211 train_loss= 6.72987 accuracy= 0.68380 time= 0.04517
Epoch: 0221 train_loss= 6.70413 accuracy= 0.68380 time= 0.04547
Epoch: 0231 train_loss= 6.70295 accuracy= 0.68380 time= 0.04465
Epoch: 0241 train_loss= 6.66812 accuracy= 0.68019 time= 0.04522
Epoch: 0251 train_loss= 6.55627 accuracy= 0.68440 time= 0.04566
Epoch: 0261 train_loss= 6.49721 accuracy= 0.68380 time= 0.04504
Epoch: 0271 train_loss= 6.46225 accuracy= 0.68440 time= 0.04542
Epoch: 0281 train_loss= 6.40785 accuracy= 0.68560 time= 0.04510
Epoch: 0291 train_loss= 6.36994 accuracy= 0.68560 time= 0.04533

accuracy 0.68500
auc 0.60706
f1_score 0.34500
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=20, cuda=False, dataset='citeseer', decoder=1, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 514
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 262 steps!

y_features shape after FMS torch.Size([3327, 925]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 0.47119 accuracy= 0.58762 time= 0.04815
Epoch: 0011 train_loss= 0.48070 accuracy= 0.58762 time= 0.03030
Epoch: 0021 train_loss= 0.48193 accuracy= 0.58762 time= 0.03006
Epoch: 0031 train_loss= 0.48234 accuracy= 0.58641 time= 0.02987
Epoch: 0041 train_loss= 0.48207 accuracy= 0.58641 time= 0.03004
Epoch: 0051 train_loss= 0.48182 accuracy= 0.58702 time= 0.02988
Epoch: 0061 train_loss= 0.48218 accuracy= 0.58581 time= 0.03006
Epoch: 0071 train_loss= 0.48259 accuracy= 0.58581 time= 0.02990
Epoch: 0081 train_loss= 0.48322 accuracy= 0.58581 time= 0.03037
Epoch: 0091 train_loss= 0.48383 accuracy= 0.58641 time= 0.02985
Epoch: 0101 train_loss= 0.48449 accuracy= 0.58641 time= 0.02999
Epoch: 0111 train_loss= 0.48519 accuracy= 0.58641 time= 0.02979
Epoch: 0121 train_loss= 0.48591 accuracy= 0.58641 time= 0.03027
Epoch: 0131 train_loss= 0.48659 accuracy= 0.58702 time= 0.02983
Epoch: 0141 train_loss= 0.48726 accuracy= 0.58762 time= 0.03001
Epoch: 0151 train_loss= 0.48792 accuracy= 0.58762 time= 0.02990
Epoch: 0161 train_loss= 0.48859 accuracy= 0.58822 time= 0.03009
Epoch: 0171 train_loss= 0.48920 accuracy= 0.58822 time= 0.02995
Epoch: 0181 train_loss= 0.48979 accuracy= 0.58822 time= 0.03011
Epoch: 0191 train_loss= 0.49035 accuracy= 0.58762 time= 0.02994
Epoch: 0201 train_loss= 0.49085 accuracy= 0.58762 time= 0.02964
Epoch: 0211 train_loss= 0.49131 accuracy= 0.58702 time= 0.02995
Epoch: 0221 train_loss= 0.49179 accuracy= 0.58702 time= 0.02997
Epoch: 0231 train_loss= 0.49215 accuracy= 0.58641 time= 0.02989
Epoch: 0241 train_loss= 0.49255 accuracy= 0.58702 time= 0.03004
Epoch: 0251 train_loss= 0.49292 accuracy= 0.58702 time= 0.02998
Epoch: 0261 train_loss= 0.49323 accuracy= 0.58702 time= 0.02990
Epoch: 0271 train_loss= 0.49348 accuracy= 0.58702 time= 0.03005
Epoch: 0281 train_loss= 0.49373 accuracy= 0.58762 time= 0.03037
Epoch: 0291 train_loss= 0.49381 accuracy= 0.58762 time= 0.03022

accuracy 0.58702
auc 0.25302
f1_score 0.14125
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=20, cuda=False, dataset='citeseer', decoder=0, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 375
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 271 steps!

y_features shape after FMS torch.Size([3327, 925]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 33.18988 accuracy= 0.62729 time= 0.05966
Epoch: 0011 train_loss= 5.58147 accuracy= 0.68320 time= 0.04321
Epoch: 0021 train_loss= 5.55406 accuracy= 0.68500 time= 0.04386
Epoch: 0031 train_loss= 5.53850 accuracy= 0.68500 time= 0.04348
Epoch: 0041 train_loss= 5.52899 accuracy= 0.68500 time= 0.04373
Epoch: 0051 train_loss= 5.52305 accuracy= 0.68440 time= 0.04355
Epoch: 0061 train_loss= 5.51932 accuracy= 0.68500 time= 0.04381
Epoch: 0071 train_loss= 5.51694 accuracy= 0.68560 time= 0.04380
Epoch: 0081 train_loss= 5.51542 accuracy= 0.68560 time= 0.04358
Epoch: 0091 train_loss= 5.51445 accuracy= 0.68560 time= 0.04376
Epoch: 0101 train_loss= 5.51384 accuracy= 0.68560 time= 0.04341
Epoch: 0111 train_loss= 5.51345 accuracy= 0.68560 time= 0.04420
Epoch: 0121 train_loss= 5.51321 accuracy= 0.68620 time= 0.04343
Epoch: 0131 train_loss= 5.51307 accuracy= 0.68560 time= 0.04427
Epoch: 0141 train_loss= 5.51298 accuracy= 0.68560 time= 0.04381
Epoch: 0151 train_loss= 5.51293 accuracy= 0.68440 time= 0.04373
Epoch: 0161 train_loss= 5.51290 accuracy= 0.68440 time= 0.04376
Epoch: 0171 train_loss= 5.51288 accuracy= 0.68440 time= 0.04334
Epoch: 0181 train_loss= 5.51287 accuracy= 0.68440 time= 0.04426
Epoch: 0191 train_loss= 5.51286 accuracy= 0.68440 time= 0.04400
Epoch: 0201 train_loss= 5.51286 accuracy= 0.68440 time= 0.04373
Epoch: 0211 train_loss= 5.51286 accuracy= 0.68440 time= 0.04412
Epoch: 0221 train_loss= 5.51286 accuracy= 0.68440 time= 0.04355
Epoch: 0231 train_loss= 5.51286 accuracy= 0.68440 time= 0.04390
Epoch: 0241 train_loss= 5.51286 accuracy= 0.68440 time= 0.04376
Epoch: 0251 train_loss= 5.51286 accuracy= 0.68440 time= 0.04338
Epoch: 0261 train_loss= 5.51286 accuracy= 0.68440 time= 0.04428
Epoch: 0271 train_loss= 5.51286 accuracy= 0.68440 time= 0.04394
Epoch: 0281 train_loss= 5.51286 accuracy= 0.68440 time= 0.04365
Epoch: 0291 train_loss= 5.51286 accuracy= 0.68440 time= 0.04414

accuracy 0.68440
auc 0.60814
f1_score 0.34375
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=20, cuda=False, dataset='citeseer', decoder=1, device=device(type='cuda'), dim_reduce=2, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 285
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>

y_features shape after NoReduction torch.Size([3327, 48139]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 0.50570 accuracy= 0.58641 time= 0.57934
Epoch: 0011 train_loss= 0.50899 accuracy= 0.58702 time= 0.57857
Epoch: 0021 train_loss= 0.50628 accuracy= 0.58702 time= 0.57715
Epoch: 0031 train_loss= 0.50572 accuracy= 0.58581 time= 0.57988
Epoch: 0041 train_loss= 0.50592 accuracy= 0.58581 time= 0.57956
Epoch: 0051 train_loss= 0.50552 accuracy= 0.58461 time= 0.57967
Epoch: 0061 train_loss= 0.50547 accuracy= 0.58521 time= 0.58219
Epoch: 0071 train_loss= 0.50546 accuracy= 0.58641 time= 0.58195
Epoch: 0081 train_loss= 0.50544 accuracy= 0.58581 time= 0.58119
Epoch: 0091 train_loss= 0.50543 accuracy= 0.58641 time= 0.57994
Epoch: 0101 train_loss= 0.50543 accuracy= 0.58581 time= 0.57790
Epoch: 0111 train_loss= 0.50543 accuracy= 0.58641 time= 0.58186
Epoch: 0121 train_loss= 0.50543 accuracy= 0.58581 time= 0.58396
Epoch: 0131 train_loss= 0.50543 accuracy= 0.58581 time= 0.58404
Epoch: 0141 train_loss= 0.50543 accuracy= 0.58581 time= 0.58419
Epoch: 0151 train_loss= 0.50543 accuracy= 0.58581 time= 0.58478
Epoch: 0161 train_loss= 0.50543 accuracy= 0.58581 time= 0.58716
Epoch: 0171 train_loss= 0.50543 accuracy= 0.58521 time= 0.58715
Epoch: 0181 train_loss= 0.50543 accuracy= 0.58581 time= 0.58687
Epoch: 0191 train_loss= 0.50543 accuracy= 0.58521 time= 0.58807
Epoch: 0201 train_loss= 0.50543 accuracy= 0.58581 time= 0.58917
Epoch: 0211 train_loss= 0.50543 accuracy= 0.58521 time= 0.58610
Epoch: 0221 train_loss= 0.50543 accuracy= 0.58581 time= 0.59048
Epoch: 0231 train_loss= 0.50543 accuracy= 0.58641 time= 0.59199
Epoch: 0241 train_loss= 0.50543 accuracy= 0.58521 time= 0.58795
Epoch: 0251 train_loss= 0.50543 accuracy= 0.58581 time= 0.59193
Epoch: 0261 train_loss= 0.50543 accuracy= 0.58581 time= 0.58700
Epoch: 0271 train_loss= 0.50543 accuracy= 0.58641 time= 0.58919
Epoch: 0281 train_loss= 0.50543 accuracy= 0.58641 time= 0.59073
Epoch: 0291 train_loss= 0.50543 accuracy= 0.58581 time= 0.59028

accuracy 0.58641
auc 0.25586
f1_score 0.14000
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=20, cuda=False, dataset='citeseer', decoder=0, device=device(type='cuda'), dim_reduce=2, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 194
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>

y_features shape after NoReduction torch.Size([3327, 48139]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 5.70344 accuracy= 0.68079 time= 0.60774
Epoch: 0011 train_loss= 5.64021 accuracy= 0.68560 time= 0.60161
Epoch: 0021 train_loss= 5.61973 accuracy= 0.68440 time= 0.60260
Epoch: 0031 train_loss= 5.53927 accuracy= 0.68500 time= 0.60164
Epoch: 0041 train_loss= 5.53919 accuracy= 0.68380 time= 0.60441
Epoch: 0051 train_loss= 5.52435 accuracy= 0.68440 time= 0.60623
Epoch: 0061 train_loss= 5.52017 accuracy= 0.68500 time= 0.60793
Epoch: 0071 train_loss= 5.52972 accuracy= 0.68320 time= 0.60799
Epoch: 0081 train_loss= 5.51570 accuracy= 0.68560 time= 0.60775
Epoch: 0091 train_loss= 5.51898 accuracy= 0.68260 time= 0.60921
Epoch: 0101 train_loss= 5.51529 accuracy= 0.68560 time= 0.61024
Epoch: 0111 train_loss= 5.51575 accuracy= 0.68500 time= 0.60952
Epoch: 0121 train_loss= 5.51326 accuracy= 0.68560 time= 0.61230
Epoch: 0131 train_loss= 5.51557 accuracy= 0.68440 time= 0.61369
Epoch: 0141 train_loss= 5.51332 accuracy= 0.68620 time= 0.61608
Epoch: 0151 train_loss= 5.51359 accuracy= 0.68680 time= 0.61248
Epoch: 0161 train_loss= 5.51455 accuracy= 0.68620 time= 0.61059
Epoch: 0171 train_loss= 5.51292 accuracy= 0.68440 time= 0.61331
Epoch: 0181 train_loss= 5.51287 accuracy= 0.68440 time= 0.61394
Epoch: 0191 train_loss= 5.51287 accuracy= 0.68440 time= 0.61268
Epoch: 0201 train_loss= 5.51358 accuracy= 0.68500 time= 0.61476
Epoch: 0211 train_loss= 5.51301 accuracy= 0.68500 time= 0.61556
Epoch: 0221 train_loss= 5.51286 accuracy= 0.68440 time= 0.61471
Epoch: 0231 train_loss= 5.51310 accuracy= 0.68500 time= 0.61481
Epoch: 0241 train_loss= 5.51296 accuracy= 0.68440 time= 0.61356
Epoch: 0251 train_loss= 5.51286 accuracy= 0.68440 time= 0.61455
Epoch: 0261 train_loss= 5.51286 accuracy= 0.68440 time= 0.61437
Epoch: 0271 train_loss= 5.51287 accuracy= 0.68440 time= 0.61538
Epoch: 0281 train_loss= 5.51286 accuracy= 0.68440 time= 0.61761
Epoch: 0291 train_loss= 5.51288 accuracy= 0.68440 time= 0.61818

accuracy 0.68440
auc 0.60818
f1_score 0.34375
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=3, beta=1, clique_size=20, cuda=False, dataset='citeseer', decoder=1, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 527
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([3327, 925]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 8555.14160 accuracy= 0.74271 time= 0.04865
Epoch: 0011 train_loss= 4862.50391 accuracy= 0.58942 time= 0.03556
Epoch: 0021 train_loss= 4143.44092 accuracy= 0.58641 time= 0.03529
Epoch: 0031 train_loss= 3697.17554 accuracy= 0.58882 time= 0.03435
Epoch: 0041 train_loss= 3367.57031 accuracy= 0.58281 time= 0.03516
Epoch: 0051 train_loss= 3111.90015 accuracy= 0.58221 time= 0.03542
Epoch: 0061 train_loss= 2895.01245 accuracy= 0.58100 time= 0.03512
Epoch: 0071 train_loss= 2706.29517 accuracy= 0.58521 time= 0.03478
Epoch: 0081 train_loss= 2540.43628 accuracy= 0.58401 time= 0.03554
Epoch: 0091 train_loss= 2390.25928 accuracy= 0.58521 time= 0.03525
Epoch: 0101 train_loss= 2259.02930 accuracy= 0.58581 time= 0.03437
Epoch: 0111 train_loss= 2140.31470 accuracy= 0.58702 time= 0.03516
Epoch: 0121 train_loss= 2026.81543 accuracy= 0.58581 time= 0.03557
Epoch: 0131 train_loss= 1931.21619 accuracy= 0.58581 time= 0.03454
Epoch: 0141 train_loss= 1838.01184 accuracy= 0.58702 time= 0.03529
Epoch: 0151 train_loss= 1750.03503 accuracy= 0.58461 time= 0.03559
Epoch: 0161 train_loss= 1672.79822 accuracy= 0.58702 time= 0.03445
Epoch: 0171 train_loss= 1595.39001 accuracy= 0.58521 time= 0.03493
Epoch: 0181 train_loss= 1526.09045 accuracy= 0.58521 time= 0.03555
Epoch: 0191 train_loss= 1459.21082 accuracy= 0.58461 time= 0.03473
Epoch: 0201 train_loss= 1389.37817 accuracy= 0.58702 time= 0.03510
Epoch: 0211 train_loss= 1327.97119 accuracy= 0.58341 time= 0.03547
Epoch: 0221 train_loss= 1272.73340 accuracy= 0.58882 time= 0.03436
Epoch: 0231 train_loss= 1213.13245 accuracy= 0.58401 time= 0.03484
Epoch: 0241 train_loss= 1163.01709 accuracy= 0.59002 time= 0.03561
Epoch: 0251 train_loss= 1112.08301 accuracy= 0.58341 time= 0.03466
Epoch: 0261 train_loss= 1063.73438 accuracy= 0.58702 time= 0.03492
Epoch: 0271 train_loss= 1012.87805 accuracy= 0.58702 time= 0.03566
Epoch: 0281 train_loss= 967.55188 accuracy= 0.58882 time= 0.03469
Epoch: 0291 train_loss= 924.82916 accuracy= 0.59182 time= 0.03489

accuracy 0.58762
auc 0.31797
f1_score 0.14250
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=3, beta=1, clique_size=20, cuda=False, dataset='citeseer', decoder=0, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 403
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([3327, 925]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 59.28243 accuracy= 0.59964 time= 0.06657
Epoch: 0011 train_loss= 16.40155 accuracy= 0.60084 time= 0.05103
Epoch: 0021 train_loss= 9.66418 accuracy= 0.65615 time= 0.05118
Epoch: 0031 train_loss= 8.26375 accuracy= 0.68620 time= 0.05052
Epoch: 0041 train_loss= 7.62763 accuracy= 0.69282 time= 0.05227
Epoch: 0051 train_loss= 7.10135 accuracy= 0.69943 time= 0.05012
Epoch: 0061 train_loss= 6.90373 accuracy= 0.71025 time= 0.04987
Epoch: 0071 train_loss= 6.80969 accuracy= 0.71626 time= 0.05059
Epoch: 0081 train_loss= 6.78532 accuracy= 0.72888 time= 0.04997
Epoch: 0091 train_loss= 6.71087 accuracy= 0.72768 time= 0.04976
Epoch: 0101 train_loss= 6.62523 accuracy= 0.73430 time= 0.05039
Epoch: 0111 train_loss= 6.74551 accuracy= 0.74752 time= 0.05007
Epoch: 0121 train_loss= 6.71210 accuracy= 0.75053 time= 0.04971
Epoch: 0131 train_loss= 6.74421 accuracy= 0.72768 time= 0.05013
Epoch: 0141 train_loss= 6.49793 accuracy= 0.73069 time= 0.05034
Epoch: 0151 train_loss= 6.44049 accuracy= 0.74271 time= 0.04927
Epoch: 0161 train_loss= 6.39011 accuracy= 0.74331 time= 0.05012
Epoch: 0171 train_loss= 6.37000 accuracy= 0.74211 time= 0.04905
Epoch: 0181 train_loss= 6.49124 accuracy= 0.73910 time= 0.05010
Epoch: 0191 train_loss= 6.44344 accuracy= 0.74211 time= 0.04917
Epoch: 0201 train_loss= 6.52503 accuracy= 0.75534 time= 0.04997
Epoch: 0211 train_loss= 6.93952 accuracy= 0.74512 time= 0.04908
Epoch: 0221 train_loss= 7.01833 accuracy= 0.71626 time= 0.05012
Epoch: 0231 train_loss= 6.92316 accuracy= 0.69041 time= 0.04936
Epoch: 0241 train_loss= 6.90995 accuracy= 0.70544 time= 0.05016
Epoch: 0251 train_loss= 6.86391 accuracy= 0.71806 time= 0.04933
Epoch: 0261 train_loss= 6.48901 accuracy= 0.73009 time= 0.04993
Epoch: 0271 train_loss= 6.44247 accuracy= 0.73850 time= 0.04929
Epoch: 0281 train_loss= 6.40467 accuracy= 0.73730 time= 0.05017
Epoch: 0291 train_loss= 6.31568 accuracy= 0.73790 time= 0.04930

accuracy 0.73971
auc 0.71524
f1_score 0.45875
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=3, beta=1, clique_size=20, cuda=False, dataset='citeseer', decoder=1, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 48
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 327 steps!

y_features shape after FMS torch.Size([3327, 925]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 8421.53809 accuracy= 0.78179 time= 0.04924
Epoch: 0011 train_loss= 4945.43848 accuracy= 0.59002 time= 0.03349
Epoch: 0021 train_loss= 4165.10840 accuracy= 0.59122 time= 0.03306
Epoch: 0031 train_loss= 3694.38770 accuracy= 0.58822 time= 0.03358
Epoch: 0041 train_loss= 3358.17676 accuracy= 0.58221 time= 0.03305
Epoch: 0051 train_loss= 3083.83057 accuracy= 0.58581 time= 0.03387
Epoch: 0061 train_loss= 2869.37256 accuracy= 0.58581 time= 0.03288
Epoch: 0071 train_loss= 2676.46655 accuracy= 0.58281 time= 0.03390
Epoch: 0081 train_loss= 5521.76758 accuracy= 0.66096 time= 0.03288
Epoch: 0091 train_loss= 2775.92993 accuracy= 0.59723 time= 0.03417
Epoch: 0101 train_loss= 2450.34033 accuracy= 0.58581 time= 0.03273
Epoch: 0111 train_loss= 2254.34668 accuracy= 0.58521 time= 0.03345
Epoch: 0121 train_loss= 2118.22974 accuracy= 0.58581 time= 0.03277
Epoch: 0131 train_loss= 2005.81433 accuracy= 0.58521 time= 0.03410
Epoch: 0141 train_loss= 1909.02344 accuracy= 0.58581 time= 0.03290
Epoch: 0151 train_loss= 1822.62793 accuracy= 0.58521 time= 0.03415
Epoch: 0161 train_loss= 1739.52319 accuracy= 0.58521 time= 0.03300
Epoch: 0171 train_loss= 1665.12537 accuracy= 0.58401 time= 0.03398
Epoch: 0181 train_loss= 1590.06311 accuracy= 0.58521 time= 0.03298
Epoch: 0191 train_loss= 1521.51819 accuracy= 0.58401 time= 0.03371
Epoch: 0201 train_loss= 1457.90735 accuracy= 0.58281 time= 0.03295
Epoch: 0211 train_loss= 1394.06409 accuracy= 0.58401 time= 0.03398
Epoch: 0221 train_loss= 1333.47510 accuracy= 0.58401 time= 0.03300
Epoch: 0231 train_loss= 1276.82947 accuracy= 0.58281 time= 0.03373
Epoch: 0241 train_loss= 1219.94250 accuracy= 0.58341 time= 0.03330
Epoch: 0251 train_loss= 1166.09900 accuracy= 0.58221 time= 0.03367
Epoch: 0261 train_loss= 1114.04382 accuracy= 0.58401 time= 0.03348
Epoch: 0271 train_loss= 1065.04895 accuracy= 0.58401 time= 0.03361
Epoch: 0281 train_loss= 1019.01654 accuracy= 0.58641 time= 0.03394
Epoch: 0291 train_loss= 974.43494 accuracy= 0.58281 time= 0.03299

accuracy 0.58341
auc 0.28338
f1_score 0.13375
Job finished!
./11_11.sh: line 83: 59367 Terminated              CUDA_VISIBLE_DEVICES=3 python3 train_scat_onedecoder.py --hidden1 0.5 --hidden2 0.25 --dim_reduce 1 --clique_size 20 --num_clique 20 --decoder 0 --att 3 --dataset citeseer



Initializing scat onedecoder
Namespace(alpha=1, att=3, beta=1, clique_size=20, cuda=False, dataset='citeseer', decoder=1, device=device(type='cuda'), dim_reduce=2, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 903
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>

y_features shape after NoReduction torch.Size([3327, 48139]) <class 'torch.Tensor'>
Traceback (most recent call last):
  File "train_scat_onedecoder.py", line 247, in <module>
    gae_ad(args)
  File "train_scat_onedecoder.py", line 173, in gae_ad
    optimizer.step()
  File "/home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 15, in decorate_context
    return func(*args, **kwargs)
  File "/home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/optim/adam.py", line 79, in step
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
RuntimeError: CUDA out of memory. Tried to allocate 8.63 GiB (GPU 0; 23.65 GiB total capacity; 18.66 GiB already allocated; 3.76 GiB free; 18.93 GiB reserved in total by PyTorch)



Initializing scat onedecoder
Namespace(alpha=1, att=3, beta=1, clique_size=20, cuda=False, dataset='citeseer', decoder=0, device=device(type='cuda'), dim_reduce=2, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 471
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>

y_features shape after NoReduction torch.Size([3327, 48139]) <class 'torch.Tensor'>
Traceback (most recent call last):
  File "train_scat_onedecoder.py", line 247, in <module>
    gae_ad(args)
  File "train_scat_onedecoder.py", line 173, in gae_ad
    optimizer.step()
  File "/home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 15, in decorate_context
    return func(*args, **kwargs)
  File "/home/augus/.conda/envs/augus/lib/python3.8/site-packages/torch/optim/adam.py", line 79, in step
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
RuntimeError: CUDA out of memory. Tried to allocate 8.63 GiB (GPU 0; 23.65 GiB total capacity; 18.71 GiB already allocated; 3.68 GiB free; 19.01 GiB reserved in total by PyTorch)



Initializing scat twodecoders training
Namespace(alpha=1, att=0, beta=1, clique_size=20, cuda=False, dataset='citeseer', device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 954
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>
using PCA to reduce dim
y_features shape after PCA torch.Size([3327, 925]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 7769.81494 feature_loss= 59.71939 structure_loss= 7710.09570 accuracy= 0.79261 accuracy_s= 0.89841 accuracy_f= 0.79200 time= 0.07783
Epoch: 0011 train_loss= 4407.72852 feature_loss= 22.77800 structure_loss= 4384.95068 accuracy= 0.58942 accuracy_s= 0.76014 accuracy_f= 0.81004 time= 0.06104
Epoch: 0021 train_loss= 3718.39087 feature_loss= 11.32315 structure_loss= 3707.06763 accuracy= 0.58461 accuracy_s= 0.75954 accuracy_f= 0.82567 time= 0.06091
Epoch: 0031 train_loss= 3325.62793 feature_loss= 8.45825 structure_loss= 3317.16968 accuracy= 0.58221 accuracy_s= 0.75954 accuracy_f= 0.83709 time= 0.06119
Epoch: 0041 train_loss= 3049.40942 feature_loss= 7.71130 structure_loss= 3041.69824 accuracy= 0.58401 accuracy_s= 0.75954 accuracy_f= 0.84911 time= 0.06081
Epoch: 0051 train_loss= 2837.32031 feature_loss= 7.23427 structure_loss= 2830.08594 accuracy= 0.58341 accuracy_s= 0.75954 accuracy_f= 0.85332 time= 0.06125
Epoch: 0061 train_loss= 2659.81665 feature_loss= 6.89183 structure_loss= 2652.92480 accuracy= 0.58341 accuracy_s= 0.75954 accuracy_f= 0.85452 time= 0.06086
Epoch: 0071 train_loss= 2508.87354 feature_loss= 6.70558 structure_loss= 2502.16797 accuracy= 0.58401 accuracy_s= 0.75954 accuracy_f= 0.85032 time= 0.06081
Epoch: 0081 train_loss= 2373.99854 feature_loss= 6.56242 structure_loss= 2367.43604 accuracy= 0.58401 accuracy_s= 0.75954 accuracy_f= 0.84731 time= 0.06085
Epoch: 0091 train_loss= 2252.79810 feature_loss= 6.42805 structure_loss= 2246.37012 accuracy= 0.58461 accuracy_s= 0.75954 accuracy_f= 0.84491 time= 0.06078
Epoch: 0101 train_loss= 2143.24292 feature_loss= 6.29080 structure_loss= 2136.95215 accuracy= 0.58702 accuracy_s= 0.75954 accuracy_f= 0.84611 time= 0.06073
Epoch: 0111 train_loss= 2042.23755 feature_loss= 6.15585 structure_loss= 2036.08167 accuracy= 0.58581 accuracy_s= 0.75954 accuracy_f= 0.84430 time= 0.06082
Epoch: 0121 train_loss= 1944.80969 feature_loss= 6.07419 structure_loss= 1938.73547 accuracy= 0.58521 accuracy_s= 0.75954 accuracy_f= 0.84190 time= 0.06096
Epoch: 0131 train_loss= 1854.62366 feature_loss= 5.90923 structure_loss= 1848.71448 accuracy= 0.58581 accuracy_s= 0.75954 accuracy_f= 0.83950 time= 0.06084
Epoch: 0141 train_loss= 1771.49060 feature_loss= 5.49025 structure_loss= 1766.00037 accuracy= 0.58581 accuracy_s= 0.75954 accuracy_f= 0.83709 time= 0.06077
Epoch: 0151 train_loss= 1691.30371 feature_loss= 5.31725 structure_loss= 1685.98645 accuracy= 0.58581 accuracy_s= 0.75954 accuracy_f= 0.83709 time= 0.06067
Epoch: 0161 train_loss= 1615.14062 feature_loss= 5.26019 structure_loss= 1609.88049 accuracy= 0.58581 accuracy_s= 0.75954 accuracy_f= 0.83769 time= 0.06107
Epoch: 0171 train_loss= 1544.81421 feature_loss= 5.24936 structure_loss= 1539.56482 accuracy= 0.58581 accuracy_s= 0.75954 accuracy_f= 0.83469 time= 0.06137
Epoch: 0181 train_loss= 1475.83301 feature_loss= 5.18294 structure_loss= 1470.65002 accuracy= 0.58521 accuracy_s= 0.75954 accuracy_f= 0.83348 time= 0.06076
Epoch: 0191 train_loss= 1410.56982 feature_loss= 5.14400 structure_loss= 1405.42578 accuracy= 0.58461 accuracy_s= 0.75954 accuracy_f= 0.83649 time= 0.06085
Epoch: 0201 train_loss= 1349.90332 feature_loss= 5.17404 structure_loss= 1344.72925 accuracy= 0.58702 accuracy_s= 0.75954 accuracy_f= 0.83829 time= 0.06097
Epoch: 0211 train_loss= 1290.27356 feature_loss= 5.15674 structure_loss= 1285.11682 accuracy= 0.58581 accuracy_s= 0.75954 accuracy_f= 0.83709 time= 0.06096
Epoch: 0221 train_loss= 1233.96619 feature_loss= 5.12791 structure_loss= 1228.83826 accuracy= 0.58762 accuracy_s= 0.75954 accuracy_f= 0.83709 time= 0.06093
Epoch: 0231 train_loss= 1179.44653 feature_loss= 5.16271 structure_loss= 1174.28381 accuracy= 0.58762 accuracy_s= 0.75954 accuracy_f= 0.83469 time= 0.06091
Epoch: 0241 train_loss= 1127.10181 feature_loss= 5.11276 structure_loss= 1121.98901 accuracy= 0.58581 accuracy_s= 0.75954 accuracy_f= 0.83469 time= 0.06093
Epoch: 0251 train_loss= 1075.72302 feature_loss= 5.19957 structure_loss= 1070.52344 accuracy= 0.58581 accuracy_s= 0.75954 accuracy_f= 0.82867 time= 0.06077
Epoch: 0261 train_loss= 1029.74072 feature_loss= 5.17634 structure_loss= 1024.56433 accuracy= 0.58461 accuracy_s= 0.75954 accuracy_f= 0.83048 time= 0.06104
Epoch: 0271 train_loss= 983.82739 feature_loss= 5.16533 structure_loss= 978.66205 accuracy= 0.58401 accuracy_s= 0.75954 accuracy_f= 0.83348 time= 0.06112
Epoch: 0281 train_loss= 940.32227 feature_loss= 5.14667 structure_loss= 935.17560 accuracy= 0.58581 accuracy_s= 0.75954 accuracy_f= 0.83228 time= 0.06085
Epoch: 0291 train_loss= 896.26007 feature_loss= 5.24364 structure_loss= 891.01642 accuracy= 0.58521 accuracy_s= 0.75954 accuracy_f= 0.82567 time= 0.06087

accuracy 0.58401
accuracy_s 0.75954
accuracy_f 0.82627
auc 0.31115
f1_score 0.13500
Job finished!
./11_11.sh: line 89: 78033 Terminated              CUDA_VISIBLE_DEVICES=3 python3 train_scat_twodecoders.py --hidden1 0.5 --hidden2 0.25 --dim_reduce 1 --clique_size 20 --num_clique 20 --att 0 --dataset citeseer



Initializing scat twodecoders training
Namespace(alpha=1, att=0, beta=1, clique_size=20, cuda=False, dataset='citeseer', device=device(type='cuda'), dim_reduce=2, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 492
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>
48139
y_features shape after NoReduction torch.Size([3327, 48139]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 7735.30176 feature_loss= 57.21809 structure_loss= 7678.08350 accuracy= 0.59122 accuracy_s= 0.75954 accuracy_f= 0.79020 time= 0.30875
Epoch: 0011 train_loss= 18891.61133 feature_loss= 11.36950 structure_loss= 18880.24219 accuracy= 0.60986 accuracy_s= 0.77036 accuracy_f= 0.82867 time= 0.27083
Epoch: 0021 train_loss= 12695.16797 feature_loss= 9.06895 structure_loss= 12686.09863 accuracy= 0.60265 accuracy_s= 0.76255 accuracy_f= 0.82988 time= 0.27062
Epoch: 0031 train_loss= 8929.36133 feature_loss= 8.41389 structure_loss= 8920.94727 accuracy= 0.59603 accuracy_s= 0.76195 accuracy_f= 0.83288 time= 0.27149
Epoch: 0041 train_loss= 6346.19971 feature_loss= 8.24496 structure_loss= 6337.95459 accuracy= 0.59363 accuracy_s= 0.76014 accuracy_f= 0.83709 time= 0.27165
Epoch: 0051 train_loss= 4605.11914 feature_loss= 8.14408 structure_loss= 4596.97510 accuracy= 0.59303 accuracy_s= 0.76014 accuracy_f= 0.83408 time= 0.27226
Epoch: 0061 train_loss= 3379.41284 feature_loss= 8.03880 structure_loss= 3371.37402 accuracy= 0.59062 accuracy_s= 0.76014 accuracy_f= 0.83589 time= 0.27182
Epoch: 0071 train_loss= 2558.37134 feature_loss= 7.94461 structure_loss= 2550.42676 accuracy= 0.59182 accuracy_s= 0.76014 accuracy_f= 0.83950 time= 0.27150
Epoch: 0081 train_loss= 1992.89417 feature_loss= 7.84556 structure_loss= 1985.04858 accuracy= 0.60265 accuracy_s= 0.75954 accuracy_f= 0.84250 time= 0.27369
Epoch: 0091 train_loss= 1592.41504 feature_loss= 7.73771 structure_loss= 1584.67737 accuracy= 0.59483 accuracy_s= 0.75954 accuracy_f= 0.84070 time= 0.27243
Epoch: 0101 train_loss= 1317.73389 feature_loss= 7.61557 structure_loss= 1310.11829 accuracy= 0.59483 accuracy_s= 0.75954 accuracy_f= 0.84190 time= 0.27333
Epoch: 0111 train_loss= 1128.68750 feature_loss= 7.46835 structure_loss= 1121.21912 accuracy= 0.59423 accuracy_s= 0.75954 accuracy_f= 0.84491 time= 0.27371
Epoch: 0121 train_loss= 989.04248 feature_loss= 7.29344 structure_loss= 981.74902 accuracy= 0.59423 accuracy_s= 0.75954 accuracy_f= 0.84430 time= 0.27377
Epoch: 0131 train_loss= 880.20691 feature_loss= 7.10608 structure_loss= 873.10083 accuracy= 0.59363 accuracy_s= 0.75954 accuracy_f= 0.84611 time= 0.27241
Epoch: 0141 train_loss= 789.99603 feature_loss= 6.96512 structure_loss= 783.03094 accuracy= 0.59122 accuracy_s= 0.75954 accuracy_f= 0.84190 time= 0.27559
Epoch: 0151 train_loss= 723.77258 feature_loss= 7.00471 structure_loss= 716.76788 accuracy= 0.59243 accuracy_s= 0.75954 accuracy_f= 0.84851 time= 0.27223
Epoch: 0161 train_loss= 668.75775 feature_loss= 7.11905 structure_loss= 661.63873 accuracy= 0.58942 accuracy_s= 0.75954 accuracy_f= 0.83469 time= 0.27279
Epoch: 0171 train_loss= 621.71198 feature_loss= 7.01011 structure_loss= 614.70184 accuracy= 0.58762 accuracy_s= 0.75954 accuracy_f= 0.84370 time= 0.27398
Epoch: 0181 train_loss= 578.92914 feature_loss= 6.81103 structure_loss= 572.11810 accuracy= 0.58581 accuracy_s= 0.75954 accuracy_f= 0.84430 time= 0.27433
Epoch: 0191 train_loss= 538.91876 feature_loss= 6.61911 structure_loss= 532.29968 accuracy= 0.58762 accuracy_s= 0.75954 accuracy_f= 0.84190 time= 0.27445
Epoch: 0201 train_loss= 505.10864 feature_loss= 6.41066 structure_loss= 498.69800 accuracy= 0.58942 accuracy_s= 0.75954 accuracy_f= 0.84671 time= 0.27380
Epoch: 0211 train_loss= 480.93808 feature_loss= 6.24962 structure_loss= 474.68848 accuracy= 0.59062 accuracy_s= 0.75954 accuracy_f= 0.84671 time= 0.27450
Epoch: 0221 train_loss= 441.33124 feature_loss= 6.17964 structure_loss= 435.15161 accuracy= 0.59002 accuracy_s= 0.75954 accuracy_f= 0.84370 time= 0.27332
Epoch: 0231 train_loss= 416.48926 feature_loss= 5.97774 structure_loss= 410.51151 accuracy= 0.59002 accuracy_s= 0.75954 accuracy_f= 0.84430 time= 0.27546
Epoch: 0241 train_loss= 385.46411 feature_loss= 5.81735 structure_loss= 379.64676 accuracy= 0.59303 accuracy_s= 0.75954 accuracy_f= 0.83950 time= 0.27581
Epoch: 0251 train_loss= 356.00891 feature_loss= 5.70850 structure_loss= 350.30042 accuracy= 0.59062 accuracy_s= 0.75954 accuracy_f= 0.84430 time= 0.27395
Epoch: 0261 train_loss= 329.44531 feature_loss= 5.61669 structure_loss= 323.82861 accuracy= 0.59303 accuracy_s= 0.75954 accuracy_f= 0.84010 time= 0.27609
Epoch: 0271 train_loss= 309.09924 feature_loss= 5.55644 structure_loss= 303.54279 accuracy= 0.63030 accuracy_s= 0.75954 accuracy_f= 0.84671 time= 0.27501
Epoch: 0281 train_loss= 289.01025 feature_loss= 5.50539 structure_loss= 283.50485 accuracy= 0.65494 accuracy_s= 0.75954 accuracy_f= 0.84370 time= 0.27582
Epoch: 0291 train_loss= 278.78134 feature_loss= 5.49462 structure_loss= 273.28671 accuracy= 0.65675 accuracy_s= 0.75954 accuracy_f= 0.83829 time= 0.27605

accuracy 0.66757
accuracy_s 0.75954
accuracy_f 0.84430
auc 0.58438
f1_score 0.30875
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=1, beta=1, clique_size=20, cuda=False, dataset='citeseer', device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 658
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>
using PCA to reduce dim
y_features shape after PCA torch.Size([3327, 925]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 425.44025 feature_loss= 59.39391 structure_loss= 366.04633 accuracy= 0.75834 accuracy_s= 0.87677 accuracy_f= 0.79261 time= 0.09864
Epoch: 0011 train_loss= 28.12353 feature_loss= 8.32795 structure_loss= 19.79558 accuracy= 0.59002 accuracy_s= 0.76075 accuracy_f= 0.82086 time= 0.07875
Epoch: 0021 train_loss= 8.26619 feature_loss= 7.05374 structure_loss= 1.21244 accuracy= 0.62369 accuracy_s= 0.75954 accuracy_f= 0.82507 time= 0.07837
Epoch: 0031 train_loss= 7.57673 feature_loss= 7.02685 structure_loss= 0.54988 accuracy= 0.66035 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07814
Epoch: 0041 train_loss= 7.48707 feature_loss= 6.98377 structure_loss= 0.50330 accuracy= 0.65855 accuracy_s= 0.75954 accuracy_f= 0.82627 time= 0.07861
Epoch: 0051 train_loss= 7.45505 feature_loss= 6.95822 structure_loss= 0.49684 accuracy= 0.65855 accuracy_s= 0.75954 accuracy_f= 0.82567 time= 0.07782
Epoch: 0061 train_loss= 7.43695 feature_loss= 6.94430 structure_loss= 0.49266 accuracy= 0.65975 accuracy_s= 0.75954 accuracy_f= 0.82567 time= 0.07859
Epoch: 0071 train_loss= 7.42062 feature_loss= 6.93278 structure_loss= 0.48784 accuracy= 0.66216 accuracy_s= 0.75954 accuracy_f= 0.82627 time= 0.07865
Epoch: 0081 train_loss= 7.40563 feature_loss= 6.92186 structure_loss= 0.48377 accuracy= 0.66035 accuracy_s= 0.75954 accuracy_f= 0.82567 time= 0.07825
Epoch: 0091 train_loss= 7.39282 feature_loss= 6.91200 structure_loss= 0.48082 accuracy= 0.66096 accuracy_s= 0.75954 accuracy_f= 0.82567 time= 0.07861
Epoch: 0101 train_loss= 7.37966 feature_loss= 6.90235 structure_loss= 0.47732 accuracy= 0.65915 accuracy_s= 0.75954 accuracy_f= 0.82627 time= 0.07873
Epoch: 0111 train_loss= 7.36658 feature_loss= 6.89264 structure_loss= 0.47394 accuracy= 0.65975 accuracy_s= 0.75954 accuracy_f= 0.82627 time= 0.07807
Epoch: 0121 train_loss= 7.35364 feature_loss= 6.88271 structure_loss= 0.47093 accuracy= 0.65915 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07860
Epoch: 0131 train_loss= 7.42887 feature_loss= 6.96121 structure_loss= 0.46766 accuracy= 0.65434 accuracy_s= 0.75954 accuracy_f= 0.82567 time= 0.07832
Epoch: 0141 train_loss= 7.32972 feature_loss= 6.86292 structure_loss= 0.46680 accuracy= 0.65675 accuracy_s= 0.76014 accuracy_f= 0.82687 time= 0.07795
Epoch: 0151 train_loss= 7.31570 feature_loss= 6.85248 structure_loss= 0.46322 accuracy= 0.65735 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07828
Epoch: 0161 train_loss= 7.30314 feature_loss= 6.84183 structure_loss= 0.46131 accuracy= 0.65855 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07811
Epoch: 0171 train_loss= 7.29108 feature_loss= 6.83094 structure_loss= 0.46013 accuracy= 0.65855 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07840
Epoch: 0181 train_loss= 7.34279 feature_loss= 6.88529 structure_loss= 0.45749 accuracy= 0.65494 accuracy_s= 0.75954 accuracy_f= 0.82627 time= 0.07817
Epoch: 0191 train_loss= 7.32326 feature_loss= 6.86673 structure_loss= 0.45654 accuracy= 0.65675 accuracy_s= 0.75954 accuracy_f= 0.82507 time= 0.07808
Epoch: 0201 train_loss= 7.30232 feature_loss= 6.84768 structure_loss= 0.45463 accuracy= 0.65735 accuracy_s= 0.75954 accuracy_f= 0.82507 time= 0.07819
Epoch: 0211 train_loss= 7.51898 feature_loss= 7.06573 structure_loss= 0.45325 accuracy= 0.66817 accuracy_s= 0.75954 accuracy_f= 0.81184 time= 0.07819
Epoch: 0221 train_loss= 7.58674 feature_loss= 7.13542 structure_loss= 0.45132 accuracy= 0.66516 accuracy_s= 0.75954 accuracy_f= 0.80944 time= 0.08070
Epoch: 0231 train_loss= 7.38846 feature_loss= 6.93676 structure_loss= 0.45169 accuracy= 0.66396 accuracy_s= 0.75954 accuracy_f= 0.81906 time= 0.07867
Epoch: 0241 train_loss= 7.24633 feature_loss= 6.79563 structure_loss= 0.45070 accuracy= 0.65855 accuracy_s= 0.75954 accuracy_f= 0.82507 time= 0.07882
Epoch: 0251 train_loss= 6.33099 feature_loss= 5.88302 structure_loss= 0.44797 accuracy= 0.65374 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07816
Epoch: 0261 train_loss= 6.12499 feature_loss= 5.67902 structure_loss= 0.44597 accuracy= 0.65615 accuracy_s= 0.75954 accuracy_f= 0.82747 time= 0.07860
Epoch: 0271 train_loss= 6.10815 feature_loss= 5.66486 structure_loss= 0.44328 accuracy= 0.65555 accuracy_s= 0.75954 accuracy_f= 0.82747 time= 0.07848
Epoch: 0281 train_loss= 6.07693 feature_loss= 5.63275 structure_loss= 0.44419 accuracy= 0.65675 accuracy_s= 0.75954 accuracy_f= 0.82627 time= 0.07849
Epoch: 0291 train_loss= 6.06119 feature_loss= 5.62221 structure_loss= 0.43898 accuracy= 0.65555 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07869

accuracy 0.65494
accuracy_s 0.75954
accuracy_f 0.82687
auc 0.52839
f1_score 0.28250
Job finished!
