nohup: ignoring input
cora, citeseer, pubmed, amz-photo, amz-computer, cora-full
cora experiment with fixed GCN hidden layer continued training



Initializing scat twodecoders training
Namespace(alpha=1, att=1, beta=1, clique_size=20, cuda=False, dataset='citeseer', device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 862
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 277 steps!
y_features shape after FMS torch.Size([3327, 925]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 40.60094 feature_loss= 40.06276 structure_loss= 0.53818 accuracy= 0.64172 accuracy_s= 0.76916 accuracy_f= 0.79681 time= 0.10362
Epoch: 0011 train_loss= 6.06027 feature_loss= 5.58161 structure_loss= 0.47866 accuracy= 0.65975 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07638
Epoch: 0021 train_loss= 6.03545 feature_loss= 5.55410 structure_loss= 0.48135 accuracy= 0.65855 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07625
Epoch: 0031 train_loss= 6.01818 feature_loss= 5.53848 structure_loss= 0.47970 accuracy= 0.65795 accuracy_s= 0.75954 accuracy_f= 0.82627 time= 0.07601
Epoch: 0041 train_loss= 6.00988 feature_loss= 5.52895 structure_loss= 0.48094 accuracy= 0.65675 accuracy_s= 0.75954 accuracy_f= 0.82567 time= 0.07632
Epoch: 0051 train_loss= 6.00436 feature_loss= 5.52301 structure_loss= 0.48136 accuracy= 0.65675 accuracy_s= 0.75954 accuracy_f= 0.82567 time= 0.07660
Epoch: 0061 train_loss= 6.00113 feature_loss= 5.51928 structure_loss= 0.48185 accuracy= 0.65675 accuracy_s= 0.75954 accuracy_f= 0.82627 time= 0.07607
Epoch: 0071 train_loss= 5.99948 feature_loss= 5.51691 structure_loss= 0.48257 accuracy= 0.65795 accuracy_s= 0.75954 accuracy_f= 0.82567 time= 0.07658
Epoch: 0081 train_loss= 5.99883 feature_loss= 5.51540 structure_loss= 0.48343 accuracy= 0.65735 accuracy_s= 0.75954 accuracy_f= 0.82627 time= 0.07665
Epoch: 0091 train_loss= 5.99881 feature_loss= 5.51444 structure_loss= 0.48437 accuracy= 0.65735 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07662
Epoch: 0101 train_loss= 5.99919 feature_loss= 5.51383 structure_loss= 0.48536 accuracy= 0.65735 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07613
Epoch: 0111 train_loss= 5.99978 feature_loss= 5.51345 structure_loss= 0.48634 accuracy= 0.65735 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07753
Epoch: 0121 train_loss= 6.00047 feature_loss= 5.51321 structure_loss= 0.48726 accuracy= 0.65735 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07685
Epoch: 0131 train_loss= 6.00127 feature_loss= 5.51306 structure_loss= 0.48820 accuracy= 0.65735 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07698
Epoch: 0141 train_loss= 6.00197 feature_loss= 5.51298 structure_loss= 0.48899 accuracy= 0.65675 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07665
Epoch: 0151 train_loss= 6.00273 feature_loss= 5.51293 structure_loss= 0.48981 accuracy= 0.65675 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07666
Epoch: 0161 train_loss= 6.00343 feature_loss= 5.51290 structure_loss= 0.49054 accuracy= 0.65675 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07630
Epoch: 0171 train_loss= 6.00410 feature_loss= 5.51288 structure_loss= 0.49122 accuracy= 0.65735 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07675
Epoch: 0181 train_loss= 6.00472 feature_loss= 5.51287 structure_loss= 0.49185 accuracy= 0.65615 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07684
Epoch: 0191 train_loss= 6.00525 feature_loss= 5.51286 structure_loss= 0.49239 accuracy= 0.65675 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07709
Epoch: 0201 train_loss= 6.00573 feature_loss= 5.51286 structure_loss= 0.49287 accuracy= 0.65675 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07695
Epoch: 0211 train_loss= 6.00615 feature_loss= 5.51286 structure_loss= 0.49329 accuracy= 0.65615 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07675
Epoch: 0221 train_loss= 6.00654 feature_loss= 5.51286 structure_loss= 0.49368 accuracy= 0.65615 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07665
Epoch: 0231 train_loss= 6.00688 feature_loss= 5.51286 structure_loss= 0.49402 accuracy= 0.65615 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07687
Epoch: 0241 train_loss= 6.00715 feature_loss= 5.51286 structure_loss= 0.49429 accuracy= 0.65615 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07691
Epoch: 0251 train_loss= 6.00745 feature_loss= 5.51286 structure_loss= 0.49459 accuracy= 0.65615 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07694
Epoch: 0261 train_loss= 6.00764 feature_loss= 5.51286 structure_loss= 0.49478 accuracy= 0.65615 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07616
Epoch: 0271 train_loss= 6.00786 feature_loss= 5.51286 structure_loss= 0.49501 accuracy= 0.65615 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07661
Epoch: 0281 train_loss= 6.00805 feature_loss= 5.51286 structure_loss= 0.49519 accuracy= 0.65555 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07689
Epoch: 0291 train_loss= 6.00823 feature_loss= 5.51286 structure_loss= 0.49538 accuracy= 0.65555 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.07662

accuracy 0.65555
accuracy_s 0.75954
accuracy_f 0.82687
auc 0.51527
f1_score 0.28375
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=2, beta=1, clique_size=20, cuda=False, dataset='citeseer', device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 476
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>
using PCA to reduce dim
y_features shape after PCA torch.Size([3327, 925]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 220.81027 feature_loss= 59.54392 structure_loss= 161.26634 accuracy= 0.78599 accuracy_s= 0.91103 accuracy_f= 0.79441 time= 0.08667
Epoch: 0011 train_loss= 23.37179 feature_loss= 8.80408 structure_loss= 14.56771 accuracy= 0.58882 accuracy_s= 0.76075 accuracy_f= 0.81785 time= 0.06871
Epoch: 0021 train_loss= 9.59103 feature_loss= 7.06149 structure_loss= 2.52954 accuracy= 0.62068 accuracy_s= 0.75954 accuracy_f= 0.82567 time= 0.06872
Epoch: 0031 train_loss= 7.45283 feature_loss= 7.02421 structure_loss= 0.42862 accuracy= 0.66336 accuracy_s= 0.75954 accuracy_f= 0.82627 time= 0.06808
Epoch: 0041 train_loss= 7.41462 feature_loss= 6.98398 structure_loss= 0.43064 accuracy= 0.66035 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06810
Epoch: 0051 train_loss= 7.36986 feature_loss= 6.95822 structure_loss= 0.41164 accuracy= 0.65735 accuracy_s= 0.75954 accuracy_f= 0.82627 time= 0.06818
Epoch: 0061 train_loss= 7.33833 feature_loss= 6.94397 structure_loss= 0.39435 accuracy= 0.65855 accuracy_s= 0.75954 accuracy_f= 0.82507 time= 0.06807
Epoch: 0071 train_loss= 7.31184 feature_loss= 6.93129 structure_loss= 0.38055 accuracy= 0.66035 accuracy_s= 0.75954 accuracy_f= 0.82567 time= 0.06841
Epoch: 0081 train_loss= 7.28819 feature_loss= 6.91862 structure_loss= 0.36957 accuracy= 0.66035 accuracy_s= 0.75954 accuracy_f= 0.82567 time= 0.06862
Epoch: 0091 train_loss= 7.26429 feature_loss= 6.90644 structure_loss= 0.35786 accuracy= 0.66456 accuracy_s= 0.75954 accuracy_f= 0.82567 time= 0.06826
Epoch: 0101 train_loss= 7.24305 feature_loss= 6.89415 structure_loss= 0.34890 accuracy= 0.66576 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06814
Epoch: 0111 train_loss= 7.49466 feature_loss= 6.88369 structure_loss= 0.61097 accuracy= 0.67779 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06833
Epoch: 0121 train_loss= 8.83458 feature_loss= 6.89898 structure_loss= 1.93560 accuracy= 0.68861 accuracy_s= 0.81244 accuracy_f= 0.82567 time= 0.06779
Epoch: 0131 train_loss= 7.45996 feature_loss= 6.86609 structure_loss= 0.59387 accuracy= 0.67418 accuracy_s= 0.75954 accuracy_f= 0.82627 time= 0.06823
Epoch: 0141 train_loss= 7.28489 feature_loss= 6.85254 structure_loss= 0.43236 accuracy= 0.67418 accuracy_s= 0.75954 accuracy_f= 0.82627 time= 0.06799
Epoch: 0151 train_loss= 7.18594 feature_loss= 6.83975 structure_loss= 0.34619 accuracy= 0.67118 accuracy_s= 0.75954 accuracy_f= 0.82567 time= 0.06806
Epoch: 0161 train_loss= 7.13989 feature_loss= 6.82601 structure_loss= 0.31388 accuracy= 0.67118 accuracy_s= 0.75954 accuracy_f= 0.82567 time= 0.06935
Epoch: 0171 train_loss= 7.11069 feature_loss= 6.81448 structure_loss= 0.29621 accuracy= 0.67057 accuracy_s= 0.75954 accuracy_f= 0.82567 time= 0.06880
Epoch: 0181 train_loss= 7.13742 feature_loss= 6.85182 structure_loss= 0.28560 accuracy= 0.66997 accuracy_s= 0.75954 accuracy_f= 0.82807 time= 0.06787
Epoch: 0191 train_loss= 7.06612 feature_loss= 6.79274 structure_loss= 0.27338 accuracy= 0.66757 accuracy_s= 0.75954 accuracy_f= 0.82507 time= 0.06866
Epoch: 0201 train_loss= 7.07070 feature_loss= 6.80923 structure_loss= 0.26147 accuracy= 0.66937 accuracy_s= 0.75954 accuracy_f= 0.82507 time= 0.06839
Epoch: 0211 train_loss= 7.06942 feature_loss= 6.81466 structure_loss= 0.25477 accuracy= 0.67118 accuracy_s= 0.75954 accuracy_f= 0.82567 time= 0.06827
Epoch: 0221 train_loss= 7.22607 feature_loss= 6.98118 structure_loss= 0.24489 accuracy= 0.66877 accuracy_s= 0.75954 accuracy_f= 0.81605 time= 0.06810
Epoch: 0231 train_loss= 7.20058 feature_loss= 6.96279 structure_loss= 0.23780 accuracy= 0.66156 accuracy_s= 0.75954 accuracy_f= 0.81846 time= 0.06871
Epoch: 0241 train_loss= 7.00839 feature_loss= 6.77577 structure_loss= 0.23261 accuracy= 0.66877 accuracy_s= 0.75954 accuracy_f= 0.82507 time= 0.06843
Epoch: 0251 train_loss= 6.90570 feature_loss= 6.67799 structure_loss= 0.22770 accuracy= 0.67178 accuracy_s= 0.75954 accuracy_f= 0.82627 time= 0.06824
Epoch: 0261 train_loss= 6.83052 feature_loss= 6.60761 structure_loss= 0.22291 accuracy= 0.67238 accuracy_s= 0.75954 accuracy_f= 0.82627 time= 0.06801
Epoch: 0271 train_loss= 6.74501 feature_loss= 6.52859 structure_loss= 0.21642 accuracy= 0.67298 accuracy_s= 0.75954 accuracy_f= 0.82627 time= 0.06847
Epoch: 0281 train_loss= 6.67935 feature_loss= 6.46575 structure_loss= 0.21360 accuracy= 0.67418 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06844
Epoch: 0291 train_loss= 6.61234 feature_loss= 6.40208 structure_loss= 0.21026 accuracy= 0.67358 accuracy_s= 0.75954 accuracy_f= 0.82627 time= 0.06913

accuracy 0.67358
accuracy_s 0.75954
accuracy_f 0.82687
auc 0.57322
f1_score 0.32125
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=2, beta=1, clique_size=20, cuda=False, dataset='citeseer', device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 449
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 282 steps!
y_features shape after FMS torch.Size([3327, 925]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 33.69166 feature_loss= 33.22064 structure_loss= 0.47102 accuracy= 0.62669 accuracy_s= 0.75954 accuracy_f= 0.79321 time= 0.08594
Epoch: 0011 train_loss= 6.06691 feature_loss= 5.58144 structure_loss= 0.48547 accuracy= 0.66035 accuracy_s= 0.75954 accuracy_f= 0.82627 time= 0.06701
Epoch: 0021 train_loss= 6.03961 feature_loss= 5.55403 structure_loss= 0.48557 accuracy= 0.65555 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06742
Epoch: 0031 train_loss= 6.02397 feature_loss= 5.53848 structure_loss= 0.48549 accuracy= 0.65494 accuracy_s= 0.75954 accuracy_f= 0.82627 time= 0.06733
Epoch: 0041 train_loss= 6.01346 feature_loss= 5.52897 structure_loss= 0.48449 accuracy= 0.65555 accuracy_s= 0.75954 accuracy_f= 0.82567 time= 0.06753
Epoch: 0051 train_loss= 6.00687 feature_loss= 5.52304 structure_loss= 0.48383 accuracy= 0.65555 accuracy_s= 0.75954 accuracy_f= 0.82567 time= 0.06712
Epoch: 0061 train_loss= 6.00300 feature_loss= 5.51931 structure_loss= 0.48369 accuracy= 0.65555 accuracy_s= 0.75954 accuracy_f= 0.82627 time= 0.06734
Epoch: 0071 train_loss= 6.00079 feature_loss= 5.51694 structure_loss= 0.48385 accuracy= 0.65555 accuracy_s= 0.75954 accuracy_f= 0.82567 time= 0.06735
Epoch: 0081 train_loss= 5.99958 feature_loss= 5.51542 structure_loss= 0.48416 accuracy= 0.65615 accuracy_s= 0.75954 accuracy_f= 0.82567 time= 0.06748
Epoch: 0091 train_loss= 5.99904 feature_loss= 5.51445 structure_loss= 0.48459 accuracy= 0.65675 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06906
Epoch: 0101 train_loss= 5.99894 feature_loss= 5.51384 structure_loss= 0.48510 accuracy= 0.65675 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06887
Epoch: 0111 train_loss= 5.99913 feature_loss= 5.51345 structure_loss= 0.48568 accuracy= 0.65615 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06787
Epoch: 0121 train_loss= 5.99944 feature_loss= 5.51321 structure_loss= 0.48622 accuracy= 0.65675 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06790
Epoch: 0131 train_loss= 5.99990 feature_loss= 5.51307 structure_loss= 0.48683 accuracy= 0.65615 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06734
Epoch: 0141 train_loss= 6.00043 feature_loss= 5.51298 structure_loss= 0.48745 accuracy= 0.65615 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06752
Epoch: 0151 train_loss= 6.00101 feature_loss= 5.51293 structure_loss= 0.48808 accuracy= 0.65675 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06755
Epoch: 0161 train_loss= 6.00158 feature_loss= 5.51290 structure_loss= 0.48868 accuracy= 0.65675 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06745
Epoch: 0171 train_loss= 6.00213 feature_loss= 5.51288 structure_loss= 0.48926 accuracy= 0.65615 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06781
Epoch: 0181 train_loss= 6.00269 feature_loss= 5.51287 structure_loss= 0.48982 accuracy= 0.65555 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06753
Epoch: 0191 train_loss= 6.00322 feature_loss= 5.51286 structure_loss= 0.49036 accuracy= 0.65555 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06735
Epoch: 0201 train_loss= 6.00372 feature_loss= 5.51286 structure_loss= 0.49087 accuracy= 0.65555 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06765
Epoch: 0211 train_loss= 6.00420 feature_loss= 5.51286 structure_loss= 0.49134 accuracy= 0.65615 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06745
Epoch: 0221 train_loss= 6.00464 feature_loss= 5.51286 structure_loss= 0.49178 accuracy= 0.65555 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06751
Epoch: 0231 train_loss= 6.00506 feature_loss= 5.51286 structure_loss= 0.49220 accuracy= 0.65555 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06772
Epoch: 0241 train_loss= 6.00543 feature_loss= 5.51286 structure_loss= 0.49257 accuracy= 0.65555 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06757
Epoch: 0251 train_loss= 6.00578 feature_loss= 5.51286 structure_loss= 0.49292 accuracy= 0.65615 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06749
Epoch: 0261 train_loss= 6.00605 feature_loss= 5.51286 structure_loss= 0.49320 accuracy= 0.65615 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06819
Epoch: 0271 train_loss= 6.00629 feature_loss= 5.51286 structure_loss= 0.49343 accuracy= 0.65555 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06780
Epoch: 0281 train_loss= 6.00647 feature_loss= 5.51286 structure_loss= 0.49361 accuracy= 0.65555 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06772
Epoch: 0291 train_loss= 6.00651 feature_loss= 5.51286 structure_loss= 0.49366 accuracy= 0.65555 accuracy_s= 0.75954 accuracy_f= 0.82687 time= 0.06754

accuracy 0.65555
accuracy_s 0.75954
accuracy_f 0.82687
auc 0.51366
f1_score 0.28375
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=3, beta=1, clique_size=20, cuda=False, dataset='citeseer', device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 628
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>
using PCA to reduce dim
y_features shape after PCA torch.Size([3327, 925]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 8661.41602 feature_loss= 59.41045 structure_loss= 8602.00586 accuracy= 0.74451 accuracy_s= 0.86414 accuracy_f= 0.79862 time= 0.09393
Epoch: 0011 train_loss= 4956.74854 feature_loss= 26.53454 structure_loss= 4930.21387 accuracy= 0.59062 accuracy_s= 0.76135 accuracy_f= 0.80162 time= 0.07202
Epoch: 0021 train_loss= 4205.43750 feature_loss= 14.68864 structure_loss= 4190.74902 accuracy= 0.58401 accuracy_s= 0.75954 accuracy_f= 0.82387 time= 0.07204
Epoch: 0031 train_loss= 3753.50195 feature_loss= 11.11340 structure_loss= 3742.38867 accuracy= 0.58281 accuracy_s= 0.75954 accuracy_f= 0.83348 time= 0.07279
Epoch: 0041 train_loss= 3424.24512 feature_loss= 9.80950 structure_loss= 3414.43555 accuracy= 0.58461 accuracy_s= 0.75954 accuracy_f= 0.83649 time= 0.07254
Epoch: 0051 train_loss= 3157.47070 feature_loss= 8.78042 structure_loss= 3148.69019 accuracy= 0.58281 accuracy_s= 0.75954 accuracy_f= 0.83649 time= 0.07288
Epoch: 0061 train_loss= 2937.12476 feature_loss= 8.26924 structure_loss= 2928.85547 accuracy= 0.58221 accuracy_s= 0.75954 accuracy_f= 0.84370 time= 0.07233
Epoch: 0071 train_loss= 2750.13013 feature_loss= 7.63350 structure_loss= 2742.49658 accuracy= 0.58281 accuracy_s= 0.75954 accuracy_f= 0.84731 time= 0.07189
Epoch: 0081 train_loss= 2586.09155 feature_loss= 7.09798 structure_loss= 2578.99365 accuracy= 0.58341 accuracy_s= 0.75954 accuracy_f= 0.84911 time= 0.07272
Epoch: 0091 train_loss= 2427.31958 feature_loss= 6.64133 structure_loss= 2420.67822 accuracy= 0.58341 accuracy_s= 0.75954 accuracy_f= 0.85392 time= 0.07263
Epoch: 0101 train_loss= 2293.57837 feature_loss= 6.25521 structure_loss= 2287.32324 accuracy= 0.58461 accuracy_s= 0.75954 accuracy_f= 0.85813 time= 0.07256
Epoch: 0111 train_loss= 2173.44580 feature_loss= 6.19068 structure_loss= 2167.25513 accuracy= 0.58521 accuracy_s= 0.75954 accuracy_f= 0.85332 time= 0.07264
Epoch: 0121 train_loss= 2060.76343 feature_loss= 6.11962 structure_loss= 2054.64380 accuracy= 0.58461 accuracy_s= 0.75954 accuracy_f= 0.85152 time= 0.07217
Epoch: 0131 train_loss= 1957.74634 feature_loss= 6.12563 structure_loss= 1951.62073 accuracy= 0.58461 accuracy_s= 0.75954 accuracy_f= 0.84430 time= 0.07213
Epoch: 0141 train_loss= 1863.99316 feature_loss= 6.03582 structure_loss= 1857.95740 accuracy= 0.58401 accuracy_s= 0.75954 accuracy_f= 0.85212 time= 0.07222
Epoch: 0151 train_loss= 1772.14758 feature_loss= 6.05587 structure_loss= 1766.09167 accuracy= 0.58401 accuracy_s= 0.75954 accuracy_f= 0.84370 time= 0.07271
Epoch: 0161 train_loss= 1699.99292 feature_loss= 6.13065 structure_loss= 1693.86230 accuracy= 0.58461 accuracy_s= 0.75954 accuracy_f= 0.84491 time= 0.07133
Epoch: 0171 train_loss= 1617.14368 feature_loss= 6.13853 structure_loss= 1611.00513 accuracy= 0.58461 accuracy_s= 0.75954 accuracy_f= 0.83529 time= 0.07277
Epoch: 0181 train_loss= 1548.16345 feature_loss= 6.07667 structure_loss= 1542.08679 accuracy= 0.58401 accuracy_s= 0.75954 accuracy_f= 0.83288 time= 0.07266
Epoch: 0191 train_loss= 1476.25696 feature_loss= 5.91818 structure_loss= 1470.33875 accuracy= 0.58341 accuracy_s= 0.75954 accuracy_f= 0.85092 time= 0.07248
Epoch: 0201 train_loss= 1413.69519 feature_loss= 5.95469 structure_loss= 1407.74048 accuracy= 0.58461 accuracy_s= 0.75954 accuracy_f= 0.84731 time= 0.07218
Epoch: 0211 train_loss= 1351.39343 feature_loss= 5.88632 structure_loss= 1345.50708 accuracy= 0.58521 accuracy_s= 0.75954 accuracy_f= 0.85272 time= 0.07236
Epoch: 0221 train_loss= 1288.46887 feature_loss= 5.84043 structure_loss= 1282.62842 accuracy= 0.58401 accuracy_s= 0.75954 accuracy_f= 0.84731 time= 0.07242
Epoch: 0231 train_loss= 1234.21472 feature_loss= 6.00474 structure_loss= 1228.20996 accuracy= 0.58461 accuracy_s= 0.75954 accuracy_f= 0.83348 time= 0.07262
Epoch: 0241 train_loss= 1177.00049 feature_loss= 5.99354 structure_loss= 1171.00696 accuracy= 0.58401 accuracy_s= 0.75954 accuracy_f= 0.83529 time= 0.07305
Epoch: 0251 train_loss= 1127.58923 feature_loss= 5.78751 structure_loss= 1121.80176 accuracy= 0.58641 accuracy_s= 0.75954 accuracy_f= 0.84551 time= 0.07294
Epoch: 0261 train_loss= 1078.48608 feature_loss= 5.81821 structure_loss= 1072.66785 accuracy= 0.59002 accuracy_s= 0.75954 accuracy_f= 0.84370 time= 0.07278
Epoch: 0271 train_loss= 1029.80945 feature_loss= 5.84761 structure_loss= 1023.96185 accuracy= 0.58822 accuracy_s= 0.75954 accuracy_f= 0.84491 time= 0.07273
Epoch: 0281 train_loss= 983.63147 feature_loss= 5.84877 structure_loss= 977.78271 accuracy= 0.58702 accuracy_s= 0.75954 accuracy_f= 0.84731 time= 0.07305
Epoch: 0291 train_loss= 940.14819 feature_loss= 5.79470 structure_loss= 934.35352 accuracy= 0.58882 accuracy_s= 0.75954 accuracy_f= 0.84370 time= 0.07287

accuracy 0.58762
accuracy_s 0.75954
accuracy_f 0.84310
auc 0.31596
f1_score 0.14250
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=3, beta=1, clique_size=20, cuda=False, dataset='citeseer', device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=20, weight_decay=0.0005)
random seed: 722
Found existing ad data, loading...
feature_dim: 3703 hidden1_dim: 1851 hidden2_dim: 925 nodes_num: 3327 anomaly_num: 800

Start modeling
using wavelet scattering transform
y_features shape after scatting (3327, 48139) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 444 steps!
y_features shape after FMS torch.Size([3327, 925]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 8616.84570 feature_loss= 59.39011 structure_loss= 8557.45605 accuracy= 0.78659 accuracy_s= 0.90021 accuracy_f= 0.79681 time= 0.09495
Epoch: 0011 train_loss= 5002.25781 feature_loss= 28.49885 structure_loss= 4973.75879 accuracy= 0.59062 accuracy_s= 0.76014 accuracy_f= 0.80763 time= 0.07136
Epoch: 0021 train_loss= 4208.57568 feature_loss= 16.97910 structure_loss= 4191.59668 accuracy= 0.58641 accuracy_s= 0.76014 accuracy_f= 0.81545 time= 0.07226
Epoch: 0031 train_loss= 3723.74902 feature_loss= 12.03926 structure_loss= 3711.70972 accuracy= 0.58882 accuracy_s= 0.75954 accuracy_f= 0.83048 time= 0.07132
Epoch: 0041 train_loss= 3383.68750 feature_loss= 10.19473 structure_loss= 3373.49268 accuracy= 0.58581 accuracy_s= 0.75954 accuracy_f= 0.83829 time= 0.07220
Epoch: 0051 train_loss= 3117.09155 feature_loss= 9.07055 structure_loss= 3108.02100 accuracy= 0.58762 accuracy_s= 0.75954 accuracy_f= 0.84310 time= 0.07186
Epoch: 0061 train_loss= 2886.55347 feature_loss= 8.12541 structure_loss= 2878.42798 accuracy= 0.58341 accuracy_s= 0.75954 accuracy_f= 0.84190 time= 0.07241
Epoch: 0071 train_loss= 2700.04517 feature_loss= 7.44702 structure_loss= 2692.59814 accuracy= 0.58401 accuracy_s= 0.75954 accuracy_f= 0.85212 time= 0.07165
Epoch: 0081 train_loss= 3014.78613 feature_loss= 7.15656 structure_loss= 3007.62964 accuracy= 0.59844 accuracy_s= 0.75954 accuracy_f= 0.83829 time= 0.07167
Epoch: 0091 train_loss= 2618.66406 feature_loss= 6.92872 structure_loss= 2611.73535 accuracy= 0.58401 accuracy_s= 0.75954 accuracy_f= 0.85392 time= 0.07142
Epoch: 0101 train_loss= 2402.54346 feature_loss= 6.78160 structure_loss= 2395.76196 accuracy= 0.58581 accuracy_s= 0.75954 accuracy_f= 0.85693 time= 0.07144
Epoch: 0111 train_loss= 2250.95898 feature_loss= 6.67142 structure_loss= 2244.28760 accuracy= 0.58521 accuracy_s= 0.75954 accuracy_f= 0.85152 time= 0.07105
Epoch: 0121 train_loss= 2130.76807 feature_loss= 6.58878 structure_loss= 2124.17920 accuracy= 0.58581 accuracy_s= 0.75954 accuracy_f= 0.85152 time= 0.07134
Epoch: 0131 train_loss= 2022.12415 feature_loss= 6.35710 structure_loss= 2015.76709 accuracy= 0.58641 accuracy_s= 0.75954 accuracy_f= 0.84851 time= 0.07145
Epoch: 0141 train_loss= 1925.70056 feature_loss= 6.12912 structure_loss= 1919.57141 accuracy= 0.58822 accuracy_s= 0.75954 accuracy_f= 0.85392 time= 0.07144
Epoch: 0151 train_loss= 1837.97900 feature_loss= 6.00961 structure_loss= 1831.96936 accuracy= 0.58641 accuracy_s= 0.75954 accuracy_f= 0.85272 time= 0.07113
Epoch: 0161 train_loss= 1755.96582 feature_loss= 5.98583 structure_loss= 1749.97998 accuracy= 0.58641 accuracy_s= 0.75954 accuracy_f= 0.84731 time= 0.07105
Epoch: 0171 train_loss= 1675.83594 feature_loss= 6.02648 structure_loss= 1669.80945 accuracy= 0.58641 accuracy_s= 0.75954 accuracy_f= 0.83709 time= 0.07106
Epoch: 0181 train_loss= 1603.41492 feature_loss= 6.27312 structure_loss= 1597.14185 accuracy= 0.58581 accuracy_s= 0.75954 accuracy_f= 0.82146 time= 0.07116
Epoch: 0191 train_loss= 1530.47595 feature_loss= 6.00097 structure_loss= 1524.47498 accuracy= 0.58581 accuracy_s= 0.75954 accuracy_f= 0.84130 time= 0.07130
Epoch: 0201 train_loss= 1466.23755 feature_loss= 5.82784 structure_loss= 1460.40967 accuracy= 0.58641 accuracy_s= 0.75954 accuracy_f= 0.85272 time= 0.07159
Epoch: 0211 train_loss= 1399.90930 feature_loss= 5.83112 structure_loss= 1394.07812 accuracy= 0.58641 accuracy_s= 0.75954 accuracy_f= 0.84911 time= 0.07172
Epoch: 0221 train_loss= 1338.28760 feature_loss= 5.90687 structure_loss= 1332.38074 accuracy= 0.58401 accuracy_s= 0.75954 accuracy_f= 0.83950 time= 0.07092
Epoch: 0231 train_loss= 1280.24500 feature_loss= 5.83290 structure_loss= 1274.41211 accuracy= 0.58461 accuracy_s= 0.75954 accuracy_f= 0.84551 time= 0.07137
Epoch: 0241 train_loss= 1225.73938 feature_loss= 6.09669 structure_loss= 1219.64270 accuracy= 0.58702 accuracy_s= 0.75954 accuracy_f= 0.82627 time= 0.07173
Epoch: 0251 train_loss= 1170.05054 feature_loss= 5.91535 structure_loss= 1164.13513 accuracy= 0.58641 accuracy_s= 0.75954 accuracy_f= 0.84551 time= 0.07160
Epoch: 0261 train_loss= 1120.04614 feature_loss= 5.76495 structure_loss= 1114.28125 accuracy= 0.58641 accuracy_s= 0.75954 accuracy_f= 0.84911 time= 0.07152
Epoch: 0271 train_loss= 1070.57617 feature_loss= 5.69673 structure_loss= 1064.87939 accuracy= 0.58641 accuracy_s= 0.75954 accuracy_f= 0.85272 time= 0.07208
Epoch: 0281 train_loss= 1024.63318 feature_loss= 5.67860 structure_loss= 1018.95453 accuracy= 0.58281 accuracy_s= 0.75954 accuracy_f= 0.84671 time= 0.07176
Epoch: 0291 train_loss= 976.99823 feature_loss= 5.82540 structure_loss= 971.17285 accuracy= 0.58401 accuracy_s= 0.75954 accuracy_f= 0.84190 time= 0.07159

accuracy 0.58461
accuracy_s 0.75954
accuracy_f 0.84130
auc 0.28790
f1_score 0.13625
Job finished!
citeseer job finished!
pubmed experiment with fixed GCN hidden layer



Initializing normal onedecoder
Namespace(alpha=1, beta=1, clique_size=50, cuda=False, dataset='pubmed', decoder=0, device=device(type='cuda'), dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 688
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
Epoch: 0001 train_loss= 20.16501 accuracy= 0.63838 time= 0.19616
Epoch: 0011 train_loss= 3.57258 accuracy= 0.66597 time= 0.17452
Epoch: 0021 train_loss= 1.05714 accuracy= 0.71375 time= 0.17373
Epoch: 0031 train_loss= 1.00367 accuracy= 0.72247 time= 0.17337
Epoch: 0041 train_loss= 0.91774 accuracy= 0.72339 time= 0.17402
Epoch: 0051 train_loss= 0.90255 accuracy= 0.72217 time= 0.17496
Epoch: 0061 train_loss= 0.88950 accuracy= 0.72268 time= 0.17020
Epoch: 0071 train_loss= 0.86908 accuracy= 0.72268 time= 0.17519
Epoch: 0081 train_loss= 0.78352 accuracy= 0.72227 time= 0.17475
Epoch: 0091 train_loss= 0.76040 accuracy= 0.72186 time= 0.17503
Epoch: 0101 train_loss= 0.64990 accuracy= 0.71994 time= 0.17512
Epoch: 0111 train_loss= 0.57877 accuracy= 0.72217 time= 0.17445
Epoch: 0121 train_loss= 3.58843 accuracy= 0.79185 time= 0.17487
Epoch: 0131 train_loss= 0.57917 accuracy= 0.72034 time= 0.17383
Epoch: 0141 train_loss= 0.79029 accuracy= 0.72186 time= 0.17378
Epoch: 0151 train_loss= 0.84057 accuracy= 0.72237 time= 0.17498
Epoch: 0161 train_loss= 0.82663 accuracy= 0.72247 time= 0.17541
Epoch: 0171 train_loss= 0.78175 accuracy= 0.72217 time= 0.17280
Epoch: 0181 train_loss= 0.77255 accuracy= 0.72207 time= 0.17552
Epoch: 0191 train_loss= 0.76772 accuracy= 0.72237 time= 0.17016
Epoch: 0201 train_loss= 0.76297 accuracy= 0.72247 time= 0.17478
Epoch: 0211 train_loss= 0.76003 accuracy= 0.72227 time= 0.17554
Epoch: 0221 train_loss= 0.75575 accuracy= 0.72247 time= 0.17686
Epoch: 0231 train_loss= 0.75230 accuracy= 0.72257 time= 0.17521
Epoch: 0241 train_loss= 0.74895 accuracy= 0.72257 time= 0.17663
Epoch: 0251 train_loss= 0.74524 accuracy= 0.72268 time= 0.17663
Epoch: 0261 train_loss= 0.74040 accuracy= 0.72257 time= 0.17761
Epoch: 0271 train_loss= 0.73271 accuracy= 0.72257 time= 0.17344
Epoch: 0281 train_loss= 0.72583 accuracy= 0.72227 time= 0.17525
Epoch: 0291 train_loss= 0.71491 accuracy= 0.72247 time= 0.17078

accuracy 0.72268
auc 0.61609
f1_score 0.31650
Job finished!



Initializing normal onedecoder
Namespace(alpha=1, beta=1, clique_size=50, cuda=False, dataset='pubmed', decoder=1, device=device(type='cuda'), dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 317
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
Epoch: 0001 train_loss= 4976.34619 accuracy= 0.63889 time= 0.19401
Epoch: 0011 train_loss= 3149.27124 accuracy= 0.64224 time= 0.16272
Epoch: 0021 train_loss= 2673.74194 accuracy= 0.64051 time= 0.16995
Epoch: 0031 train_loss= 2225.98901 accuracy= 0.64061 time= 0.17129
Epoch: 0041 train_loss= 1043.23499 accuracy= 0.64467 time= 0.17060
Epoch: 0051 train_loss= 382.92755 accuracy= 0.66405 time= 0.16909
Epoch: 0061 train_loss= 119.96483 accuracy= 0.66405 time= 0.16958
Epoch: 0071 train_loss= 71.40739 accuracy= 0.67348 time= 0.16914
Epoch: 0081 train_loss= 23.33544 accuracy= 0.64782 time= 0.16908
Epoch: 0091 train_loss= 5.05199 accuracy= 0.64477 time= 0.16906
Epoch: 0101 train_loss= 1.70838 accuracy= 0.64305 time= 0.16882
Epoch: 0111 train_loss= 1.04015 accuracy= 0.64061 time= 0.16903
Epoch: 0121 train_loss= 0.70948 accuracy= 0.64345 time= 0.16949
Epoch: 0131 train_loss= 0.55353 accuracy= 0.64406 time= 0.16932
Epoch: 0141 train_loss= 0.48780 accuracy= 0.64234 time= 0.16921
Epoch: 0151 train_loss= 0.45432 accuracy= 0.64234 time= 0.16913
Epoch: 0161 train_loss= 0.43889 accuracy= 0.64274 time= 0.16967
Epoch: 0171 train_loss= 0.43103 accuracy= 0.64203 time= 0.17120
Epoch: 0181 train_loss= 0.42686 accuracy= 0.64244 time= 0.17099
Epoch: 0191 train_loss= 0.42384 accuracy= 0.64214 time= 0.17006
Epoch: 0201 train_loss= 0.42294 accuracy= 0.64264 time= 0.17016
Epoch: 0211 train_loss= 0.42141 accuracy= 0.64254 time= 0.16965
Epoch: 0221 train_loss= 0.41982 accuracy= 0.64224 time= 0.16992
Epoch: 0231 train_loss= 0.44600 accuracy= 0.64234 time= 0.16992
Epoch: 0241 train_loss= 0.41858 accuracy= 0.64264 time= 0.17059
Epoch: 0251 train_loss= 0.41827 accuracy= 0.64274 time= 0.16985
Epoch: 0261 train_loss= 0.41877 accuracy= 0.64254 time= 0.17009
Epoch: 0271 train_loss= 0.41756 accuracy= 0.64224 time= 0.17014
Epoch: 0281 train_loss= 0.41765 accuracy= 0.64234 time= 0.16999
Epoch: 0291 train_loss= 0.41769 accuracy= 0.64295 time= 0.17016

accuracy 0.64285
auc 0.25616
f1_score 0.11975
Job finished!



Initializing normal twodecoders
Namespace(alpha=1, beta=1, clique_size=50, cuda=False, dataset='pubmed', device=device(type='cuda'), dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 563
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
Epoch: 0001 train_loss= 4838.54102 feature_loss= 20.23541 structure_loss= 4818.30566 accuracy= 0.64863 accuracy_s= 0.79916 accuracy_f= 0.81803 time= 0.33007
Epoch: 0011 train_loss= 3112.00098 feature_loss= 13.30220 structure_loss= 3098.69873 accuracy= 0.64234 accuracy_s= 0.79926 accuracy_f= 0.81681 time= 0.29679
Epoch: 0021 train_loss= 2646.96094 feature_loss= 8.49632 structure_loss= 2638.46460 accuracy= 0.64092 accuracy_s= 0.79835 accuracy_f= 0.81874 time= 0.29705
Epoch: 0031 train_loss= 2185.33447 feature_loss= 3.92506 structure_loss= 2181.40942 accuracy= 0.64143 accuracy_s= 0.79764 accuracy_f= 0.81995 time= 0.29670
Epoch: 0041 train_loss= 1005.06958 feature_loss= 1.83032 structure_loss= 1003.23926 accuracy= 0.64650 accuracy_s= 0.79774 accuracy_f= 0.83994 time= 0.29455
Epoch: 0051 train_loss= 399.86661 feature_loss= 1.55616 structure_loss= 398.31046 accuracy= 0.66831 accuracy_s= 0.80443 accuracy_f= 0.84085 time= 0.29465
Epoch: 0061 train_loss= 176.84445 feature_loss= 1.11909 structure_loss= 175.72537 accuracy= 0.68951 accuracy_s= 0.81488 accuracy_f= 0.84217 time= 0.29601
Epoch: 0071 train_loss= 108.18593 feature_loss= 0.78835 structure_loss= 107.39758 accuracy= 0.69965 accuracy_s= 0.81376 accuracy_f= 0.84724 time= 0.29374
Epoch: 0081 train_loss= 54.88686 feature_loss= 0.64463 structure_loss= 54.24223 accuracy= 0.70604 accuracy_s= 0.81234 accuracy_f= 0.84896 time= 0.29210
Epoch: 0091 train_loss= 5.99054 feature_loss= 0.92643 structure_loss= 5.06411 accuracy= 0.70462 accuracy_s= 0.79713 accuracy_f= 0.83385 time= 0.29021
Epoch: 0101 train_loss= 2.42819 feature_loss= 0.67542 structure_loss= 1.75277 accuracy= 0.70665 accuracy_s= 0.79713 accuracy_f= 0.85048 time= 0.29061
Epoch: 0111 train_loss= 1.56488 feature_loss= 0.48644 structure_loss= 1.07845 accuracy= 0.70371 accuracy_s= 0.79713 accuracy_f= 0.85160 time= 0.29142
Epoch: 0121 train_loss= 1.13201 feature_loss= 0.40028 structure_loss= 0.73173 accuracy= 0.69671 accuracy_s= 0.79713 accuracy_f= 0.85211 time= 0.29330
Epoch: 0131 train_loss= 0.95729 feature_loss= 0.39652 structure_loss= 0.56077 accuracy= 0.69387 accuracy_s= 0.79713 accuracy_f= 0.85180 time= 0.29489
Epoch: 0141 train_loss= 0.87494 feature_loss= 0.38760 structure_loss= 0.48734 accuracy= 0.69194 accuracy_s= 0.79713 accuracy_f= 0.85180 time= 0.29559
Epoch: 0151 train_loss= 0.85538 feature_loss= 0.38823 structure_loss= 0.46715 accuracy= 0.69062 accuracy_s= 0.79713 accuracy_f= 0.85190 time= 0.29418
Epoch: 0161 train_loss= 0.83172 feature_loss= 0.38615 structure_loss= 0.44556 accuracy= 0.68961 accuracy_s= 0.79713 accuracy_f= 0.85201 time= 0.29270
Epoch: 0171 train_loss= 0.83215 feature_loss= 0.38551 structure_loss= 0.44665 accuracy= 0.68920 accuracy_s= 0.79713 accuracy_f= 0.85211 time= 0.29294
Epoch: 0181 train_loss= 0.81757 feature_loss= 0.38583 structure_loss= 0.43174 accuracy= 0.68930 accuracy_s= 0.79713 accuracy_f= 0.85211 time= 0.29435
Epoch: 0191 train_loss= 0.81108 feature_loss= 0.38709 structure_loss= 0.42399 accuracy= 0.68890 accuracy_s= 0.79713 accuracy_f= 0.85190 time= 0.29144
Epoch: 0201 train_loss= 0.82441 feature_loss= 0.38748 structure_loss= 0.43693 accuracy= 0.68890 accuracy_s= 0.79713 accuracy_f= 0.85211 time= 0.29091
Epoch: 0211 train_loss= 0.84372 feature_loss= 0.38543 structure_loss= 0.45829 accuracy= 0.68890 accuracy_s= 0.79713 accuracy_f= 0.85201 time= 0.29069
Epoch: 0221 train_loss= 0.80842 feature_loss= 0.38733 structure_loss= 0.42110 accuracy= 0.68890 accuracy_s= 0.79713 accuracy_f= 0.85211 time= 0.29225
Epoch: 0231 train_loss= 0.80639 feature_loss= 0.38724 structure_loss= 0.41915 accuracy= 0.68880 accuracy_s= 0.79713 accuracy_f= 0.85190 time= 0.29305
Epoch: 0241 train_loss= 0.80417 feature_loss= 0.38539 structure_loss= 0.41878 accuracy= 0.68890 accuracy_s= 0.79713 accuracy_f= 0.85211 time= 0.28921
Epoch: 0251 train_loss= 0.80453 feature_loss= 0.38539 structure_loss= 0.41914 accuracy= 0.68890 accuracy_s= 0.79713 accuracy_f= 0.85211 time= 0.29319
Epoch: 0261 train_loss= 0.81227 feature_loss= 0.38542 structure_loss= 0.42685 accuracy= 0.68880 accuracy_s= 0.79713 accuracy_f= 0.85211 time= 0.29094
Epoch: 0271 train_loss= 0.80335 feature_loss= 0.38580 structure_loss= 0.41756 accuracy= 0.68890 accuracy_s= 0.79713 accuracy_f= 0.85211 time= 0.29333
Epoch: 0281 train_loss= 0.81101 feature_loss= 0.39368 structure_loss= 0.41733 accuracy= 0.68890 accuracy_s= 0.79713 accuracy_f= 0.85201 time= 0.29316
Epoch: 0291 train_loss= 0.80351 feature_loss= 0.38575 structure_loss= 0.41775 accuracy= 0.68890 accuracy_s= 0.79713 accuracy_f= 0.85211 time= 0.29427

accuracy 0.68890
accuracy_s 0.79713
accuracy_f 0.85190
auc 0.36712
f1_score 0.23325
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=50, cuda=False, dataset='pubmed', decoder=1, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 102
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
using wavelet scattering transform
y_features shape after scatting (19717, 6500) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([19717, 125]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 5948.50684 accuracy= 0.74448 time= 0.15042
Epoch: 0011 train_loss= 3949.13232 accuracy= 0.71284 time= 0.11791
Epoch: 0021 train_loss= 3321.19873 accuracy= 0.70918 time= 0.11752
Epoch: 0031 train_loss= 2990.40576 accuracy= 0.71294 time= 0.11730
Epoch: 0041 train_loss= 2757.79736 accuracy= 0.70279 time= 0.11810
Epoch: 0051 train_loss= 2580.29810 accuracy= 0.69448 time= 0.11760
Epoch: 0061 train_loss= 2434.02368 accuracy= 0.69255 time= 0.11763
Epoch: 0071 train_loss= 2308.18677 accuracy= 0.68545 time= 0.11780
Epoch: 0081 train_loss= 2196.64966 accuracy= 0.68048 time= 0.11770
Epoch: 0091 train_loss= 2096.49658 accuracy= 0.67865 time= 0.11880
Epoch: 0101 train_loss= 2004.03918 accuracy= 0.67358 time= 0.11875
Epoch: 0111 train_loss= 1915.81445 accuracy= 0.67297 time= 0.12024
Epoch: 0121 train_loss= 1835.26990 accuracy= 0.67003 time= 0.11900
Epoch: 0131 train_loss= 1756.83948 accuracy= 0.66952 time= 0.11798
Epoch: 0141 train_loss= 1682.62720 accuracy= 0.67084 time= 0.11859
Epoch: 0151 train_loss= 1611.26904 accuracy= 0.66912 time= 0.11791
Epoch: 0161 train_loss= 1542.69678 accuracy= 0.66851 time= 0.11864
Epoch: 0171 train_loss= 1477.40259 accuracy= 0.66973 time= 0.11842
Epoch: 0181 train_loss= 1414.04663 accuracy= 0.66912 time= 0.11887
Epoch: 0191 train_loss= 1353.16956 accuracy= 0.66871 time= 0.11825
Epoch: 0201 train_loss= 1294.63342 accuracy= 0.66831 time= 0.11890
Epoch: 0211 train_loss= 1237.86145 accuracy= 0.66770 time= 0.11950
Epoch: 0221 train_loss= 1183.97009 accuracy= 0.66821 time= 0.11942
Epoch: 0231 train_loss= 1130.98914 accuracy= 0.66729 time= 0.11905
Epoch: 0241 train_loss= 1080.19214 accuracy= 0.66750 time= 0.11884
Epoch: 0251 train_loss= 1031.13806 accuracy= 0.66810 time= 0.11893
Epoch: 0261 train_loss= 984.30725 accuracy= 0.66760 time= 0.11930
Epoch: 0271 train_loss= 939.29706 accuracy= 0.66760 time= 0.11902
Epoch: 0281 train_loss= 895.39807 accuracy= 0.66739 time= 0.11879
Epoch: 0291 train_loss= 853.03143 accuracy= 0.66679 time= 0.11930

accuracy 0.66790
auc 0.44790
f1_score 0.18150
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=50, cuda=False, dataset='pubmed', decoder=0, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 764
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
using wavelet scattering transform
y_features shape after scatting (19717, 6500) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([19717, 125]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 21.33574 accuracy= 0.73546 time= 0.16402
Epoch: 0011 train_loss= 16.56864 accuracy= 0.69782 time= 0.12618
Epoch: 0021 train_loss= 11.45705 accuracy= 0.68596 time= 0.12501
Epoch: 0031 train_loss= 7.66871 accuracy= 0.67165 time= 0.12616
Epoch: 0041 train_loss= 5.01661 accuracy= 0.66881 time= 0.12415
Epoch: 0051 train_loss= 3.73009 accuracy= 0.68514 time= 0.12157
Epoch: 0061 train_loss= 3.06086 accuracy= 0.70553 time= 0.12288
Epoch: 0071 train_loss= 2.45889 accuracy= 0.70898 time= 0.12469
Epoch: 0081 train_loss= 1.98732 accuracy= 0.71862 time= 0.12051
Epoch: 0091 train_loss= 1.50930 accuracy= 0.71862 time= 0.12407
Epoch: 0101 train_loss= 1.44265 accuracy= 0.72318 time= 0.12355
Epoch: 0111 train_loss= 1.37850 accuracy= 0.72460 time= 0.12419
Epoch: 0121 train_loss= 1.39395 accuracy= 0.72521 time= 0.12318
Epoch: 0131 train_loss= 1.37818 accuracy= 0.72511 time= 0.12388
Epoch: 0141 train_loss= 1.36976 accuracy= 0.72531 time= 0.12039
Epoch: 0151 train_loss= 1.37338 accuracy= 0.72531 time= 0.12518
Epoch: 0161 train_loss= 1.36233 accuracy= 0.72511 time= 0.12432
Epoch: 0171 train_loss= 1.35137 accuracy= 0.72531 time= 0.12467
Epoch: 0181 train_loss= 1.36617 accuracy= 0.72481 time= 0.12404
Epoch: 0191 train_loss= 1.35565 accuracy= 0.72582 time= 0.12500
Epoch: 0201 train_loss= 1.33941 accuracy= 0.72572 time= 0.12472
Epoch: 0211 train_loss= 1.33418 accuracy= 0.72562 time= 0.12481
Epoch: 0221 train_loss= 1.33753 accuracy= 0.72572 time= 0.12362
Epoch: 0231 train_loss= 1.32995 accuracy= 0.72531 time= 0.12523
Epoch: 0241 train_loss= 1.31732 accuracy= 0.72572 time= 0.12535
Epoch: 0251 train_loss= 1.29769 accuracy= 0.72572 time= 0.12046
Epoch: 0261 train_loss= 1.31318 accuracy= 0.72562 time= 0.12562
Epoch: 0271 train_loss= 1.29935 accuracy= 0.72612 time= 0.12468
Epoch: 0281 train_loss= 1.28533 accuracy= 0.72643 time= 0.12491
Epoch: 0291 train_loss= 1.26915 accuracy= 0.72440 time= 0.12539

accuracy 0.72582
auc 0.62172
f1_score 0.32425
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=50, cuda=False, dataset='pubmed', decoder=1, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 178
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
using wavelet scattering transform
y_features shape after scatting (19717, 6500) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 87 steps!

y_features shape after FMS torch.Size([19717, 125]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 6049.77197 accuracy= 0.76741 time= 0.15538
Epoch: 0011 train_loss= 4061.30029 accuracy= 0.80220 time= 0.11740
Epoch: 0021 train_loss= 3313.40234 accuracy= 0.77928 time= 0.11733
Epoch: 0031 train_loss= 2933.50049 accuracy= 0.76295 time= 0.11762
Epoch: 0041 train_loss= 2688.42725 accuracy= 0.76244 time= 0.11759
Epoch: 0051 train_loss= 2504.82739 accuracy= 0.76558 time= 0.11800
Epoch: 0061 train_loss= 2358.78979 accuracy= 0.77187 time= 0.11830
Epoch: 0071 train_loss= 2235.60132 accuracy= 0.77542 time= 0.11818
Epoch: 0081 train_loss= 2127.51123 accuracy= 0.77542 time= 0.11767
Epoch: 0091 train_loss= 2029.65247 accuracy= 0.77644 time= 0.11828
Epoch: 0101 train_loss= 1940.14209 accuracy= 0.77765 time= 0.11793
Epoch: 0111 train_loss= 1858.15369 accuracy= 0.77928 time= 0.11810
Epoch: 0121 train_loss= 1778.28674 accuracy= 0.77928 time= 0.11817
Epoch: 0131 train_loss= 1704.18164 accuracy= 0.77857 time= 0.11808
Epoch: 0141 train_loss= 1633.26917 accuracy= 0.77938 time= 0.11865
Epoch: 0151 train_loss= 1565.31201 accuracy= 0.77816 time= 0.11807
Epoch: 0161 train_loss= 1499.02539 accuracy= 0.77806 time= 0.11828
Epoch: 0171 train_loss= 1436.95691 accuracy= 0.77887 time= 0.11824
Epoch: 0181 train_loss= 1375.19873 accuracy= 0.77816 time= 0.11814
Epoch: 0191 train_loss= 1318.07422 accuracy= 0.77674 time= 0.11896
Epoch: 0201 train_loss= 1261.23596 accuracy= 0.77796 time= 0.11826
Epoch: 0211 train_loss= 1207.76428 accuracy= 0.77674 time= 0.11867
Epoch: 0221 train_loss= 1154.44360 accuracy= 0.77654 time= 0.11894
Epoch: 0231 train_loss= 1104.57373 accuracy= 0.77664 time= 0.11908
Epoch: 0241 train_loss= 1055.75500 accuracy= 0.77593 time= 0.11873
Epoch: 0251 train_loss= 1009.08252 accuracy= 0.77644 time= 0.11886
Epoch: 0261 train_loss= 963.35004 accuracy= 0.77613 time= 0.11911
Epoch: 0271 train_loss= 919.74957 accuracy= 0.77634 time= 0.11787
Epoch: 0281 train_loss= 877.98761 accuracy= 0.77593 time= 0.11883
Epoch: 0291 train_loss= 837.27649 accuracy= 0.77563 time= 0.11896

accuracy 0.77603
auc 0.66758
f1_score 0.44800
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=50, cuda=False, dataset='pubmed', decoder=0, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 920
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
using wavelet scattering transform
y_features shape after scatting (19717, 6500) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 87 steps!

y_features shape after FMS torch.Size([19717, 125]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 21.08688 accuracy= 0.80230 time= 0.15782
Epoch: 0011 train_loss= 16.07150 accuracy= 0.81732 time= 0.12222
Epoch: 0021 train_loss= 11.09608 accuracy= 0.80585 time= 0.12131
Epoch: 0031 train_loss= 7.29778 accuracy= 0.76883 time= 0.12054
Epoch: 0041 train_loss= 4.89293 accuracy= 0.72836 time= 0.12061
Epoch: 0051 train_loss= 3.15983 accuracy= 0.70107 time= 0.12379
Epoch: 0061 train_loss= 2.62925 accuracy= 0.71304 time= 0.12485
Epoch: 0071 train_loss= 2.13191 accuracy= 0.71994 time= 0.12359
Epoch: 0081 train_loss= 1.75544 accuracy= 0.72044 time= 0.12474
Epoch: 0091 train_loss= 1.56251 accuracy= 0.71740 time= 0.12452
Epoch: 0101 train_loss= 1.52320 accuracy= 0.72176 time= 0.12435
Epoch: 0111 train_loss= 1.49150 accuracy= 0.72450 time= 0.12440
Epoch: 0121 train_loss= 1.47584 accuracy= 0.72511 time= 0.12448
Epoch: 0131 train_loss= 1.46438 accuracy= 0.72511 time= 0.12521
Epoch: 0141 train_loss= 1.45468 accuracy= 0.72541 time= 0.12403
Epoch: 0151 train_loss= 1.44606 accuracy= 0.72572 time= 0.12255
Epoch: 0161 train_loss= 1.43653 accuracy= 0.72541 time= 0.12360
Epoch: 0171 train_loss= 1.42556 accuracy= 0.72572 time= 0.12233
Epoch: 0181 train_loss= 1.41578 accuracy= 0.72602 time= 0.12246
Epoch: 0191 train_loss= 1.40359 accuracy= 0.72612 time= 0.12325
Epoch: 0201 train_loss= 1.38881 accuracy= 0.72623 time= 0.12567
Epoch: 0211 train_loss= 1.36424 accuracy= 0.72572 time= 0.12548
Epoch: 0221 train_loss= 1.32037 accuracy= 0.72704 time= 0.12342
Epoch: 0231 train_loss= 1.35970 accuracy= 0.73191 time= 0.12279
Epoch: 0241 train_loss= 1.35474 accuracy= 0.73099 time= 0.12153
Epoch: 0251 train_loss= 1.32369 accuracy= 0.72754 time= 0.12336
Epoch: 0261 train_loss= 1.28409 accuracy= 0.72531 time= 0.12187
Epoch: 0271 train_loss= 1.25934 accuracy= 0.72633 time= 0.12039
Epoch: 0281 train_loss= 1.23368 accuracy= 0.72633 time= 0.12115
Epoch: 0291 train_loss= 1.21299 accuracy= 0.72836 time= 0.12255

accuracy 0.72582
auc 0.62216
f1_score 0.32425
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=50, cuda=False, dataset='pubmed', decoder=1, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 447
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
using wavelet scattering transform
y_features shape after scatting (19717, 6500) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([19717, 125]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 3906.16846 accuracy= 0.72947 time= 0.27786
Epoch: 0011 train_loss= 1103.91528 accuracy= 0.66750 time= 0.24029
Epoch: 0021 train_loss= 482.92017 accuracy= 0.65918 time= 0.23933
Epoch: 0031 train_loss= 120.23082 accuracy= 0.65725 time= 0.24063
Epoch: 0041 train_loss= 9.76142 accuracy= 0.65066 time= 0.24083
Epoch: 0051 train_loss= 1.63746 accuracy= 0.64487 time= 0.23909
Epoch: 0061 train_loss= 0.67166 accuracy= 0.64802 time= 0.23929
Epoch: 0071 train_loss= 0.53080 accuracy= 0.64660 time= 0.24041
Epoch: 0081 train_loss= 0.50267 accuracy= 0.64487 time= 0.23888
Epoch: 0091 train_loss= 0.48191 accuracy= 0.64406 time= 0.23911
Epoch: 0101 train_loss= 0.47589 accuracy= 0.64498 time= 0.24012
Epoch: 0111 train_loss= 0.46746 accuracy= 0.64366 time= 0.23970
Epoch: 0121 train_loss= 0.46425 accuracy= 0.64467 time= 0.24059
Epoch: 0131 train_loss= 0.45962 accuracy= 0.64305 time= 0.24037
Epoch: 0141 train_loss= 0.45747 accuracy= 0.64467 time= 0.23989
Epoch: 0151 train_loss= 0.45367 accuracy= 0.64416 time= 0.24046
Epoch: 0161 train_loss= 0.44919 accuracy= 0.64427 time= 0.24137
Epoch: 0171 train_loss= 0.44757 accuracy= 0.64416 time= 0.24053
Epoch: 0181 train_loss= 0.44571 accuracy= 0.64335 time= 0.23998
Epoch: 0191 train_loss= 0.44311 accuracy= 0.64376 time= 0.23404
Epoch: 0201 train_loss= 0.44040 accuracy= 0.64396 time= 0.24178
Epoch: 0211 train_loss= 0.44110 accuracy= 0.64376 time= 0.24135
Epoch: 0221 train_loss= 0.43733 accuracy= 0.64396 time= 0.24123
Epoch: 0231 train_loss= 0.43646 accuracy= 0.64396 time= 0.24132
Epoch: 0241 train_loss= 0.43540 accuracy= 0.64376 time= 0.24083
Epoch: 0251 train_loss= 0.43359 accuracy= 0.64386 time= 0.24179
Epoch: 0261 train_loss= 0.43259 accuracy= 0.64406 time= 0.24185
Epoch: 0271 train_loss= 0.43058 accuracy= 0.64356 time= 0.24128
Epoch: 0281 train_loss= 0.43022 accuracy= 0.64396 time= 0.24123
Epoch: 0291 train_loss= 0.42958 accuracy= 0.64295 time= 0.24215

accuracy 0.64315
auc 0.25590
f1_score 0.12050
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=50, cuda=False, dataset='pubmed', decoder=0, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 219
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
using wavelet scattering transform
y_features shape after scatting (19717, 6500) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([19717, 125]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 21.15310 accuracy= 0.75757 time= 0.28951
Epoch: 0011 train_loss= 6.77949 accuracy= 0.68403 time= 0.24899
Epoch: 0021 train_loss= 3.77713 accuracy= 0.68099 time= 0.24741
Epoch: 0031 train_loss= 2.96291 accuracy= 0.68017 time= 0.24718
Epoch: 0041 train_loss= 2.58460 accuracy= 0.70888 time= 0.24420
Epoch: 0051 train_loss= 2.39520 accuracy= 0.70624 time= 0.24789
Epoch: 0061 train_loss= 1.94649 accuracy= 0.70452 time= 0.24762
Epoch: 0071 train_loss= 1.63327 accuracy= 0.71669 time= 0.24518
Epoch: 0081 train_loss= 1.49872 accuracy= 0.72014 time= 0.24497
Epoch: 0091 train_loss= 1.38238 accuracy= 0.72065 time= 0.24786
Epoch: 0101 train_loss= 1.23883 accuracy= 0.72247 time= 0.24790
Epoch: 0111 train_loss= 1.05692 accuracy= 0.72420 time= 0.24904
Epoch: 0121 train_loss= 0.88403 accuracy= 0.72420 time= 0.24737
Epoch: 0131 train_loss= 0.81402 accuracy= 0.72450 time= 0.24999
Epoch: 0141 train_loss= 0.79006 accuracy= 0.72207 time= 0.24666
Epoch: 0151 train_loss= 0.78056 accuracy= 0.72237 time= 0.24818
Epoch: 0161 train_loss= 0.77295 accuracy= 0.72288 time= 0.24935
Epoch: 0171 train_loss= 0.75867 accuracy= 0.72257 time= 0.24503
Epoch: 0181 train_loss= 0.72446 accuracy= 0.72257 time= 0.24862
Epoch: 0191 train_loss= 0.65354 accuracy= 0.72146 time= 0.24415
Epoch: 0201 train_loss= 0.58390 accuracy= 0.72197 time= 0.24936
Epoch: 0211 train_loss= 0.54584 accuracy= 0.72186 time= 0.24424
Epoch: 0221 train_loss= 0.53279 accuracy= 0.72176 time= 0.24644
Epoch: 0231 train_loss= 0.53255 accuracy= 0.72257 time= 0.24680
Epoch: 0241 train_loss= 0.53871 accuracy= 0.72115 time= 0.24832
Epoch: 0251 train_loss= 0.52802 accuracy= 0.72237 time= 0.24841
Epoch: 0261 train_loss= 0.52569 accuracy= 0.72257 time= 0.24804
Epoch: 0271 train_loss= 0.52452 accuracy= 0.72268 time= 0.24754
Epoch: 0281 train_loss= 0.52386 accuracy= 0.72268 time= 0.24842
Epoch: 0291 train_loss= 0.52338 accuracy= 0.72257 time= 0.24768

accuracy 0.72004
auc 0.61256
f1_score 0.31000
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=50, cuda=False, dataset='pubmed', decoder=1, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 879
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
using wavelet scattering transform
y_features shape after scatting (19717, 6500) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 87 steps!

y_features shape after FMS torch.Size([19717, 125]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 76.24522 accuracy= 0.77918 time= 0.27858
Epoch: 0011 train_loss= 13.59238 accuracy= 0.69468 time= 0.24040
Epoch: 0021 train_loss= 6.78097 accuracy= 0.66273 time= 0.24031
Epoch: 0031 train_loss= 3.77870 accuracy= 0.66668 time= 0.24095
Epoch: 0041 train_loss= 2.26577 accuracy= 0.65603 time= 0.23999
Epoch: 0051 train_loss= 1.49083 accuracy= 0.65421 time= 0.24088
Epoch: 0061 train_loss= 1.06571 accuracy= 0.65309 time= 0.24156
Epoch: 0071 train_loss= 0.82269 accuracy= 0.65167 time= 0.24189
Epoch: 0081 train_loss= 0.67789 accuracy= 0.65147 time= 0.24183
Epoch: 0091 train_loss= 0.59066 accuracy= 0.65137 time= 0.24179
Epoch: 0101 train_loss= 0.53593 accuracy= 0.65086 time= 0.24216
Epoch: 0111 train_loss= 0.50127 accuracy= 0.65025 time= 0.24190
Epoch: 0121 train_loss= 0.47806 accuracy= 0.65015 time= 0.24152
Epoch: 0131 train_loss= 0.46159 accuracy= 0.64944 time= 0.24130
Epoch: 0141 train_loss= 0.45132 accuracy= 0.64934 time= 0.24206
Epoch: 0151 train_loss= 0.44325 accuracy= 0.64924 time= 0.24171
Epoch: 0161 train_loss= 0.43754 accuracy= 0.64914 time= 0.24342
Epoch: 0171 train_loss= 0.43305 accuracy= 0.64843 time= 0.24195
Epoch: 0181 train_loss= 0.42972 accuracy= 0.64843 time= 0.24187
Epoch: 0191 train_loss= 0.42720 accuracy= 0.64772 time= 0.24247
Epoch: 0201 train_loss= 0.42511 accuracy= 0.64731 time= 0.24243
Epoch: 0211 train_loss= 0.42355 accuracy= 0.64843 time= 0.24244
Epoch: 0221 train_loss= 0.42210 accuracy= 0.64751 time= 0.24238
Epoch: 0231 train_loss= 0.42119 accuracy= 0.64761 time= 0.24272
Epoch: 0241 train_loss= 0.42000 accuracy= 0.64670 time= 0.24279
Epoch: 0251 train_loss= 0.41943 accuracy= 0.64670 time= 0.24325
Epoch: 0261 train_loss= 0.41854 accuracy= 0.64711 time= 0.24292
Epoch: 0271 train_loss= 0.41791 accuracy= 0.64589 time= 0.24318
Epoch: 0281 train_loss= 0.41754 accuracy= 0.64640 time= 0.24341
Epoch: 0291 train_loss= 0.41711 accuracy= 0.64599 time= 0.24346

accuracy 0.64569
auc 0.25772
f1_score 0.12675
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=50, cuda=False, dataset='pubmed', decoder=0, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 921
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
using wavelet scattering transform
y_features shape after scatting (19717, 6500) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 87 steps!

y_features shape after FMS torch.Size([19717, 125]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 20.96543 accuracy= 0.79784 time= 0.28672
Epoch: 0011 train_loss= 12.93040 accuracy= 0.77116 time= 0.24968
Epoch: 0021 train_loss= 5.74012 accuracy= 0.71852 time= 0.24859
Epoch: 0031 train_loss= 3.12794 accuracy= 0.71203 time= 0.24957
Epoch: 0041 train_loss= 2.75461 accuracy= 0.70421 time= 0.24741
Epoch: 0051 train_loss= 2.57548 accuracy= 0.69164 time= 0.24654
Epoch: 0061 train_loss= 2.25008 accuracy= 0.69072 time= 0.24866
Epoch: 0071 train_loss= 1.75040 accuracy= 0.71507 time= 0.24761
Epoch: 0081 train_loss= 1.63044 accuracy= 0.71923 time= 0.24171
Epoch: 0091 train_loss= 1.55313 accuracy= 0.71984 time= 0.24888
Epoch: 0101 train_loss= 1.53875 accuracy= 0.71984 time= 0.24786
Epoch: 0111 train_loss= 1.50913 accuracy= 0.71984 time= 0.24890
Epoch: 0121 train_loss= 1.48451 accuracy= 0.72115 time= 0.24974
Epoch: 0131 train_loss= 1.47262 accuracy= 0.72105 time= 0.24746
Epoch: 0141 train_loss= 1.45274 accuracy= 0.72186 time= 0.24996
Epoch: 0151 train_loss= 1.41550 accuracy= 0.72237 time= 0.24952
Epoch: 0161 train_loss= 1.35974 accuracy= 0.72146 time= 0.25058
Epoch: 0171 train_loss= 1.30660 accuracy= 0.72308 time= 0.24996
Epoch: 0181 train_loss= 1.27110 accuracy= 0.72339 time= 0.25051
Epoch: 0191 train_loss= 1.20579 accuracy= 0.72369 time= 0.24887
Epoch: 0201 train_loss= 1.17187 accuracy= 0.72389 time= 0.24957
Epoch: 0211 train_loss= 1.10882 accuracy= 0.72562 time= 0.24974
Epoch: 0221 train_loss= 1.00551 accuracy= 0.72572 time= 0.24753
Epoch: 0231 train_loss= 0.58069 accuracy= 0.72623 time= 0.24704
Epoch: 0241 train_loss= 0.42461 accuracy= 0.72166 time= 0.24739
Epoch: 0251 train_loss= 0.39460 accuracy= 0.72268 time= 0.24849
Epoch: 0261 train_loss= 0.39062 accuracy= 0.72207 time= 0.24799
Epoch: 0271 train_loss= 0.38676 accuracy= 0.72237 time= 0.24810
Epoch: 0281 train_loss= 0.38591 accuracy= 0.72227 time= 0.24751
Epoch: 0291 train_loss= 0.38559 accuracy= 0.72247 time= 0.24821

accuracy 0.72247
auc 0.61613
f1_score 0.31600
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=50, cuda=False, dataset='pubmed', decoder=1, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 901
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
using wavelet scattering transform
y_features shape after scatting (19717, 6500) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([19717, 125]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 2646.37939 accuracy= 0.76599 time= 0.15056
Epoch: 0011 train_loss= 829.51605 accuracy= 0.65613 time= 0.12055
Epoch: 0021 train_loss= 496.84979 accuracy= 0.65755 time= 0.11964
Epoch: 0031 train_loss= 245.97609 accuracy= 0.65492 time= 0.12014
Epoch: 0041 train_loss= 96.87709 accuracy= 0.65360 time= 0.12008
Epoch: 0051 train_loss= 28.83481 accuracy= 0.65269 time= 0.11984
Epoch: 0061 train_loss= 7.45523 accuracy= 0.64914 time= 0.11977
Epoch: 0071 train_loss= 2.36367 accuracy= 0.64721 time= 0.11954
Epoch: 0081 train_loss= 1.22504 accuracy= 0.64609 time= 0.11976
Epoch: 0091 train_loss= 0.87932 accuracy= 0.64630 time= 0.12000
Epoch: 0101 train_loss= 0.74067 accuracy= 0.64498 time= 0.11962
Epoch: 0111 train_loss= 0.67346 accuracy= 0.64538 time= 0.11949
Epoch: 0121 train_loss= 0.63611 accuracy= 0.64467 time= 0.11922
Epoch: 0131 train_loss= 0.59378 accuracy= 0.64569 time= 0.11925
Epoch: 0141 train_loss= 0.56878 accuracy= 0.64559 time= 0.11929
Epoch: 0151 train_loss= 0.54864 accuracy= 0.64487 time= 0.11923
Epoch: 0161 train_loss= 0.53132 accuracy= 0.64467 time= 0.11940
Epoch: 0171 train_loss= 0.51864 accuracy= 0.64396 time= 0.11986
Epoch: 0181 train_loss= 0.50310 accuracy= 0.64477 time= 0.12392
Epoch: 0191 train_loss= 0.49542 accuracy= 0.64508 time= 0.11988
Epoch: 0201 train_loss= 0.48980 accuracy= 0.64467 time= 0.12008
Epoch: 0211 train_loss= 0.47885 accuracy= 0.64487 time= 0.12008
Epoch: 0221 train_loss= 0.47348 accuracy= 0.64396 time= 0.12027
Epoch: 0231 train_loss= 0.46877 accuracy= 0.64386 time= 0.12023
Epoch: 0241 train_loss= 0.46336 accuracy= 0.64376 time= 0.11964
Epoch: 0251 train_loss= 0.46182 accuracy= 0.64447 time= 0.12012
Epoch: 0261 train_loss= 0.45641 accuracy= 0.64487 time= 0.12032
Epoch: 0271 train_loss= 0.45313 accuracy= 0.64447 time= 0.11994
Epoch: 0281 train_loss= 0.45086 accuracy= 0.64447 time= 0.12026
Epoch: 0291 train_loss= 0.44655 accuracy= 0.64427 time= 0.11948

accuracy 0.64467
auc 0.25816
f1_score 0.12425
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=50, cuda=False, dataset='pubmed', decoder=0, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 742
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
using wavelet scattering transform
y_features shape after scatting (19717, 6500) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([19717, 125]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 21.28270 accuracy= 0.78039 time= 0.16666
Epoch: 0011 train_loss= 12.81666 accuracy= 0.74895 time= 0.12802
Epoch: 0021 train_loss= 7.93203 accuracy= 0.70918 time= 0.12621
Epoch: 0031 train_loss= 5.75299 accuracy= 0.69438 time= 0.12626
Epoch: 0041 train_loss= 4.29497 accuracy= 0.68667 time= 0.12343
Epoch: 0051 train_loss= 3.03314 accuracy= 0.68048 time= 0.12371
Epoch: 0061 train_loss= 2.52126 accuracy= 0.67551 time= 0.12730
Epoch: 0071 train_loss= 2.36756 accuracy= 0.67652 time= 0.12677
Epoch: 0081 train_loss= 2.27185 accuracy= 0.68068 time= 0.12663
Epoch: 0091 train_loss= 2.10624 accuracy= 0.68078 time= 0.12589
Epoch: 0101 train_loss= 1.87710 accuracy= 0.68413 time= 0.12879
Epoch: 0111 train_loss= 1.80578 accuracy= 0.68717 time= 0.12722
Epoch: 0121 train_loss= 1.74256 accuracy= 0.68920 time= 0.12793
Epoch: 0131 train_loss= 1.63319 accuracy= 0.69093 time= 0.12764
Epoch: 0141 train_loss= 1.58951 accuracy= 0.69235 time= 0.12588
Epoch: 0151 train_loss= 1.54654 accuracy= 0.69123 time= 0.12779
Epoch: 0161 train_loss= 1.50905 accuracy= 0.68809 time= 0.12695
Epoch: 0171 train_loss= 1.54442 accuracy= 0.69316 time= 0.12704
Epoch: 0181 train_loss= 1.49572 accuracy= 0.69529 time= 0.12673
Epoch: 0191 train_loss= 1.45544 accuracy= 0.69732 time= 0.12726
Epoch: 0201 train_loss= 1.44980 accuracy= 0.69590 time= 0.12829
Epoch: 0211 train_loss= 1.40848 accuracy= 0.69803 time= 0.12690
Epoch: 0221 train_loss= 1.36978 accuracy= 0.69833 time= 0.12752
Epoch: 0231 train_loss= 1.33586 accuracy= 0.69894 time= 0.12677
Epoch: 0241 train_loss= 1.30122 accuracy= 0.70006 time= 0.12786
Epoch: 0251 train_loss= 1.26444 accuracy= 0.70097 time= 0.12758
Epoch: 0261 train_loss= 1.22327 accuracy= 0.70269 time= 0.12802
Epoch: 0271 train_loss= 1.18270 accuracy= 0.70198 time= 0.12723
Epoch: 0281 train_loss= 1.11270 accuracy= 0.70300 time= 0.12867
Epoch: 0291 train_loss= 0.91308 accuracy= 0.70776 time= 0.12425

accuracy 0.70847
auc 0.60904
f1_score 0.28150
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=50, cuda=False, dataset='pubmed', decoder=1, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 88
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
using wavelet scattering transform
y_features shape after scatting (19717, 6500) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 86 steps!

y_features shape after FMS torch.Size([19717, 125]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 36.24615 accuracy= 0.77451 time= 0.16127
Epoch: 0011 train_loss= 12.42566 accuracy= 0.81356 time= 0.11939
Epoch: 0021 train_loss= 5.96258 accuracy= 0.77725 time= 0.11997
Epoch: 0031 train_loss= 3.33941 accuracy= 0.77126 time= 0.12034
Epoch: 0041 train_loss= 1.98602 accuracy= 0.76213 time= 0.12123
Epoch: 0051 train_loss= 1.27058 accuracy= 0.75686 time= 0.11979
Epoch: 0061 train_loss= 0.89900 accuracy= 0.74509 time= 0.12125
Epoch: 0071 train_loss= 0.70143 accuracy= 0.72237 time= 0.12026
Epoch: 0081 train_loss= 0.59387 accuracy= 0.69377 time= 0.12048
Epoch: 0091 train_loss= 0.53319 accuracy= 0.67307 time= 0.11978
Epoch: 0101 train_loss= 0.49658 accuracy= 0.66263 time= 0.11963
Epoch: 0111 train_loss= 0.47389 accuracy= 0.65471 time= 0.12005
Epoch: 0121 train_loss= 0.45886 accuracy= 0.65076 time= 0.12042
Epoch: 0131 train_loss= 0.44878 accuracy= 0.64964 time= 0.12076
Epoch: 0141 train_loss= 0.44194 accuracy= 0.64853 time= 0.12007
Epoch: 0151 train_loss= 0.43647 accuracy= 0.64822 time= 0.12077
Epoch: 0161 train_loss= 0.43271 accuracy= 0.64731 time= 0.11995
Epoch: 0171 train_loss= 0.42959 accuracy= 0.64731 time= 0.12038
Epoch: 0181 train_loss= 0.42726 accuracy= 0.64741 time= 0.12064
Epoch: 0191 train_loss= 0.42536 accuracy= 0.64660 time= 0.12034
Epoch: 0201 train_loss= 0.42386 accuracy= 0.64650 time= 0.12074
Epoch: 0211 train_loss= 0.42257 accuracy= 0.64711 time= 0.12040
Epoch: 0221 train_loss= 0.42142 accuracy= 0.64680 time= 0.12023
Epoch: 0231 train_loss= 0.42041 accuracy= 0.64640 time= 0.12071
Epoch: 0241 train_loss= 0.41961 accuracy= 0.64690 time= 0.12100
Epoch: 0251 train_loss= 0.41893 accuracy= 0.64660 time= 0.12095
Epoch: 0261 train_loss= 0.41832 accuracy= 0.64630 time= 0.12047
Epoch: 0271 train_loss= 0.41774 accuracy= 0.64650 time= 0.12061
Epoch: 0281 train_loss= 0.41724 accuracy= 0.64660 time= 0.12033
Epoch: 0291 train_loss= 0.41683 accuracy= 0.64660 time= 0.12106

accuracy 0.64640
auc 0.25923
f1_score 0.12850
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=50, cuda=False, dataset='pubmed', decoder=0, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 311
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
using wavelet scattering transform
y_features shape after scatting (19717, 6500) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 87 steps!

y_features shape after FMS torch.Size([19717, 125]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 20.93160 accuracy= 0.80940 time= 0.17132
Epoch: 0011 train_loss= 15.33385 accuracy= 0.81082 time= 0.12782
Epoch: 0021 train_loss= 7.17112 accuracy= 0.78932 time= 0.12682
Epoch: 0031 train_loss= 2.88182 accuracy= 0.74530 time= 0.12801
Epoch: 0041 train_loss= 1.76621 accuracy= 0.72136 time= 0.12347
Epoch: 0051 train_loss= 1.52424 accuracy= 0.72328 time= 0.12455
Epoch: 0061 train_loss= 1.50535 accuracy= 0.72369 time= 0.12456
Epoch: 0071 train_loss= 1.48659 accuracy= 0.72481 time= 0.12498
Epoch: 0081 train_loss= 1.44951 accuracy= 0.72541 time= 0.12553
Epoch: 0091 train_loss= 1.41971 accuracy= 0.72531 time= 0.12407
Epoch: 0101 train_loss= 1.36764 accuracy= 0.72562 time= 0.12652
Epoch: 0111 train_loss= 1.27327 accuracy= 0.72572 time= 0.12592
Epoch: 0121 train_loss= 0.87366 accuracy= 0.72440 time= 0.12634
Epoch: 0131 train_loss= 0.44420 accuracy= 0.72278 time= 0.12604
Epoch: 0141 train_loss= 0.42353 accuracy= 0.72247 time= 0.12123
Epoch: 0151 train_loss= 0.39388 accuracy= 0.72257 time= 0.12705
Epoch: 0161 train_loss= 0.38992 accuracy= 0.72217 time= 0.12563
Epoch: 0171 train_loss= 0.38654 accuracy= 0.72257 time= 0.12684
Epoch: 0181 train_loss= 0.38594 accuracy= 0.72237 time= 0.12553
Epoch: 0191 train_loss= 0.38554 accuracy= 0.72268 time= 0.12622
Epoch: 0201 train_loss= 0.38546 accuracy= 0.72268 time= 0.12612
Epoch: 0211 train_loss= 0.38541 accuracy= 0.72237 time= 0.12348
Epoch: 0221 train_loss= 0.38540 accuracy= 0.72257 time= 0.12628
Epoch: 0231 train_loss= 0.38539 accuracy= 0.72257 time= 0.12771
Epoch: 0241 train_loss= 0.38539 accuracy= 0.72257 time= 0.12654
Epoch: 0251 train_loss= 0.38539 accuracy= 0.72257 time= 0.12640
Epoch: 0261 train_loss= 0.38539 accuracy= 0.72257 time= 0.12286
Epoch: 0271 train_loss= 0.38539 accuracy= 0.72257 time= 0.12357
Epoch: 0281 train_loss= 0.38539 accuracy= 0.72257 time= 0.12708
Epoch: 0291 train_loss= 0.38539 accuracy= 0.72257 time= 0.12383

accuracy 0.72257
auc 0.61612
f1_score 0.31625
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=3, beta=1, clique_size=50, cuda=False, dataset='pubmed', decoder=1, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 944
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
using wavelet scattering transform
y_features shape after scatting (19717, 6500) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([19717, 125]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 6119.99072 accuracy= 0.70807 time= 0.27482
Epoch: 0011 train_loss= 3903.18726 accuracy= 0.66090 time= 0.23995
Epoch: 0021 train_loss= 3435.13843 accuracy= 0.66050 time= 0.23905
Epoch: 0031 train_loss= 3129.48389 accuracy= 0.65461 time= 0.23849
Epoch: 0041 train_loss= 2895.96509 accuracy= 0.65390 time= 0.23962
Epoch: 0051 train_loss= 2709.37256 accuracy= 0.65390 time= 0.23994
Epoch: 0061 train_loss= 2547.61646 accuracy= 0.65421 time= 0.23875
Epoch: 0071 train_loss= 2408.04175 accuracy= 0.65400 time= 0.24051
Epoch: 0081 train_loss= 2284.34692 accuracy= 0.65400 time= 0.23854
Epoch: 0091 train_loss= 2171.99731 accuracy= 0.65451 time= 0.24053
Epoch: 0101 train_loss= 2064.36304 accuracy= 0.65390 time= 0.24019
Epoch: 0111 train_loss= 1967.12976 accuracy= 0.65431 time= 0.24013
Epoch: 0121 train_loss= 1876.91614 accuracy= 0.65360 time= 0.24079
Epoch: 0131 train_loss= 1788.83911 accuracy= 0.65390 time= 0.23978
Epoch: 0141 train_loss= 1708.40479 accuracy= 0.65258 time= 0.24084
Epoch: 0151 train_loss= 1629.90759 accuracy= 0.65208 time= 0.24043
Epoch: 0161 train_loss= 1556.45789 accuracy= 0.65350 time= 0.24088
Epoch: 0171 train_loss= 1487.77600 accuracy= 0.65269 time= 0.24112
Epoch: 0181 train_loss= 1420.43860 accuracy= 0.65248 time= 0.24176
Epoch: 0191 train_loss= 1356.13550 accuracy= 0.65289 time= 0.24143
Epoch: 0201 train_loss= 1294.74890 accuracy= 0.65258 time= 0.24056
Epoch: 0211 train_loss= 1237.35071 accuracy= 0.65258 time= 0.23997
Epoch: 0221 train_loss= 1179.96289 accuracy= 0.65177 time= 0.24031
Epoch: 0231 train_loss= 1125.58118 accuracy= 0.65218 time= 0.24057
Epoch: 0241 train_loss= 1074.94543 accuracy= 0.65269 time= 0.24082
Epoch: 0251 train_loss= 1025.85950 accuracy= 0.65258 time= 0.24162
Epoch: 0261 train_loss= 978.52606 accuracy= 0.65309 time= 0.24185
Epoch: 0271 train_loss= 932.77783 accuracy= 0.65198 time= 0.24211
Epoch: 0281 train_loss= 888.78949 accuracy= 0.65309 time= 0.24202
Epoch: 0291 train_loss= 847.23334 accuracy= 0.65258 time= 0.24183

accuracy 0.65289
auc 0.31767
f1_score 0.14450
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=3, beta=1, clique_size=50, cuda=False, dataset='pubmed', decoder=0, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 138
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
using wavelet scattering transform
y_features shape after scatting (19717, 6500) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([19717, 125]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 21.21396 accuracy= 0.74073 time= 0.28681
Epoch: 0011 train_loss= 14.23991 accuracy= 0.70381 time= 0.24636
Epoch: 0021 train_loss= 9.63758 accuracy= 0.69225 time= 0.24771
Epoch: 0031 train_loss= 6.69235 accuracy= 0.68910 time= 0.24250
Epoch: 0041 train_loss= 4.76420 accuracy= 0.66902 time= 0.24668
Epoch: 0051 train_loss= 2.71756 accuracy= 0.70675 time= 0.24660
Epoch: 0061 train_loss= 2.32027 accuracy= 0.71263 time= 0.24484
Epoch: 0071 train_loss= 2.18326 accuracy= 0.71760 time= 0.24600
Epoch: 0081 train_loss= 2.11511 accuracy= 0.71973 time= 0.24532
Epoch: 0091 train_loss= 2.04434 accuracy= 0.72065 time= 0.24689
Epoch: 0101 train_loss= 1.98651 accuracy= 0.72085 time= 0.24598
Epoch: 0111 train_loss= 1.84530 accuracy= 0.71781 time= 0.24634
Epoch: 0121 train_loss= 1.31965 accuracy= 0.71821 time= 0.24700
Epoch: 0131 train_loss= 0.78592 accuracy= 0.72308 time= 0.24693
Epoch: 0141 train_loss= 0.40263 accuracy= 0.72247 time= 0.24686
Epoch: 0151 train_loss= 0.40591 accuracy= 0.72156 time= 0.24586
Epoch: 0161 train_loss= 0.39049 accuracy= 0.72197 time= 0.24664
Epoch: 0171 train_loss= 0.38754 accuracy= 0.72227 time= 0.24399
Epoch: 0181 train_loss= 0.38600 accuracy= 0.72278 time= 0.24469
Epoch: 0191 train_loss= 0.38568 accuracy= 0.72207 time= 0.24471
Epoch: 0201 train_loss= 0.38545 accuracy= 0.72227 time= 0.24284
Epoch: 0211 train_loss= 0.38543 accuracy= 0.72268 time= 0.24288
Epoch: 0221 train_loss= 0.38542 accuracy= 0.72268 time= 0.24106
Epoch: 0231 train_loss= 0.38539 accuracy= 0.72247 time= 0.24221
Epoch: 0241 train_loss= 0.38539 accuracy= 0.72247 time= 0.24636
Epoch: 0251 train_loss= 0.38539 accuracy= 0.72268 time= 0.24456
Epoch: 0261 train_loss= 0.38558 accuracy= 0.72247 time= 0.24153
Epoch: 0271 train_loss= 0.38539 accuracy= 0.72257 time= 0.24673
Epoch: 0281 train_loss= 0.38539 accuracy= 0.72257 time= 0.24417
Epoch: 0291 train_loss= 0.38539 accuracy= 0.72257 time= 0.24701

accuracy 0.72257
auc 0.61612
f1_score 0.31625
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=3, beta=1, clique_size=50, cuda=False, dataset='pubmed', decoder=1, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 768
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
using wavelet scattering transform
y_features shape after scatting (19717, 6500) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 86 steps!

y_features shape after FMS torch.Size([19717, 125]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 6745.75098 accuracy= 0.79825 time= 0.28137
Epoch: 0011 train_loss= 4028.89258 accuracy= 0.73546 time= 0.23853
Epoch: 0021 train_loss= 3484.58447 accuracy= 0.75432 time= 0.23805
Epoch: 0031 train_loss= 3156.99634 accuracy= 0.75716 time= 0.23801
Epoch: 0041 train_loss= 2912.65088 accuracy= 0.75483 time= 0.23727
Epoch: 0051 train_loss= 2720.42212 accuracy= 0.75554 time= 0.23766
Epoch: 0061 train_loss= 2557.44751 accuracy= 0.75848 time= 0.23189
Epoch: 0071 train_loss= 2414.15112 accuracy= 0.76355 time= 0.23981
Epoch: 0081 train_loss= 2287.38184 accuracy= 0.76639 time= 0.23905
Epoch: 0091 train_loss= 2171.99902 accuracy= 0.76751 time= 0.23920
Epoch: 0101 train_loss= 2066.20410 accuracy= 0.76771 time= 0.23872
Epoch: 0111 train_loss= 1969.38342 accuracy= 0.76751 time= 0.23866
Epoch: 0121 train_loss= 1876.69324 accuracy= 0.76792 time= 0.24038
Epoch: 0131 train_loss= 1789.88574 accuracy= 0.76792 time= 0.24008
Epoch: 0141 train_loss= 1708.99475 accuracy= 0.76944 time= 0.23908
Epoch: 0151 train_loss= 1631.53809 accuracy= 0.76974 time= 0.24122
Epoch: 0161 train_loss= 1557.61646 accuracy= 0.76994 time= 0.23904
Epoch: 0171 train_loss= 1488.23828 accuracy= 0.77025 time= 0.24056
Epoch: 0181 train_loss= 1422.10071 accuracy= 0.77035 time= 0.24085
Epoch: 0191 train_loss= 1357.49243 accuracy= 0.76984 time= 0.23983
Epoch: 0201 train_loss= 1297.82788 accuracy= 0.77086 time= 0.24037
Epoch: 0211 train_loss= 1239.67334 accuracy= 0.76994 time= 0.24041
Epoch: 0221 train_loss= 1184.39539 accuracy= 0.76984 time= 0.24049
Epoch: 0231 train_loss= 1131.14124 accuracy= 0.76964 time= 0.23908
Epoch: 0241 train_loss= 1080.02075 accuracy= 0.77005 time= 0.23992
Epoch: 0251 train_loss= 1030.43079 accuracy= 0.76934 time= 0.23976
Epoch: 0261 train_loss= 983.41949 accuracy= 0.76984 time= 0.24004
Epoch: 0271 train_loss= 938.42358 accuracy= 0.76994 time= 0.24008
Epoch: 0281 train_loss= 893.97900 accuracy= 0.76994 time= 0.23983
Epoch: 0291 train_loss= 853.67340 accuracy= 0.76923 time= 0.23980

accuracy 0.76954
auc 0.65038
f1_score 0.43200
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=3, beta=1, clique_size=50, cuda=False, dataset='pubmed', decoder=0, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 709
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
using wavelet scattering transform
y_features shape after scatting (19717, 6500) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 87 steps!

y_features shape after FMS torch.Size([19717, 125]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 21.06370 accuracy= 0.79317 time= 0.27835
Epoch: 0011 train_loss= 13.57712 accuracy= 0.77421 time= 0.24617
Epoch: 0021 train_loss= 8.61565 accuracy= 0.72481 time= 0.24649
Epoch: 0031 train_loss= 6.52915 accuracy= 0.72694 time= 0.24750
Epoch: 0041 train_loss= 5.15488 accuracy= 0.70756 time= 0.24534
Epoch: 0051 train_loss= 2.99445 accuracy= 0.69083 time= 0.24648
Epoch: 0061 train_loss= 2.50150 accuracy= 0.71487 time= 0.24393
Epoch: 0071 train_loss= 2.29504 accuracy= 0.71527 time= 0.24515
Epoch: 0081 train_loss= 2.19946 accuracy= 0.71679 time= 0.24539
Epoch: 0091 train_loss= 2.12062 accuracy= 0.71913 time= 0.24173
Epoch: 0101 train_loss= 2.04328 accuracy= 0.72136 time= 0.24571
Epoch: 0111 train_loss= 1.96833 accuracy= 0.72176 time= 0.24650
Epoch: 0121 train_loss= 1.84562 accuracy= 0.72075 time= 0.24763
Epoch: 0131 train_loss= 1.59674 accuracy= 0.72308 time= 0.24575
Epoch: 0141 train_loss= 0.91575 accuracy= 0.72237 time= 0.24564
Epoch: 0151 train_loss= 0.45907 accuracy= 0.72328 time= 0.24729
Epoch: 0161 train_loss= 0.42528 accuracy= 0.72308 time= 0.24216
Epoch: 0171 train_loss= 0.40648 accuracy= 0.72217 time= 0.24151
Epoch: 0181 train_loss= 0.38757 accuracy= 0.72217 time= 0.24644
Epoch: 0191 train_loss= 0.38815 accuracy= 0.72308 time= 0.24710
Epoch: 0201 train_loss= 0.38561 accuracy= 0.72237 time= 0.24625
Epoch: 0211 train_loss= 0.38572 accuracy= 0.72217 time= 0.24710
Epoch: 0221 train_loss= 0.38542 accuracy= 0.72257 time= 0.24759
Epoch: 0231 train_loss= 0.38543 accuracy= 0.72247 time= 0.24776
Epoch: 0241 train_loss= 0.38539 accuracy= 0.72257 time= 0.24595
Epoch: 0251 train_loss= 0.38539 accuracy= 0.72268 time= 0.24734
Epoch: 0261 train_loss= 0.38539 accuracy= 0.72257 time= 0.24346
Epoch: 0271 train_loss= 0.38550 accuracy= 0.72257 time= 0.24172
Epoch: 0281 train_loss= 0.38541 accuracy= 0.72268 time= 0.24619
Epoch: 0291 train_loss= 0.38540 accuracy= 0.72247 time= 0.24587

accuracy 0.72247
auc 0.61612
f1_score 0.31600
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=0, beta=1, clique_size=50, cuda=False, dataset='pubmed', device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 190
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
using wavelet scattering transform
y_features shape after scatting (19717, 6500) <class 'numpy.ndarray'>
using PCA to reduce dim
y_features shape after PCA torch.Size([19717, 125]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 5848.48682 feature_loss= 21.29923 structure_loss= 5827.18750 accuracy= 0.74347 accuracy_s= 0.83770 accuracy_f= 0.81681 time= 0.28450
Epoch: 0011 train_loss= 3949.30371 feature_loss= 16.62259 structure_loss= 3932.68115 accuracy= 0.71852 accuracy_s= 0.80869 accuracy_f= 0.81143 time= 0.23932
Epoch: 0021 train_loss= 3322.41113 feature_loss= 12.00284 structure_loss= 3310.40820 accuracy= 0.70584 accuracy_s= 0.81488 accuracy_f= 0.81417 time= 0.23992
Epoch: 0031 train_loss= 2980.56006 feature_loss= 8.41152 structure_loss= 2972.14844 accuracy= 0.71091 accuracy_s= 0.81640 accuracy_f= 0.81752 time= 0.23772
Epoch: 0041 train_loss= 2745.40894 feature_loss= 5.43833 structure_loss= 2739.97070 accuracy= 0.69722 accuracy_s= 0.81498 accuracy_f= 0.82076 time= 0.24087
Epoch: 0051 train_loss= 2569.45630 feature_loss= 3.88063 structure_loss= 2565.57568 accuracy= 0.68859 accuracy_s= 0.80768 accuracy_f= 0.82198 time= 0.23974
Epoch: 0061 train_loss= 2423.94727 feature_loss= 2.76652 structure_loss= 2421.18066 accuracy= 0.68342 accuracy_s= 0.80656 accuracy_f= 0.82979 time= 0.23496
Epoch: 0071 train_loss= 2301.91992 feature_loss= 2.28008 structure_loss= 2299.63989 accuracy= 0.67906 accuracy_s= 0.80575 accuracy_f= 0.83801 time= 0.23843
Epoch: 0081 train_loss= 2190.12427 feature_loss= 2.02234 structure_loss= 2188.10181 accuracy= 0.67662 accuracy_s= 0.80504 accuracy_f= 0.84541 time= 0.23503
Epoch: 0091 train_loss= 2089.83569 feature_loss= 1.85271 structure_loss= 2087.98291 accuracy= 0.67622 accuracy_s= 0.80474 accuracy_f= 0.84764 time= 0.24115
Epoch: 0101 train_loss= 1998.11572 feature_loss= 1.51970 structure_loss= 1996.59607 accuracy= 0.67480 accuracy_s= 0.80433 accuracy_f= 0.84562 time= 0.23602
Epoch: 0111 train_loss= 1910.61438 feature_loss= 1.40867 structure_loss= 1909.20569 accuracy= 0.67460 accuracy_s= 0.80382 accuracy_f= 0.84967 time= 0.24079
Epoch: 0121 train_loss= 1829.55591 feature_loss= 1.37758 structure_loss= 1828.17834 accuracy= 0.67419 accuracy_s= 0.80413 accuracy_f= 0.85038 time= 0.24101
Epoch: 0131 train_loss= 1753.03345 feature_loss= 1.36292 structure_loss= 1751.67053 accuracy= 0.67297 accuracy_s= 0.80382 accuracy_f= 0.85008 time= 0.23735
Epoch: 0141 train_loss= 1678.73914 feature_loss= 1.37057 structure_loss= 1677.36853 accuracy= 0.67236 accuracy_s= 0.80382 accuracy_f= 0.85038 time= 0.24174
Epoch: 0151 train_loss= 1607.75623 feature_loss= 1.37777 structure_loss= 1606.37842 accuracy= 0.67176 accuracy_s= 0.80352 accuracy_f= 0.84977 time= 0.24219
Epoch: 0161 train_loss= 1539.48389 feature_loss= 1.36732 structure_loss= 1538.11658 accuracy= 0.67307 accuracy_s= 0.80362 accuracy_f= 0.84988 time= 0.24253
Epoch: 0171 train_loss= 1473.94116 feature_loss= 1.34979 structure_loss= 1472.59143 accuracy= 0.67094 accuracy_s= 0.80362 accuracy_f= 0.85038 time= 0.24217
Epoch: 0181 train_loss= 1411.31165 feature_loss= 1.34947 structure_loss= 1409.96216 accuracy= 0.66963 accuracy_s= 0.80352 accuracy_f= 0.85008 time= 0.24237
Epoch: 0191 train_loss= 1349.91418 feature_loss= 1.33826 structure_loss= 1348.57593 accuracy= 0.67003 accuracy_s= 0.80332 accuracy_f= 0.85048 time= 0.24167
Epoch: 0201 train_loss= 1291.23120 feature_loss= 1.31122 structure_loss= 1289.92004 accuracy= 0.66952 accuracy_s= 0.80322 accuracy_f= 0.85059 time= 0.24196
Epoch: 0211 train_loss= 1234.23926 feature_loss= 1.29593 structure_loss= 1232.94336 accuracy= 0.66963 accuracy_s= 0.80311 accuracy_f= 0.85059 time= 0.24150
Epoch: 0221 train_loss= 1179.96277 feature_loss= 1.27884 structure_loss= 1178.68396 accuracy= 0.66963 accuracy_s= 0.80332 accuracy_f= 0.85089 time= 0.24226
Epoch: 0231 train_loss= 1126.99780 feature_loss= 1.25000 structure_loss= 1125.74780 accuracy= 0.66912 accuracy_s= 0.80301 accuracy_f= 0.85069 time= 0.24288
Epoch: 0241 train_loss= 1077.00366 feature_loss= 1.23196 structure_loss= 1075.77173 accuracy= 0.66871 accuracy_s= 0.80322 accuracy_f= 0.85069 time= 0.24197
Epoch: 0251 train_loss= 1028.06018 feature_loss= 1.16609 structure_loss= 1026.89404 accuracy= 0.66851 accuracy_s= 0.80291 accuracy_f= 0.85059 time= 0.24218
Epoch: 0261 train_loss= 980.09143 feature_loss= 0.72033 structure_loss= 979.37109 accuracy= 0.66476 accuracy_s= 0.80281 accuracy_f= 0.85140 time= 0.24335
Epoch: 0271 train_loss= 934.96259 feature_loss= 0.42122 structure_loss= 934.54138 accuracy= 0.66516 accuracy_s= 0.80301 accuracy_f= 0.85150 time= 0.24177
Epoch: 0281 train_loss= 891.61377 feature_loss= 0.42321 structure_loss= 891.19055 accuracy= 0.66486 accuracy_s= 0.80301 accuracy_f= 0.85241 time= 0.24004
Epoch: 0291 train_loss= 849.61444 feature_loss= 0.39164 structure_loss= 849.22278 accuracy= 0.66536 accuracy_s= 0.80291 accuracy_f= 0.85211 time= 0.24102

accuracy 0.66516
accuracy_s 0.80301
accuracy_f 0.85180
auc 0.43830
f1_score 0.17475
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=0, beta=1, clique_size=50, cuda=False, dataset='pubmed', device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 217
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
using wavelet scattering transform
y_features shape after scatting (19717, 6500) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 87 steps!
y_features shape after FMS torch.Size([19717, 125]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 6380.61377 feature_loss= 21.07491 structure_loss= 6359.53906 accuracy= 0.77481 accuracy_s= 0.86134 accuracy_f= 0.80971 time= 0.29713
Epoch: 0011 train_loss= 4212.17090 feature_loss= 15.75114 structure_loss= 4196.41992 accuracy= 0.80687 accuracy_s= 0.85738 accuracy_f= 0.80453 time= 0.24049
Epoch: 0021 train_loss= 3372.39453 feature_loss= 11.06394 structure_loss= 3361.33057 accuracy= 0.78546 accuracy_s= 0.85505 accuracy_f= 0.80869 time= 0.23839
Epoch: 0031 train_loss= 2975.48413 feature_loss= 7.89757 structure_loss= 2967.58667 accuracy= 0.77278 accuracy_s= 0.85363 accuracy_f= 0.81133 time= 0.23784
Epoch: 0041 train_loss= 2722.37622 feature_loss= 5.35533 structure_loss= 2717.02100 accuracy= 0.76426 accuracy_s= 0.85353 accuracy_f= 0.81681 time= 0.23729
Epoch: 0051 train_loss= 2534.47314 feature_loss= 3.24626 structure_loss= 2531.22681 accuracy= 0.76437 accuracy_s= 0.85566 accuracy_f= 0.81681 time= 0.23925
Epoch: 0061 train_loss= 2381.96729 feature_loss= 2.86874 structure_loss= 2379.09863 accuracy= 0.76802 accuracy_s= 0.85850 accuracy_f= 0.82939 time= 0.23798
Epoch: 0071 train_loss= 2256.35303 feature_loss= 2.52547 structure_loss= 2253.82764 accuracy= 0.76873 accuracy_s= 0.86306 accuracy_f= 0.83709 time= 0.24053
Epoch: 0081 train_loss= 2146.13794 feature_loss= 2.31533 structure_loss= 2143.82251 accuracy= 0.77045 accuracy_s= 0.86631 accuracy_f= 0.84470 time= 0.23689
Epoch: 0091 train_loss= 2048.71655 feature_loss= 2.17616 structure_loss= 2046.54041 accuracy= 0.77238 accuracy_s= 0.86955 accuracy_f= 0.84551 time= 0.23834
Epoch: 0101 train_loss= 1957.49890 feature_loss= 1.95924 structure_loss= 1955.53967 accuracy= 0.77299 accuracy_s= 0.87260 accuracy_f= 0.84551 time= 0.24175
Epoch: 0111 train_loss= 1872.81445 feature_loss= 1.87835 structure_loss= 1870.93616 accuracy= 0.77319 accuracy_s= 0.87757 accuracy_f= 0.84896 time= 0.24109
Epoch: 0121 train_loss= 1794.63684 feature_loss= 1.59529 structure_loss= 1793.04150 accuracy= 0.77268 accuracy_s= 0.88061 accuracy_f= 0.84420 time= 0.23654
Epoch: 0131 train_loss= 1719.94849 feature_loss= 1.44336 structure_loss= 1718.50513 accuracy= 0.77431 accuracy_s= 0.88284 accuracy_f= 0.84856 time= 0.23835
Epoch: 0141 train_loss= 1649.07214 feature_loss= 1.42375 structure_loss= 1647.64844 accuracy= 0.77390 accuracy_s= 0.88609 accuracy_f= 0.85028 time= 0.24255
Epoch: 0151 train_loss= 1580.75647 feature_loss= 1.38762 structure_loss= 1579.36890 accuracy= 0.77451 accuracy_s= 0.88893 accuracy_f= 0.85059 time= 0.24241
Epoch: 0161 train_loss= 1515.43066 feature_loss= 1.36179 structure_loss= 1514.06885 accuracy= 0.77360 accuracy_s= 0.88964 accuracy_f= 0.85028 time= 0.24261
Epoch: 0171 train_loss= 1453.54602 feature_loss= 1.28174 structure_loss= 1452.26428 accuracy= 0.77471 accuracy_s= 0.89299 accuracy_f= 0.84744 time= 0.24190
Epoch: 0181 train_loss= 1391.70398 feature_loss= 1.35863 structure_loss= 1390.34534 accuracy= 0.77492 accuracy_s= 0.89552 accuracy_f= 0.84541 time= 0.24263
Epoch: 0191 train_loss= 1334.65417 feature_loss= 1.35110 structure_loss= 1333.30310 accuracy= 0.77522 accuracy_s= 0.89836 accuracy_f= 0.84906 time= 0.23931
Epoch: 0201 train_loss= 1278.09387 feature_loss= 1.34360 structure_loss= 1276.75024 accuracy= 0.77461 accuracy_s= 0.89927 accuracy_f= 0.85008 time= 0.24371
Epoch: 0211 train_loss= 1223.28052 feature_loss= 1.32278 structure_loss= 1221.95776 accuracy= 0.77532 accuracy_s= 0.90019 accuracy_f= 0.85069 time= 0.24252
Epoch: 0221 train_loss= 1170.63147 feature_loss= 1.29060 structure_loss= 1169.34082 accuracy= 0.77613 accuracy_s= 0.90191 accuracy_f= 0.85109 time= 0.24340
Epoch: 0231 train_loss= 1120.07849 feature_loss= 1.26165 structure_loss= 1118.81689 accuracy= 0.77492 accuracy_s= 0.90181 accuracy_f= 0.85069 time= 0.24141
Epoch: 0241 train_loss= 1071.94238 feature_loss= 1.22294 structure_loss= 1070.71948 accuracy= 0.77431 accuracy_s= 0.90171 accuracy_f= 0.85038 time= 0.24105
Epoch: 0251 train_loss= 1024.74976 feature_loss= 1.19658 structure_loss= 1023.55316 accuracy= 0.77380 accuracy_s= 0.90151 accuracy_f= 0.85089 time= 0.24065
Epoch: 0261 train_loss= 979.03284 feature_loss= 1.16011 structure_loss= 977.87274 accuracy= 0.77390 accuracy_s= 0.90140 accuracy_f= 0.85079 time= 0.24369
Epoch: 0271 train_loss= 934.92090 feature_loss= 1.11904 structure_loss= 933.80188 accuracy= 0.77431 accuracy_s= 0.90120 accuracy_f= 0.85109 time= 0.24076
Epoch: 0281 train_loss= 892.84583 feature_loss= 1.05438 structure_loss= 891.79144 accuracy= 0.77563 accuracy_s= 0.90130 accuracy_f= 0.85048 time= 0.24011
Epoch: 0291 train_loss= 850.53705 feature_loss= 0.57177 structure_loss= 849.96527 accuracy= 0.77552 accuracy_s= 0.90171 accuracy_f= 0.85150 time= 0.24271

accuracy 0.77542
accuracy_s 0.90171
accuracy_f 0.85190
auc 0.66394
f1_score 0.44650
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=1, beta=1, clique_size=50, cuda=False, dataset='pubmed', device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 325
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
using wavelet scattering transform
y_features shape after scatting (19717, 6500) <class 'numpy.ndarray'>
using PCA to reduce dim
y_features shape after PCA torch.Size([19717, 125]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 4186.33350 feature_loss= 21.18504 structure_loss= 4165.14844 accuracy= 0.76051 accuracy_s= 0.83689 accuracy_f= 0.81204 time= 0.41180
Epoch: 0011 train_loss= 1145.32556 feature_loss= 17.39369 structure_loss= 1127.93188 accuracy= 0.66689 accuracy_s= 0.80180 accuracy_f= 0.81265 time= 0.36367
Epoch: 0021 train_loss= 522.04077 feature_loss= 12.39232 structure_loss= 509.64847 accuracy= 0.66080 accuracy_s= 0.80017 accuracy_f= 0.81498 time= 0.36438
Epoch: 0031 train_loss= 142.23523 feature_loss= 6.13157 structure_loss= 136.10365 accuracy= 0.66252 accuracy_s= 0.79794 accuracy_f= 0.81610 time= 0.36333
Epoch: 0041 train_loss= 13.77923 feature_loss= 2.17296 structure_loss= 11.60628 accuracy= 0.66090 accuracy_s= 0.79713 accuracy_f= 0.83831 time= 0.36209
Epoch: 0051 train_loss= 3.44102 feature_loss= 1.81527 structure_loss= 1.62575 accuracy= 0.67358 accuracy_s= 0.79713 accuracy_f= 0.84927 time= 0.36198
Epoch: 0061 train_loss= 2.15073 feature_loss= 1.54409 structure_loss= 0.60664 accuracy= 0.68210 accuracy_s= 0.79713 accuracy_f= 0.85008 time= 0.36385
Epoch: 0071 train_loss= 2.00084 feature_loss= 1.48154 structure_loss= 0.51930 accuracy= 0.68819 accuracy_s= 0.79713 accuracy_f= 0.85028 time= 0.35961
Epoch: 0081 train_loss= 1.91805 feature_loss= 1.44313 structure_loss= 0.47492 accuracy= 0.68514 accuracy_s= 0.79713 accuracy_f= 0.84988 time= 0.36425
Epoch: 0091 train_loss= 1.47807 feature_loss= 1.01655 structure_loss= 0.46152 accuracy= 0.67034 accuracy_s= 0.79713 accuracy_f= 0.85140 time= 0.36054
Epoch: 0101 train_loss= 1.09585 feature_loss= 0.64176 structure_loss= 0.45410 accuracy= 0.67835 accuracy_s= 0.79713 accuracy_f= 0.85201 time= 0.36402
Epoch: 0111 train_loss= 0.84529 feature_loss= 0.39278 structure_loss= 0.45252 accuracy= 0.68606 accuracy_s= 0.79713 accuracy_f= 0.85170 time= 0.36412
Epoch: 0121 train_loss= 0.86858 feature_loss= 0.41949 structure_loss= 0.44910 accuracy= 0.68616 accuracy_s= 0.79713 accuracy_f= 0.85180 time= 0.36611
Epoch: 0131 train_loss= 0.83763 feature_loss= 0.38930 structure_loss= 0.44833 accuracy= 0.68656 accuracy_s= 0.79713 accuracy_f= 0.85201 time= 0.36093
Epoch: 0141 train_loss= 0.83289 feature_loss= 0.38869 structure_loss= 0.44421 accuracy= 0.68707 accuracy_s= 0.79713 accuracy_f= 0.85190 time= 0.35997
Epoch: 0151 train_loss= 0.82875 feature_loss= 0.38615 structure_loss= 0.44259 accuracy= 0.68727 accuracy_s= 0.79713 accuracy_f= 0.85201 time= 0.35944
Epoch: 0161 train_loss= 0.82610 feature_loss= 0.38574 structure_loss= 0.44037 accuracy= 0.68748 accuracy_s= 0.79713 accuracy_f= 0.85211 time= 0.36358
Epoch: 0171 train_loss= 0.82383 feature_loss= 0.38550 structure_loss= 0.43833 accuracy= 0.68748 accuracy_s= 0.79713 accuracy_f= 0.85211 time= 0.36210
Epoch: 0181 train_loss= 0.82400 feature_loss= 0.38543 structure_loss= 0.43857 accuracy= 0.68798 accuracy_s= 0.79713 accuracy_f= 0.85211 time= 0.35792
Epoch: 0191 train_loss= 0.82054 feature_loss= 0.38541 structure_loss= 0.43513 accuracy= 0.68809 accuracy_s= 0.79713 accuracy_f= 0.85211 time= 0.36049
Epoch: 0201 train_loss= 0.82013 feature_loss= 0.38540 structure_loss= 0.43474 accuracy= 0.68758 accuracy_s= 0.79713 accuracy_f= 0.85211 time= 0.36387
Epoch: 0211 train_loss= 0.81854 feature_loss= 0.38539 structure_loss= 0.43315 accuracy= 0.68768 accuracy_s= 0.79713 accuracy_f= 0.85211 time= 0.36680
Epoch: 0221 train_loss= 0.81652 feature_loss= 0.38539 structure_loss= 0.43113 accuracy= 0.68778 accuracy_s= 0.79713 accuracy_f= 0.85211 time= 0.36079
Epoch: 0231 train_loss= 0.81783 feature_loss= 0.38539 structure_loss= 0.43243 accuracy= 0.68758 accuracy_s= 0.79713 accuracy_f= 0.85211 time= 0.36494
Epoch: 0241 train_loss= 0.81671 feature_loss= 0.38539 structure_loss= 0.43132 accuracy= 0.68758 accuracy_s= 0.79713 accuracy_f= 0.85211 time= 0.36531
Epoch: 0251 train_loss= 0.81412 feature_loss= 0.38540 structure_loss= 0.42872 accuracy= 0.68758 accuracy_s= 0.79713 accuracy_f= 0.85211 time= 0.36184
Epoch: 0261 train_loss= 0.81409 feature_loss= 0.38539 structure_loss= 0.42870 accuracy= 0.68809 accuracy_s= 0.79713 accuracy_f= 0.85211 time= 0.36806
Epoch: 0271 train_loss= 0.81326 feature_loss= 0.38539 structure_loss= 0.42787 accuracy= 0.68798 accuracy_s= 0.79713 accuracy_f= 0.85211 time= 0.36085
Epoch: 0281 train_loss= 0.81135 feature_loss= 0.38539 structure_loss= 0.42596 accuracy= 0.68788 accuracy_s= 0.79713 accuracy_f= 0.85211 time= 0.36584
Epoch: 0291 train_loss= 0.81133 feature_loss= 0.38539 structure_loss= 0.42594 accuracy= 0.68778 accuracy_s= 0.79713 accuracy_f= 0.85211 time= 0.37025

accuracy 0.68809
accuracy_s 0.79713
accuracy_f 0.85211
auc 0.36454
f1_score 0.23125
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=1, beta=1, clique_size=50, cuda=False, dataset='pubmed', device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 864
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
using wavelet scattering transform
y_features shape after scatting (19717, 6500) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 87 steps!
y_features shape after FMS torch.Size([19717, 125]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 96.88015 feature_loss= 20.97363 structure_loss= 75.90652 accuracy= 0.79906 accuracy_s= 0.86874 accuracy_f= 0.80991 time= 0.41484
Epoch: 0011 train_loss= 27.50661 feature_loss= 14.04721 structure_loss= 13.45941 accuracy= 0.77542 accuracy_s= 0.83233 accuracy_f= 0.80372 time= 0.36395
Epoch: 0021 train_loss= 12.69180 feature_loss= 5.90622 structure_loss= 6.78557 accuracy= 0.70949 accuracy_s= 0.80474 accuracy_f= 0.81376 time= 0.36302
Epoch: 0031 train_loss= 6.22416 feature_loss= 2.34967 structure_loss= 3.87449 accuracy= 0.67460 accuracy_s= 0.80403 accuracy_f= 0.82056 time= 0.36423
Epoch: 0041 train_loss= 4.22833 feature_loss= 1.84192 structure_loss= 2.38642 accuracy= 0.66861 accuracy_s= 0.80322 accuracy_f= 0.81874 time= 0.36555
Epoch: 0051 train_loss= 3.25263 feature_loss= 1.67459 structure_loss= 1.57804 accuracy= 0.66912 accuracy_s= 0.80220 accuracy_f= 0.81823 time= 0.36409
Epoch: 0061 train_loss= 2.65582 feature_loss= 1.52368 structure_loss= 1.13214 accuracy= 0.67064 accuracy_s= 0.80119 accuracy_f= 0.81843 time= 0.36440
Epoch: 0071 train_loss= 1.84078 feature_loss= 0.96101 structure_loss= 0.87977 accuracy= 0.66831 accuracy_s= 0.80017 accuracy_f= 0.81833 time= 0.36534
Epoch: 0081 train_loss= 1.40402 feature_loss= 0.68020 structure_loss= 0.72383 accuracy= 0.67277 accuracy_s= 0.79967 accuracy_f= 0.81914 time= 0.36612
Epoch: 0091 train_loss= 1.27185 feature_loss= 0.64493 structure_loss= 0.62692 accuracy= 0.67206 accuracy_s= 0.79855 accuracy_f= 0.81924 time= 0.36337
Epoch: 0101 train_loss= 1.19086 feature_loss= 0.62536 structure_loss= 0.56549 accuracy= 0.67439 accuracy_s= 0.79825 accuracy_f= 0.81884 time= 0.36596
Epoch: 0111 train_loss= 1.13054 feature_loss= 0.60674 structure_loss= 0.52380 accuracy= 0.67551 accuracy_s= 0.79784 accuracy_f= 0.81955 time= 0.36731
Epoch: 0121 train_loss= 1.09433 feature_loss= 0.59817 structure_loss= 0.49616 accuracy= 0.67652 accuracy_s= 0.79774 accuracy_f= 0.81985 time= 0.36700
Epoch: 0131 train_loss= 1.06524 feature_loss= 0.58842 structure_loss= 0.47683 accuracy= 0.67744 accuracy_s= 0.79764 accuracy_f= 0.82005 time= 0.36595
Epoch: 0141 train_loss= 1.04325 feature_loss= 0.58032 structure_loss= 0.46294 accuracy= 0.67804 accuracy_s= 0.79764 accuracy_f= 0.82046 time= 0.36734
Epoch: 0151 train_loss= 1.02476 feature_loss= 0.57221 structure_loss= 0.45255 accuracy= 0.67815 accuracy_s= 0.79754 accuracy_f= 0.82076 time= 0.36800
Epoch: 0161 train_loss= 1.00996 feature_loss= 0.56457 structure_loss= 0.44539 accuracy= 0.67886 accuracy_s= 0.79733 accuracy_f= 0.82056 time= 0.36671
Epoch: 0171 train_loss= 0.99663 feature_loss= 0.55691 structure_loss= 0.43972 accuracy= 0.67936 accuracy_s= 0.79723 accuracy_f= 0.82107 time= 0.36718
Epoch: 0181 train_loss= 0.98503 feature_loss= 0.54965 structure_loss= 0.43539 accuracy= 0.67977 accuracy_s= 0.79723 accuracy_f= 0.82137 time= 0.36591
Epoch: 0191 train_loss= 0.97429 feature_loss= 0.54242 structure_loss= 0.43187 accuracy= 0.68058 accuracy_s= 0.79723 accuracy_f= 0.82127 time= 0.36583
Epoch: 0201 train_loss= 0.96412 feature_loss= 0.53511 structure_loss= 0.42901 accuracy= 0.68088 accuracy_s= 0.79723 accuracy_f= 0.82168 time= 0.36322
Epoch: 0211 train_loss= 0.95517 feature_loss= 0.52817 structure_loss= 0.42701 accuracy= 0.68028 accuracy_s= 0.79723 accuracy_f= 0.82249 time= 0.36599
Epoch: 0221 train_loss= 0.94630 feature_loss= 0.52122 structure_loss= 0.42508 accuracy= 0.68149 accuracy_s= 0.79723 accuracy_f= 0.82340 time= 0.36441
Epoch: 0231 train_loss= 0.93825 feature_loss= 0.51445 structure_loss= 0.42380 accuracy= 0.68088 accuracy_s= 0.79723 accuracy_f= 0.82421 time= 0.36731
Epoch: 0241 train_loss= 0.93007 feature_loss= 0.50774 structure_loss= 0.42234 accuracy= 0.68149 accuracy_s= 0.79723 accuracy_f= 0.82502 time= 0.36927
Epoch: 0251 train_loss= 0.92244 feature_loss= 0.50118 structure_loss= 0.42126 accuracy= 0.68119 accuracy_s= 0.79723 accuracy_f= 0.82634 time= 0.36846
Epoch: 0261 train_loss= 0.91514 feature_loss= 0.49477 structure_loss= 0.42037 accuracy= 0.68149 accuracy_s= 0.79723 accuracy_f= 0.82655 time= 0.36224
Epoch: 0271 train_loss= 0.90818 feature_loss= 0.48852 structure_loss= 0.41966 accuracy= 0.68159 accuracy_s= 0.79723 accuracy_f= 0.82817 time= 0.36281
Epoch: 0281 train_loss= 0.90120 feature_loss= 0.48229 structure_loss= 0.41890 accuracy= 0.68200 accuracy_s= 0.79723 accuracy_f= 0.82878 time= 0.36376
Epoch: 0291 train_loss= 0.89449 feature_loss= 0.47615 structure_loss= 0.41834 accuracy= 0.68281 accuracy_s= 0.79723 accuracy_f= 0.82949 time= 0.36265

accuracy 0.68322
accuracy_s 0.79723
accuracy_f 0.83040
auc 0.36343
f1_score 0.21925
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=2, beta=1, clique_size=50, cuda=False, dataset='pubmed', device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 522
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
using wavelet scattering transform
y_features shape after scatting (19717, 6500) <class 'numpy.ndarray'>
using PCA to reduce dim
y_features shape after PCA torch.Size([19717, 125]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 3020.89746 feature_loss= 21.27531 structure_loss= 2999.62207 accuracy= 0.74905 accuracy_s= 0.82959 accuracy_f= 0.81529 time= 0.27878
Epoch: 0011 train_loss= 880.21960 feature_loss= 16.60361 structure_loss= 863.61597 accuracy= 0.66618 accuracy_s= 0.80169 accuracy_f= 0.81620 time= 0.23831
Epoch: 0021 train_loss= 537.21143 feature_loss= 10.76751 structure_loss= 526.44391 accuracy= 0.65705 accuracy_s= 0.80058 accuracy_f= 0.81660 time= 0.24372
Epoch: 0031 train_loss= 272.00821 feature_loss= 6.97405 structure_loss= 265.03415 accuracy= 0.65867 accuracy_s= 0.79875 accuracy_f= 0.81833 time= 0.24397
Epoch: 0041 train_loss= 111.93443 feature_loss= 3.32130 structure_loss= 108.61313 accuracy= 0.65279 accuracy_s= 0.79723 accuracy_f= 0.81884 time= 0.23622
Epoch: 0051 train_loss= 36.52052 feature_loss= 2.39684 structure_loss= 34.12368 accuracy= 0.64974 accuracy_s= 0.79713 accuracy_f= 0.83567 time= 0.23698
Epoch: 0061 train_loss= 10.98824 feature_loss= 1.99619 structure_loss= 8.99205 accuracy= 0.64924 accuracy_s= 0.79713 accuracy_f= 0.84379 time= 0.24451
Epoch: 0071 train_loss= 4.31424 feature_loss= 1.58608 structure_loss= 2.72817 accuracy= 0.64863 accuracy_s= 0.79713 accuracy_f= 0.84633 time= 0.24312
Epoch: 0081 train_loss= 2.79093 feature_loss= 1.46716 structure_loss= 1.32377 accuracy= 0.65056 accuracy_s= 0.79713 accuracy_f= 0.84835 time= 0.23641
Epoch: 0091 train_loss= 2.35927 feature_loss= 1.41131 structure_loss= 0.94796 accuracy= 0.65370 accuracy_s= 0.79713 accuracy_f= 0.84998 time= 0.24455
Epoch: 0101 train_loss= 2.18497 feature_loss= 1.39765 structure_loss= 0.78733 accuracy= 0.66080 accuracy_s= 0.79713 accuracy_f= 0.85079 time= 0.24372
Epoch: 0111 train_loss= 2.10701 feature_loss= 1.39479 structure_loss= 0.71222 accuracy= 0.66313 accuracy_s= 0.79713 accuracy_f= 0.85028 time= 0.24331
Epoch: 0121 train_loss= 2.02784 feature_loss= 1.37176 structure_loss= 0.65607 accuracy= 0.66861 accuracy_s= 0.79713 accuracy_f= 0.85059 time= 0.24365
Epoch: 0131 train_loss= 1.99957 feature_loss= 1.38493 structure_loss= 0.61464 accuracy= 0.67176 accuracy_s= 0.79713 accuracy_f= 0.85018 time= 0.23648
Epoch: 0141 train_loss= 1.96153 feature_loss= 1.37350 structure_loss= 0.58803 accuracy= 0.67419 accuracy_s= 0.79713 accuracy_f= 0.85008 time= 0.24267
Epoch: 0151 train_loss= 1.92893 feature_loss= 1.35605 structure_loss= 0.57288 accuracy= 0.67551 accuracy_s= 0.79713 accuracy_f= 0.85028 time= 0.23755
Epoch: 0161 train_loss= 1.89442 feature_loss= 1.34462 structure_loss= 0.54980 accuracy= 0.67723 accuracy_s= 0.79713 accuracy_f= 0.85048 time= 0.24186
Epoch: 0171 train_loss= 1.83550 feature_loss= 1.30000 structure_loss= 0.53550 accuracy= 0.67794 accuracy_s= 0.79713 accuracy_f= 0.85079 time= 0.24099
Epoch: 0181 train_loss= 1.34887 feature_loss= 0.82821 structure_loss= 0.52066 accuracy= 0.66415 accuracy_s= 0.79713 accuracy_f= 0.85170 time= 0.23823
Epoch: 0191 train_loss= 0.95455 feature_loss= 0.44767 structure_loss= 0.50688 accuracy= 0.67490 accuracy_s= 0.79713 accuracy_f= 0.85231 time= 0.24230
Epoch: 0201 train_loss= 0.92418 feature_loss= 0.42293 structure_loss= 0.50125 accuracy= 0.67845 accuracy_s= 0.79713 accuracy_f= 0.85221 time= 0.24202
Epoch: 0211 train_loss= 0.89117 feature_loss= 0.39680 structure_loss= 0.49437 accuracy= 0.67896 accuracy_s= 0.79713 accuracy_f= 0.85150 time= 0.24504
Epoch: 0221 train_loss= 0.87297 feature_loss= 0.38943 structure_loss= 0.48354 accuracy= 0.68017 accuracy_s= 0.79713 accuracy_f= 0.85201 time= 0.24136
Epoch: 0231 train_loss= 0.86360 feature_loss= 0.38682 structure_loss= 0.47678 accuracy= 0.68038 accuracy_s= 0.79713 accuracy_f= 0.85221 time= 0.23843
Epoch: 0241 train_loss= 0.85882 feature_loss= 0.38593 structure_loss= 0.47289 accuracy= 0.68170 accuracy_s= 0.79713 accuracy_f= 0.85201 time= 0.23779
Epoch: 0251 train_loss= 0.85208 feature_loss= 0.38555 structure_loss= 0.46653 accuracy= 0.68190 accuracy_s= 0.79713 accuracy_f= 0.85190 time= 0.24213
Epoch: 0261 train_loss= 0.84961 feature_loss= 0.38548 structure_loss= 0.46413 accuracy= 0.68332 accuracy_s= 0.79713 accuracy_f= 0.85201 time= 0.23996
Epoch: 0271 train_loss= 0.84346 feature_loss= 0.38577 structure_loss= 0.45770 accuracy= 0.68301 accuracy_s= 0.79713 accuracy_f= 0.85221 time= 0.24046
Epoch: 0281 train_loss= 0.84062 feature_loss= 0.38540 structure_loss= 0.45522 accuracy= 0.68372 accuracy_s= 0.79713 accuracy_f= 0.85211 time= 0.23998
Epoch: 0291 train_loss= 0.83603 feature_loss= 0.38599 structure_loss= 0.45004 accuracy= 0.68454 accuracy_s= 0.79713 accuracy_f= 0.85201 time= 0.23746

accuracy 0.68474
accuracy_s 0.79713
accuracy_f 0.85211
auc 0.36142
f1_score 0.22300
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=2, beta=1, clique_size=50, cuda=False, dataset='pubmed', device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 466
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
using wavelet scattering transform
y_features shape after scatting (19717, 6500) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 87 steps!
y_features shape after FMS torch.Size([19717, 125]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 56.43591 feature_loss= 20.96679 structure_loss= 35.46913 accuracy= 0.79226 accuracy_s= 0.86377 accuracy_f= 0.80971 time= 0.29122
Epoch: 0011 train_loss= 27.11924 feature_loss= 14.84714 structure_loss= 12.27210 accuracy= 0.80900 accuracy_s= 0.88365 accuracy_f= 0.80190 time= 0.24334
Epoch: 0021 train_loss= 11.87678 feature_loss= 6.01790 structure_loss= 5.85887 accuracy= 0.78983 accuracy_s= 0.88386 accuracy_f= 0.80453 time= 0.24357
Epoch: 0031 train_loss= 6.32774 feature_loss= 3.08198 structure_loss= 3.24576 accuracy= 0.76903 accuracy_s= 0.86813 accuracy_f= 0.81316 time= 0.24157
Epoch: 0041 train_loss= 4.14004 feature_loss= 2.22931 structure_loss= 1.91073 accuracy= 0.74621 accuracy_s= 0.85474 accuracy_f= 0.81539 time= 0.24022
Epoch: 0051 train_loss= 3.19162 feature_loss= 1.97436 structure_loss= 1.21726 accuracy= 0.72156 accuracy_s= 0.84714 accuracy_f= 0.81589 time= 0.23893
Epoch: 0061 train_loss= 2.60506 feature_loss= 1.74038 structure_loss= 0.86468 accuracy= 0.70158 accuracy_s= 0.84196 accuracy_f= 0.81701 time= 0.24178
Epoch: 0071 train_loss= 1.80829 feature_loss= 1.12390 structure_loss= 0.68439 accuracy= 0.68626 accuracy_s= 0.83598 accuracy_f= 0.81762 time= 0.24216
Epoch: 0081 train_loss= 1.50073 feature_loss= 0.91329 structure_loss= 0.58745 accuracy= 0.67409 accuracy_s= 0.82604 accuracy_f= 0.81843 time= 0.24207
Epoch: 0091 train_loss= 1.41588 feature_loss= 0.88349 structure_loss= 0.53240 accuracy= 0.67460 accuracy_s= 0.81427 accuracy_f= 0.81853 time= 0.24450
Epoch: 0101 train_loss= 1.35065 feature_loss= 0.85097 structure_loss= 0.49968 accuracy= 0.67439 accuracy_s= 0.80839 accuracy_f= 0.81853 time= 0.24561
Epoch: 0111 train_loss= 1.30383 feature_loss= 0.82520 structure_loss= 0.47863 accuracy= 0.67541 accuracy_s= 0.80180 accuracy_f= 0.81833 time= 0.24289
Epoch: 0121 train_loss= 1.27209 feature_loss= 0.80742 structure_loss= 0.46467 accuracy= 0.67642 accuracy_s= 0.79967 accuracy_f= 0.81863 time= 0.24512
Epoch: 0131 train_loss= 1.24521 feature_loss= 0.79059 structure_loss= 0.45463 accuracy= 0.67744 accuracy_s= 0.79885 accuracy_f= 0.81874 time= 0.23886
Epoch: 0141 train_loss= 1.22240 feature_loss= 0.77532 structure_loss= 0.44708 accuracy= 0.67794 accuracy_s= 0.79825 accuracy_f= 0.81894 time= 0.23828
Epoch: 0151 train_loss= 1.20245 feature_loss= 0.76084 structure_loss= 0.44161 accuracy= 0.67865 accuracy_s= 0.79784 accuracy_f= 0.81894 time= 0.24346
Epoch: 0161 train_loss= 1.18421 feature_loss= 0.74695 structure_loss= 0.43725 accuracy= 0.67997 accuracy_s= 0.79754 accuracy_f= 0.81874 time= 0.24482
Epoch: 0171 train_loss= 1.16697 feature_loss= 0.73324 structure_loss= 0.43372 accuracy= 0.68017 accuracy_s= 0.79754 accuracy_f= 0.81884 time= 0.24525
Epoch: 0181 train_loss= 1.15076 feature_loss= 0.71977 structure_loss= 0.43098 accuracy= 0.68149 accuracy_s= 0.79743 accuracy_f= 0.81874 time= 0.24334
Epoch: 0191 train_loss= 1.13526 feature_loss= 0.70662 structure_loss= 0.42864 accuracy= 0.68180 accuracy_s= 0.79743 accuracy_f= 0.81894 time= 0.24369
Epoch: 0201 train_loss= 1.12052 feature_loss= 0.69366 structure_loss= 0.42686 accuracy= 0.68251 accuracy_s= 0.79743 accuracy_f= 0.81874 time= 0.24465
Epoch: 0211 train_loss= 1.10634 feature_loss= 0.68103 structure_loss= 0.42531 accuracy= 0.68129 accuracy_s= 0.79743 accuracy_f= 0.81874 time= 0.24567
Epoch: 0221 train_loss= 1.09227 feature_loss= 0.66834 structure_loss= 0.42392 accuracy= 0.68149 accuracy_s= 0.79743 accuracy_f= 0.81874 time= 0.24412
Epoch: 0231 train_loss= 1.07855 feature_loss= 0.65583 structure_loss= 0.42272 accuracy= 0.68190 accuracy_s= 0.79733 accuracy_f= 0.81884 time= 0.24297
Epoch: 0241 train_loss= 1.06523 feature_loss= 0.64343 structure_loss= 0.42180 accuracy= 0.68109 accuracy_s= 0.79733 accuracy_f= 0.81904 time= 0.24273
Epoch: 0251 train_loss= 1.05215 feature_loss= 0.63129 structure_loss= 0.42087 accuracy= 0.68149 accuracy_s= 0.79733 accuracy_f= 0.81894 time= 0.23314
Epoch: 0261 train_loss= 1.03958 feature_loss= 0.61955 structure_loss= 0.42003 accuracy= 0.68048 accuracy_s= 0.79733 accuracy_f= 0.81874 time= 0.24230
Epoch: 0271 train_loss= 1.02693 feature_loss= 0.60763 structure_loss= 0.41930 accuracy= 0.68159 accuracy_s= 0.79733 accuracy_f= 0.81914 time= 0.24357
Epoch: 0281 train_loss= 1.01490 feature_loss= 0.59617 structure_loss= 0.41873 accuracy= 0.68200 accuracy_s= 0.79733 accuracy_f= 0.81975 time= 0.24304
Epoch: 0291 train_loss= 1.00285 feature_loss= 0.58471 structure_loss= 0.41814 accuracy= 0.68322 accuracy_s= 0.79733 accuracy_f= 0.82076 time= 0.24238

accuracy 0.68251
accuracy_s 0.79723
accuracy_f 0.82056
auc 0.37706
f1_score 0.21750
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=3, beta=1, clique_size=50, cuda=False, dataset='pubmed', device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 358
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
using wavelet scattering transform
y_features shape after scatting (19717, 6500) <class 'numpy.ndarray'>
using PCA to reduce dim
y_features shape after PCA torch.Size([19717, 125]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 6480.20459 feature_loss= 21.26497 structure_loss= 6458.93945 accuracy= 0.71740 accuracy_s= 0.82979 accuracy_f= 0.81549 time= 0.40110
Epoch: 0011 train_loss= 3973.71191 feature_loss= 15.76618 structure_loss= 3957.94580 accuracy= 0.67551 accuracy_s= 0.80971 accuracy_f= 0.81803 time= 0.36319
Epoch: 0021 train_loss= 3470.43506 feature_loss= 8.73195 structure_loss= 3461.70312 accuracy= 0.67196 accuracy_s= 0.80464 accuracy_f= 0.81904 time= 0.36319
Epoch: 0031 train_loss= 3153.32886 feature_loss= 3.94767 structure_loss= 3149.38110 accuracy= 0.66242 accuracy_s= 0.80342 accuracy_f= 0.82381 time= 0.36289
Epoch: 0041 train_loss= 2917.27466 feature_loss= 2.68593 structure_loss= 2914.58862 accuracy= 0.66181 accuracy_s= 0.80301 accuracy_f= 0.83060 time= 0.36112
Epoch: 0051 train_loss= 2727.08618 feature_loss= 2.49162 structure_loss= 2724.59448 accuracy= 0.65938 accuracy_s= 0.80261 accuracy_f= 0.84156 time= 0.36035
Epoch: 0061 train_loss= 2568.98682 feature_loss= 2.37408 structure_loss= 2566.61279 accuracy= 0.65847 accuracy_s= 0.80240 accuracy_f= 0.84399 time= 0.36151
Epoch: 0071 train_loss= 2430.76758 feature_loss= 2.26368 structure_loss= 2428.50391 accuracy= 0.65908 accuracy_s= 0.80190 accuracy_f= 0.84673 time= 0.36011
Epoch: 0081 train_loss= 2307.61597 feature_loss= 2.16178 structure_loss= 2305.45410 accuracy= 0.65735 accuracy_s= 0.80200 accuracy_f= 0.84551 time= 0.36158
Epoch: 0091 train_loss= 2191.94019 feature_loss= 2.04882 structure_loss= 2189.89136 accuracy= 0.65857 accuracy_s= 0.80190 accuracy_f= 0.84562 time= 0.35973
Epoch: 0101 train_loss= 2086.26807 feature_loss= 1.77213 structure_loss= 2084.49585 accuracy= 0.65826 accuracy_s= 0.80210 accuracy_f= 0.84328 time= 0.36322
Epoch: 0111 train_loss= 1984.64551 feature_loss= 1.16830 structure_loss= 1983.47717 accuracy= 0.65786 accuracy_s= 0.80180 accuracy_f= 0.84988 time= 0.36400
Epoch: 0121 train_loss= 1895.38586 feature_loss= 0.75705 structure_loss= 1894.62878 accuracy= 0.65766 accuracy_s= 0.80180 accuracy_f= 0.85109 time= 0.36303
Epoch: 0131 train_loss= 1808.43677 feature_loss= 0.75564 structure_loss= 1807.68115 accuracy= 0.65837 accuracy_s= 0.80169 accuracy_f= 0.85170 time= 0.36252
Epoch: 0141 train_loss= 1727.23523 feature_loss= 0.58073 structure_loss= 1726.65454 accuracy= 0.65786 accuracy_s= 0.80159 accuracy_f= 0.85140 time= 0.36222
Epoch: 0151 train_loss= 1647.76807 feature_loss= 0.42648 structure_loss= 1647.34155 accuracy= 0.65837 accuracy_s= 0.80180 accuracy_f= 0.85140 time= 0.36447
Epoch: 0161 train_loss= 1573.51123 feature_loss= 0.39663 structure_loss= 1573.11462 accuracy= 0.65766 accuracy_s= 0.80139 accuracy_f= 0.85190 time= 0.36420
Epoch: 0171 train_loss= 1500.20105 feature_loss= 0.39055 structure_loss= 1499.81055 accuracy= 0.65745 accuracy_s= 0.80169 accuracy_f= 0.85190 time= 0.36471
Epoch: 0181 train_loss= 1435.62012 feature_loss= 0.38780 structure_loss= 1435.23230 accuracy= 0.65816 accuracy_s= 0.80159 accuracy_f= 0.85170 time= 0.36195
Epoch: 0191 train_loss= 1371.63208 feature_loss= 0.38565 structure_loss= 1371.24646 accuracy= 0.65796 accuracy_s= 0.80159 accuracy_f= 0.85211 time= 0.36276
Epoch: 0201 train_loss= 1307.56726 feature_loss= 0.38574 structure_loss= 1307.18152 accuracy= 0.65725 accuracy_s= 0.80159 accuracy_f= 0.85201 time= 0.36281
Epoch: 0211 train_loss= 1249.86108 feature_loss= 0.38541 structure_loss= 1249.47571 accuracy= 0.65735 accuracy_s= 0.80139 accuracy_f= 0.85201 time= 0.36239
Epoch: 0221 train_loss= 1192.97327 feature_loss= 0.38542 structure_loss= 1192.58789 accuracy= 0.65816 accuracy_s= 0.80139 accuracy_f= 0.85201 time= 0.36333
Epoch: 0231 train_loss= 1140.20264 feature_loss= 0.38623 structure_loss= 1139.81641 accuracy= 0.65806 accuracy_s= 0.80129 accuracy_f= 0.85201 time= 0.36367
Epoch: 0241 train_loss= 1086.77734 feature_loss= 0.38539 structure_loss= 1086.39197 accuracy= 0.65695 accuracy_s= 0.80149 accuracy_f= 0.85211 time= 0.36375
Epoch: 0251 train_loss= 1037.82715 feature_loss= 0.38539 structure_loss= 1037.44177 accuracy= 0.65867 accuracy_s= 0.80149 accuracy_f= 0.85211 time= 0.36265
Epoch: 0261 train_loss= 988.92059 feature_loss= 0.38539 structure_loss= 988.53522 accuracy= 0.65887 accuracy_s= 0.80098 accuracy_f= 0.85211 time= 0.36135
Epoch: 0271 train_loss= 943.39819 feature_loss= 0.38553 structure_loss= 943.01263 accuracy= 0.65847 accuracy_s= 0.80098 accuracy_f= 0.85201 time= 0.36255
Epoch: 0281 train_loss= 898.83240 feature_loss= 0.38539 structure_loss= 898.44702 accuracy= 0.65857 accuracy_s= 0.80129 accuracy_f= 0.85211 time= 0.36546
Epoch: 0291 train_loss= 856.80597 feature_loss= 0.38539 structure_loss= 856.42059 accuracy= 0.65796 accuracy_s= 0.80129 accuracy_f= 0.85211 time= 0.36479

accuracy 0.65755
accuracy_s 0.80109
accuracy_f 0.85170
auc 0.34203
f1_score 0.15600
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=3, beta=1, clique_size=50, cuda=False, dataset='pubmed', device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=40, weight_decay=0.0005)
random seed: 211
Found existing ad data, loading...
feature_dim: 500 hidden1_dim: 250 hidden2_dim: 125 nodes_num: 19717 anomaly_num: 4000

Start modeling
using wavelet scattering transform
y_features shape after scatting (19717, 6500) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 87 steps!
y_features shape after FMS torch.Size([19717, 125]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 6261.43945 feature_loss= 21.06875 structure_loss= 6240.37061 accuracy= 0.76954 accuracy_s= 0.84176 accuracy_f= 0.80839 time= 0.40324
Epoch: 0011 train_loss= 3893.32324 feature_loss= 14.93049 structure_loss= 3878.39282 accuracy= 0.73566 accuracy_s= 0.83821 accuracy_f= 0.80545 time= 0.36050
Epoch: 0021 train_loss= 3409.39868 feature_loss= 10.37371 structure_loss= 3399.02490 accuracy= 0.75087 accuracy_s= 0.83811 accuracy_f= 0.81103 time= 0.36180
Epoch: 0031 train_loss= 3097.96387 feature_loss= 7.08205 structure_loss= 3090.88184 accuracy= 0.76082 accuracy_s= 0.84409 accuracy_f= 0.81579 time= 0.36175
Epoch: 0041 train_loss= 2865.68628 feature_loss= 4.56059 structure_loss= 2861.12573 accuracy= 0.76102 accuracy_s= 0.85251 accuracy_f= 0.82046 time= 0.36185
Epoch: 0051 train_loss= 2673.14185 feature_loss= 3.51220 structure_loss= 2669.62964 accuracy= 0.76051 accuracy_s= 0.85647 accuracy_f= 0.82350 time= 0.36171
Epoch: 0061 train_loss= 2515.02393 feature_loss= 3.05966 structure_loss= 2511.96436 accuracy= 0.76335 accuracy_s= 0.86185 accuracy_f= 0.83375 time= 0.36325
Epoch: 0071 train_loss= 2374.61938 feature_loss= 2.69012 structure_loss= 2371.92920 accuracy= 0.76376 accuracy_s= 0.86631 accuracy_f= 0.83811 time= 0.36438
Epoch: 0081 train_loss= 2250.29102 feature_loss= 2.16224 structure_loss= 2248.12866 accuracy= 0.76426 accuracy_s= 0.86935 accuracy_f= 0.83943 time= 0.36444
Epoch: 0091 train_loss= 2137.11450 feature_loss= 1.86875 structure_loss= 2135.24585 accuracy= 0.76345 accuracy_s= 0.87503 accuracy_f= 0.84379 time= 0.36549
Epoch: 0101 train_loss= 2031.55103 feature_loss= 1.52733 structure_loss= 2030.02368 accuracy= 0.76528 accuracy_s= 0.87929 accuracy_f= 0.84653 time= 0.36812
Epoch: 0111 train_loss= 1935.74048 feature_loss= 1.46845 structure_loss= 1934.27197 accuracy= 0.76721 accuracy_s= 0.88376 accuracy_f= 0.84967 time= 0.36144
Epoch: 0121 train_loss= 1846.39624 feature_loss= 1.43155 structure_loss= 1844.96472 accuracy= 0.76700 accuracy_s= 0.88670 accuracy_f= 0.85069 time= 0.36450
Epoch: 0131 train_loss= 1760.24890 feature_loss= 1.39514 structure_loss= 1758.85376 accuracy= 0.76741 accuracy_s= 0.88822 accuracy_f= 0.85038 time= 0.36325
Epoch: 0141 train_loss= 1678.28333 feature_loss= 1.34821 structure_loss= 1676.93506 accuracy= 0.76741 accuracy_s= 0.88893 accuracy_f= 0.84896 time= 0.36302
Epoch: 0151 train_loss= 1603.82593 feature_loss= 1.39789 structure_loss= 1602.42798 accuracy= 0.76731 accuracy_s= 0.88903 accuracy_f= 0.84846 time= 0.35949
Epoch: 0161 train_loss= 1530.54089 feature_loss= 1.36939 structure_loss= 1529.17151 accuracy= 0.76741 accuracy_s= 0.88933 accuracy_f= 0.84917 time= 0.36312
Epoch: 0171 train_loss= 1462.13025 feature_loss= 1.28960 structure_loss= 1460.84070 accuracy= 0.76761 accuracy_s= 0.88984 accuracy_f= 0.85008 time= 0.36182
Epoch: 0181 train_loss= 1396.35742 feature_loss= 1.19955 structure_loss= 1395.15784 accuracy= 0.76771 accuracy_s= 0.89075 accuracy_f= 0.85048 time= 0.36267
Epoch: 0191 train_loss= 1332.58069 feature_loss= 1.15258 structure_loss= 1331.42810 accuracy= 0.76741 accuracy_s= 0.89207 accuracy_f= 0.85059 time= 0.35906
Epoch: 0201 train_loss= 1273.24268 feature_loss= 1.10858 structure_loss= 1272.13416 accuracy= 0.76680 accuracy_s= 0.89217 accuracy_f= 0.85119 time= 0.36633
Epoch: 0211 train_loss= 1216.11523 feature_loss= 0.92897 structure_loss= 1215.18628 accuracy= 0.76792 accuracy_s= 0.89177 accuracy_f= 0.85028 time= 0.36322
Epoch: 0221 train_loss= 1159.46094 feature_loss= 0.45856 structure_loss= 1159.00232 accuracy= 0.76792 accuracy_s= 0.89268 accuracy_f= 0.85211 time= 0.36512
Epoch: 0231 train_loss= 1106.14624 feature_loss= 0.41944 structure_loss= 1105.72681 accuracy= 0.76690 accuracy_s= 0.89258 accuracy_f= 0.85180 time= 0.36454
Epoch: 0241 train_loss= 1055.50574 feature_loss= 0.39532 structure_loss= 1055.11047 accuracy= 0.76792 accuracy_s= 0.89319 accuracy_f= 0.85170 time= 0.36501
Epoch: 0251 train_loss= 1007.02698 feature_loss= 0.38922 structure_loss= 1006.63776 accuracy= 0.76700 accuracy_s= 0.89420 accuracy_f= 0.85211 time= 0.36468
Epoch: 0261 train_loss= 959.55420 feature_loss= 0.38686 structure_loss= 959.16736 accuracy= 0.76680 accuracy_s= 0.89370 accuracy_f= 0.85201 time= 0.36734
Epoch: 0271 train_loss= 914.50244 feature_loss= 0.38587 structure_loss= 914.11658 accuracy= 0.76731 accuracy_s= 0.89441 accuracy_f= 0.85201 time= 0.36333
Epoch: 0281 train_loss= 871.90454 feature_loss= 0.38572 structure_loss= 871.51880 accuracy= 0.76802 accuracy_s= 0.89420 accuracy_f= 0.85201 time= 0.36820
Epoch: 0291 train_loss= 829.66901 feature_loss= 0.38545 structure_loss= 829.28357 accuracy= 0.76700 accuracy_s= 0.89461 accuracy_f= 0.85201 time= 0.36146

accuracy 0.76690
accuracy_s 0.89471
accuracy_f 0.85221
auc 0.65351
f1_score 0.42550
Job finished!
pubmed job finished!
amazon_electronics_photo experiment with fixed GCN hidden layer



Initializing normal onedecoder
Namespace(alpha=1, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', decoder=0, device=device(type='cuda'), dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 182
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
Epoch: 0001 train_loss= 24.67105 accuracy= 0.65569 time= 0.06179
Epoch: 0011 train_loss= 16.12351 accuracy= 0.66876 time= 0.04772
Epoch: 0021 train_loss= 15.34637 accuracy= 0.66745 time= 0.04798
Epoch: 0031 train_loss= 14.89425 accuracy= 0.66510 time= 0.04821
Epoch: 0041 train_loss= 14.51458 accuracy= 0.66405 time= 0.04808
Epoch: 0051 train_loss= 14.18713 accuracy= 0.66353 time= 0.04780
Epoch: 0061 train_loss= 13.88852 accuracy= 0.66327 time= 0.04808
Epoch: 0071 train_loss= 13.63484 accuracy= 0.66405 time= 0.04820
Epoch: 0081 train_loss= 13.40897 accuracy= 0.66405 time= 0.04847
Epoch: 0091 train_loss= 13.19640 accuracy= 0.66510 time= 0.04846
Epoch: 0101 train_loss= 13.01369 accuracy= 0.66588 time= 0.04818
Epoch: 0111 train_loss= 12.85798 accuracy= 0.66693 time= 0.04810
Epoch: 0121 train_loss= 12.73396 accuracy= 0.66693 time= 0.04836
Epoch: 0131 train_loss= 12.60246 accuracy= 0.66876 time= 0.04835
Epoch: 0141 train_loss= 12.48635 accuracy= 0.66954 time= 0.04855
Epoch: 0151 train_loss= 12.35747 accuracy= 0.67111 time= 0.04800
Epoch: 0161 train_loss= 12.24474 accuracy= 0.67268 time= 0.04798
Epoch: 0171 train_loss= 12.15927 accuracy= 0.67346 time= 0.04840
Epoch: 0181 train_loss= 12.08656 accuracy= 0.67634 time= 0.04827
Epoch: 0191 train_loss= 12.06439 accuracy= 0.67765 time= 0.04835
Epoch: 0201 train_loss= 11.98691 accuracy= 0.67895 time= 0.04777
Epoch: 0211 train_loss= 11.94045 accuracy= 0.67712 time= 0.04819
Epoch: 0221 train_loss= 11.85131 accuracy= 0.68105 time= 0.04845
Epoch: 0231 train_loss= 11.82712 accuracy= 0.68235 time= 0.04847
Epoch: 0241 train_loss= 11.77266 accuracy= 0.68601 time= 0.04832
Epoch: 0251 train_loss= 11.73207 accuracy= 0.68523 time= 0.04773
Epoch: 0261 train_loss= 11.69660 accuracy= 0.68601 time= 0.04797
Epoch: 0271 train_loss= 11.65173 accuracy= 0.68680 time= 0.04850
Epoch: 0281 train_loss= 11.60319 accuracy= 0.69203 time= 0.04829
Epoch: 0291 train_loss= 11.56902 accuracy= 0.69072 time= 0.04776

accuracy 0.69386
auc 0.58782
f1_score 0.34944
Job finished!



Initializing normal onedecoder
Namespace(alpha=1, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', decoder=1, device=device(type='cuda'), dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 66
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
Epoch: 0001 train_loss= 5017.94189 accuracy= 0.61072 time= 0.05666
Epoch: 0011 train_loss= 1045.36377 accuracy= 0.63190 time= 0.04438
Epoch: 0021 train_loss= 618.09595 accuracy= 0.60183 time= 0.04428
Epoch: 0031 train_loss= 505.81549 accuracy= 0.59739 time= 0.04408
Epoch: 0041 train_loss= 407.64252 accuracy= 0.63686 time= 0.04413
Epoch: 0051 train_loss= 379.36386 accuracy= 0.61935 time= 0.04414
Epoch: 0061 train_loss= 366.91434 accuracy= 0.63503 time= 0.04428
Epoch: 0071 train_loss= 351.54410 accuracy= 0.62876 time= 0.04407
Epoch: 0081 train_loss= 339.25482 accuracy= 0.63817 time= 0.04417
Epoch: 0091 train_loss= 331.96072 accuracy= 0.63582 time= 0.04396
Epoch: 0101 train_loss= 323.93771 accuracy= 0.64026 time= 0.04423
Epoch: 0111 train_loss= 315.47162 accuracy= 0.63346 time= 0.04493
Epoch: 0121 train_loss= 308.97134 accuracy= 0.62745 time= 0.04500
Epoch: 0131 train_loss= 304.77548 accuracy= 0.63320 time= 0.04447
Epoch: 0141 train_loss= 297.75058 accuracy= 0.63268 time= 0.04416
Epoch: 0151 train_loss= 290.69348 accuracy= 0.63085 time= 0.04402
Epoch: 0161 train_loss= 283.91940 accuracy= 0.63529 time= 0.04393
Epoch: 0171 train_loss= 276.22595 accuracy= 0.63608 time= 0.04406
Epoch: 0181 train_loss= 273.88324 accuracy= 0.63425 time= 0.04415
Epoch: 0191 train_loss= 265.44901 accuracy= 0.63922 time= 0.04481
Epoch: 0201 train_loss= 257.12360 accuracy= 0.63634 time= 0.04475
Epoch: 0211 train_loss= 251.25671 accuracy= 0.63216 time= 0.04506
Epoch: 0221 train_loss= 243.60237 accuracy= 0.62928 time= 0.04416
Epoch: 0231 train_loss= 234.18155 accuracy= 0.63425 time= 0.04410
Epoch: 0241 train_loss= 232.66383 accuracy= 0.63477 time= 0.04429
Epoch: 0251 train_loss= 223.53995 accuracy= 0.63033 time= 0.04384
Epoch: 0261 train_loss= 218.23778 accuracy= 0.62693 time= 0.04394
Epoch: 0271 train_loss= 212.08447 accuracy= 0.63712 time= 0.04432
Epoch: 0281 train_loss= 206.45862 accuracy= 0.62771 time= 0.04848
Epoch: 0291 train_loss= 202.32062 accuracy= 0.62954 time= 0.04439

accuracy 0.62275
auc 0.45816
f1_score 0.19833
Job finished!



Initializing normal twodecoders
Namespace(alpha=1, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', device=device(type='cuda'), dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 320
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
Epoch: 0001 train_loss= 5365.07910 feature_loss= 24.53651 structure_loss= 5340.54248 accuracy= 0.61150 accuracy_s= 0.77725 accuracy_f= 0.81229 time= 0.09555
Epoch: 0011 train_loss= 1137.23438 feature_loss= 16.91896 structure_loss= 1120.31543 accuracy= 0.62850 accuracy_s= 0.78013 accuracy_f= 0.83007 time= 0.07746
Epoch: 0021 train_loss= 653.74036 feature_loss= 15.78728 structure_loss= 637.95306 accuracy= 0.60418 accuracy_s= 0.76889 accuracy_f= 0.83216 time= 0.07827
Epoch: 0031 train_loss= 534.68481 feature_loss= 15.00493 structure_loss= 519.67987 accuracy= 0.60078 accuracy_s= 0.76706 accuracy_f= 0.83268 time= 0.07760
Epoch: 0041 train_loss= 427.38736 feature_loss= 14.44666 structure_loss= 412.94070 accuracy= 0.63268 accuracy_s= 0.76993 accuracy_f= 0.83503 time= 0.07803
Epoch: 0051 train_loss= 398.06351 feature_loss= 14.06476 structure_loss= 383.99875 accuracy= 0.64863 accuracy_s= 0.76810 accuracy_f= 0.83686 time= 0.07762
Epoch: 0061 train_loss= 379.36316 feature_loss= 13.79720 structure_loss= 365.56595 accuracy= 0.65752 accuracy_s= 0.76575 accuracy_f= 0.83765 time= 0.07700
Epoch: 0071 train_loss= 368.58826 feature_loss= 13.56918 structure_loss= 355.01907 accuracy= 0.66013 accuracy_s= 0.77098 accuracy_f= 0.83739 time= 0.07748
Epoch: 0081 train_loss= 359.13312 feature_loss= 13.36888 structure_loss= 345.76425 accuracy= 0.66092 accuracy_s= 0.77935 accuracy_f= 0.83739 time= 0.07786
Epoch: 0091 train_loss= 347.44272 feature_loss= 13.18754 structure_loss= 334.25519 accuracy= 0.66170 accuracy_s= 0.78510 accuracy_f= 0.83895 time= 0.07763
Epoch: 0101 train_loss= 342.41983 feature_loss= 13.02335 structure_loss= 329.39648 accuracy= 0.66327 accuracy_s= 0.78719 accuracy_f= 0.83922 time= 0.07718
Epoch: 0111 train_loss= 334.43494 feature_loss= 12.90926 structure_loss= 321.52570 accuracy= 0.66144 accuracy_s= 0.78327 accuracy_f= 0.84052 time= 0.07741
Epoch: 0121 train_loss= 326.52036 feature_loss= 12.75489 structure_loss= 313.76547 accuracy= 0.66222 accuracy_s= 0.78536 accuracy_f= 0.84078 time= 0.07808
Epoch: 0131 train_loss= 321.05225 feature_loss= 12.64271 structure_loss= 308.40955 accuracy= 0.66248 accuracy_s= 0.78536 accuracy_f= 0.84078 time= 0.07750
Epoch: 0141 train_loss= 310.81436 feature_loss= 12.54605 structure_loss= 298.26831 accuracy= 0.66405 accuracy_s= 0.78275 accuracy_f= 0.84105 time= 0.07791
Epoch: 0151 train_loss= 307.07190 feature_loss= 12.45288 structure_loss= 294.61902 accuracy= 0.66301 accuracy_s= 0.78667 accuracy_f= 0.84314 time= 0.07801
Epoch: 0161 train_loss= 299.14142 feature_loss= 12.36815 structure_loss= 286.77328 accuracy= 0.66484 accuracy_s= 0.78379 accuracy_f= 0.84314 time= 0.07760
Epoch: 0171 train_loss= 296.45465 feature_loss= 12.29633 structure_loss= 284.15833 accuracy= 0.66353 accuracy_s= 0.78301 accuracy_f= 0.84471 time= 0.07797
Epoch: 0181 train_loss= 288.56693 feature_loss= 12.22255 structure_loss= 276.34436 accuracy= 0.66484 accuracy_s= 0.78902 accuracy_f= 0.84418 time= 0.07728
Epoch: 0191 train_loss= 279.72861 feature_loss= 12.16724 structure_loss= 267.56137 accuracy= 0.66510 accuracy_s= 0.78667 accuracy_f= 0.84471 time= 0.07747
Epoch: 0201 train_loss= 279.29886 feature_loss= 12.12358 structure_loss= 267.17529 accuracy= 0.66379 accuracy_s= 0.78824 accuracy_f= 0.84758 time= 0.07842
Epoch: 0211 train_loss= 272.51410 feature_loss= 12.09665 structure_loss= 260.41745 accuracy= 0.66275 accuracy_s= 0.78301 accuracy_f= 0.84732 time= 0.07770
Epoch: 0221 train_loss= 265.93588 feature_loss= 12.04975 structure_loss= 253.88614 accuracy= 0.66562 accuracy_s= 0.78275 accuracy_f= 0.84732 time= 0.07778
Epoch: 0231 train_loss= 262.71158 feature_loss= 11.99015 structure_loss= 250.72142 accuracy= 0.66745 accuracy_s= 0.78379 accuracy_f= 0.84784 time= 0.07777
Epoch: 0241 train_loss= 256.22894 feature_loss= 11.95116 structure_loss= 244.27780 accuracy= 0.66928 accuracy_s= 0.78196 accuracy_f= 0.84758 time= 0.07735
Epoch: 0251 train_loss= 245.72935 feature_loss= 11.90412 structure_loss= 233.82523 accuracy= 0.66771 accuracy_s= 0.78379 accuracy_f= 0.84758 time= 0.07816
Epoch: 0261 train_loss= 243.33447 feature_loss= 11.87608 structure_loss= 231.45839 accuracy= 0.66745 accuracy_s= 0.78353 accuracy_f= 0.84889 time= 0.07737
Epoch: 0271 train_loss= 239.86194 feature_loss= 11.85342 structure_loss= 228.00851 accuracy= 0.66693 accuracy_s= 0.78327 accuracy_f= 0.84810 time= 0.07759
Epoch: 0281 train_loss= 234.76816 feature_loss= 11.82349 structure_loss= 222.94467 accuracy= 0.66902 accuracy_s= 0.78275 accuracy_f= 0.84837 time= 0.07772
Epoch: 0291 train_loss= 228.73283 feature_loss= 11.80932 structure_loss= 216.92352 accuracy= 0.66954 accuracy_s= 0.78719 accuracy_f= 0.84941 time= 0.07784

accuracy 0.67137
accuracy_s 0.78039
accuracy_f 0.84863
auc 0.57785
f1_score 0.30167
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', decoder=1, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 585
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
using wavelet scattering transform
y_features shape after scatting (7650, 9685) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([7650, 186]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 10789.54688 accuracy= 0.60732 time= 0.04304
Epoch: 0011 train_loss= 6939.76660 accuracy= 0.61203 time= 0.03076
Epoch: 0021 train_loss= 5045.06543 accuracy= 0.61778 time= 0.03059
Epoch: 0031 train_loss= 3937.23584 accuracy= 0.61961 time= 0.03143
Epoch: 0041 train_loss= 3183.52124 accuracy= 0.62379 time= 0.03081
Epoch: 0051 train_loss= 2647.13672 accuracy= 0.62771 time= 0.03068
Epoch: 0061 train_loss= 2273.35522 accuracy= 0.63686 time= 0.03317
Epoch: 0071 train_loss= 2013.85303 accuracy= 0.64863 time= 0.03081
Epoch: 0081 train_loss= 1804.49646 accuracy= 0.65046 time= 0.03124
Epoch: 0091 train_loss= 1643.67163 accuracy= 0.64941 time= 0.03043
Epoch: 0101 train_loss= 1505.31812 accuracy= 0.64680 time= 0.03139
Epoch: 0111 train_loss= 1372.09302 accuracy= 0.65333 time= 0.03109
Epoch: 0121 train_loss= 1254.66296 accuracy= 0.65307 time= 0.03074
Epoch: 0131 train_loss= 1146.34399 accuracy= 0.65359 time= 0.03099
Epoch: 0141 train_loss= 1052.03381 accuracy= 0.65020 time= 0.03100
Epoch: 0151 train_loss= 987.78668 accuracy= 0.65020 time= 0.03077
Epoch: 0161 train_loss= 910.16113 accuracy= 0.65098 time= 0.03070
Epoch: 0171 train_loss= 849.59082 accuracy= 0.65098 time= 0.03149
Epoch: 0181 train_loss= 804.09998 accuracy= 0.65072 time= 0.03061
Epoch: 0191 train_loss= 765.80823 accuracy= 0.64967 time= 0.03065
Epoch: 0201 train_loss= 725.79376 accuracy= 0.64837 time= 0.03086
Epoch: 0211 train_loss= 695.12366 accuracy= 0.64680 time= 0.03094
Epoch: 0221 train_loss= 661.27740 accuracy= 0.64758 time= 0.03072
Epoch: 0231 train_loss= 637.32574 accuracy= 0.64784 time= 0.03082
Epoch: 0241 train_loss= 607.01160 accuracy= 0.64471 time= 0.03143
Epoch: 0251 train_loss= 585.67328 accuracy= 0.64471 time= 0.03078
Epoch: 0261 train_loss= 562.25281 accuracy= 0.64314 time= 0.03066
Epoch: 0271 train_loss= 539.78558 accuracy= 0.64523 time= 0.03121
Epoch: 0281 train_loss= 519.13727 accuracy= 0.64471 time= 0.03095
Epoch: 0291 train_loss= 505.35059 accuracy= 0.64209 time= 0.03068

accuracy 0.64235
auc 0.49146
f1_score 0.24000
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', decoder=0, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 800
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
using wavelet scattering transform
y_features shape after scatting (7650, 9685) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([7650, 186]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 29.01915 accuracy= 0.62144 time= 0.04698
Epoch: 0011 train_loss= 22.02495 accuracy= 0.66065 time= 0.03460
Epoch: 0021 train_loss= 19.62391 accuracy= 0.66275 time= 0.03520
Epoch: 0031 train_loss= 17.55149 accuracy= 0.66641 time= 0.03525
Epoch: 0041 train_loss= 16.08390 accuracy= 0.67033 time= 0.03498
Epoch: 0051 train_loss= 14.95950 accuracy= 0.66954 time= 0.03453
Epoch: 0061 train_loss= 14.21076 accuracy= 0.67085 time= 0.03418
Epoch: 0071 train_loss= 13.80561 accuracy= 0.67242 time= 0.03478
Epoch: 0081 train_loss= 13.52187 accuracy= 0.67111 time= 0.03513
Epoch: 0091 train_loss= 13.29093 accuracy= 0.67294 time= 0.03474
Epoch: 0101 train_loss= 13.09279 accuracy= 0.67346 time= 0.03468
Epoch: 0111 train_loss= 12.92751 accuracy= 0.67346 time= 0.03472
Epoch: 0121 train_loss= 12.78753 accuracy= 0.67634 time= 0.03482
Epoch: 0131 train_loss= 12.67371 accuracy= 0.67582 time= 0.03379
Epoch: 0141 train_loss= 12.58229 accuracy= 0.67712 time= 0.03391
Epoch: 0151 train_loss= 12.45607 accuracy= 0.67712 time= 0.03626
Epoch: 0161 train_loss= 12.37281 accuracy= 0.67895 time= 0.03395
Epoch: 0171 train_loss= 12.29471 accuracy= 0.68183 time= 0.03387
Epoch: 0181 train_loss= 12.22522 accuracy= 0.68261 time= 0.03393
Epoch: 0191 train_loss= 12.16764 accuracy= 0.68340 time= 0.03403
Epoch: 0201 train_loss= 12.10589 accuracy= 0.68078 time= 0.03415
Epoch: 0211 train_loss= 12.07168 accuracy= 0.68314 time= 0.03440
Epoch: 0221 train_loss= 12.01916 accuracy= 0.68288 time= 0.03422
Epoch: 0231 train_loss= 11.97787 accuracy= 0.68444 time= 0.03447
Epoch: 0241 train_loss= 11.95314 accuracy= 0.68157 time= 0.03415
Epoch: 0251 train_loss= 11.91574 accuracy= 0.68366 time= 0.03410
Epoch: 0261 train_loss= 11.87547 accuracy= 0.68549 time= 0.03417
Epoch: 0271 train_loss= 11.87008 accuracy= 0.68418 time= 0.03422
Epoch: 0281 train_loss= 11.84702 accuracy= 0.68444 time= 0.03399
Epoch: 0291 train_loss= 11.81129 accuracy= 0.68706 time= 0.03400

accuracy 0.68680
auc 0.57916
f1_score 0.33444
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', decoder=1, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 964
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
using wavelet scattering transform
y_features shape after scatting (7650, 9685) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 58 steps!

y_features shape after FMS torch.Size([7650, 186]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 10395.05371 accuracy= 0.60967 time= 0.04181
Epoch: 0011 train_loss= 5772.87402 accuracy= 0.61386 time= 0.03080
Epoch: 0021 train_loss= 4137.72119 accuracy= 0.62118 time= 0.03071
Epoch: 0031 train_loss= 3182.52222 accuracy= 0.62980 time= 0.03065
Epoch: 0041 train_loss= 2562.35205 accuracy= 0.63425 time= 0.03067
Epoch: 0051 train_loss= 2106.30347 accuracy= 0.64261 time= 0.03087
Epoch: 0061 train_loss= 1822.62463 accuracy= 0.64967 time= 0.03073
Epoch: 0071 train_loss= 1616.65662 accuracy= 0.65150 time= 0.03076
Epoch: 0081 train_loss= 1451.24072 accuracy= 0.65673 time= 0.03055
Epoch: 0091 train_loss= 1328.51892 accuracy= 0.65987 time= 0.03067
Epoch: 0101 train_loss= 1219.06042 accuracy= 0.66196 time= 0.03052
Epoch: 0111 train_loss= 1119.53967 accuracy= 0.65856 time= 0.03029
Epoch: 0121 train_loss= 1051.24927 accuracy= 0.65386 time= 0.03068
Epoch: 0131 train_loss= 984.82318 accuracy= 0.65490 time= 0.03050
Epoch: 0141 train_loss= 936.52014 accuracy= 0.65464 time= 0.03047
Epoch: 0151 train_loss= 885.36707 accuracy= 0.65490 time= 0.03060
Epoch: 0161 train_loss= 840.14050 accuracy= 0.65229 time= 0.03053
Epoch: 0171 train_loss= 801.34503 accuracy= 0.65490 time= 0.03063
Epoch: 0181 train_loss= 765.17065 accuracy= 0.65569 time= 0.03065
Epoch: 0191 train_loss= 731.83929 accuracy= 0.65359 time= 0.03064
Epoch: 0201 train_loss= 697.96484 accuracy= 0.65333 time= 0.03057
Epoch: 0211 train_loss= 672.59387 accuracy= 0.64915 time= 0.03070
Epoch: 0221 train_loss= 639.55481 accuracy= 0.64732 time= 0.03081
Epoch: 0231 train_loss= 608.49347 accuracy= 0.64915 time= 0.03064
Epoch: 0241 train_loss= 588.19727 accuracy= 0.64340 time= 0.03076
Epoch: 0251 train_loss= 567.92749 accuracy= 0.64235 time= 0.03055
Epoch: 0261 train_loss= 535.76294 accuracy= 0.64340 time= 0.03069
Epoch: 0271 train_loss= 522.29315 accuracy= 0.63974 time= 0.03075
Epoch: 0281 train_loss= 497.79959 accuracy= 0.63739 time= 0.03059
Epoch: 0291 train_loss= 489.73877 accuracy= 0.63686 time= 0.03057

accuracy 0.63765
auc 0.50326
f1_score 0.23000
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', decoder=0, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 276
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
using wavelet scattering transform
y_features shape after scatting (7650, 9685) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 58 steps!

y_features shape after FMS torch.Size([7650, 186]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 28.96248 accuracy= 0.62379 time= 0.05453
Epoch: 0011 train_loss= 21.90681 accuracy= 0.65699 time= 0.03351
Epoch: 0021 train_loss= 19.24197 accuracy= 0.66458 time= 0.03421
Epoch: 0031 train_loss= 16.89865 accuracy= 0.66902 time= 0.03428
Epoch: 0041 train_loss= 15.45785 accuracy= 0.66928 time= 0.03365
Epoch: 0051 train_loss= 14.65745 accuracy= 0.67111 time= 0.03329
Epoch: 0061 train_loss= 14.15318 accuracy= 0.67320 time= 0.03424
Epoch: 0071 train_loss= 13.89279 accuracy= 0.66850 time= 0.03386
Epoch: 0081 train_loss= 13.46241 accuracy= 0.67320 time= 0.03399
Epoch: 0091 train_loss= 13.19142 accuracy= 0.67216 time= 0.03440
Epoch: 0101 train_loss= 12.99525 accuracy= 0.67373 time= 0.03372
Epoch: 0111 train_loss= 12.83320 accuracy= 0.67529 time= 0.03330
Epoch: 0121 train_loss= 12.86156 accuracy= 0.67686 time= 0.03453
Epoch: 0131 train_loss= 12.67815 accuracy= 0.66980 time= 0.03368
Epoch: 0141 train_loss= 12.52991 accuracy= 0.67608 time= 0.03328
Epoch: 0151 train_loss= 12.41848 accuracy= 0.67739 time= 0.03400
Epoch: 0161 train_loss= 12.32121 accuracy= 0.67739 time= 0.03373
Epoch: 0171 train_loss= 12.24741 accuracy= 0.67895 time= 0.03333
Epoch: 0181 train_loss= 12.21469 accuracy= 0.68026 time= 0.03423
Epoch: 0191 train_loss= 12.13494 accuracy= 0.68052 time= 0.03373
Epoch: 0201 train_loss= 12.07960 accuracy= 0.68288 time= 0.03334
Epoch: 0211 train_loss= 12.03827 accuracy= 0.68183 time= 0.03416
Epoch: 0221 train_loss= 12.00540 accuracy= 0.68105 time= 0.03375
Epoch: 0231 train_loss= 11.95626 accuracy= 0.68627 time= 0.03348
Epoch: 0241 train_loss= 11.92922 accuracy= 0.68444 time= 0.03417
Epoch: 0251 train_loss= 11.90039 accuracy= 0.68314 time= 0.03428
Epoch: 0261 train_loss= 11.89097 accuracy= 0.68418 time= 0.03377
Epoch: 0271 train_loss= 11.84333 accuracy= 0.68549 time= 0.03361
Epoch: 0281 train_loss= 11.82088 accuracy= 0.68601 time= 0.03417
Epoch: 0291 train_loss= 11.80385 accuracy= 0.68654 time= 0.03392

accuracy 0.68471
auc 0.57936
f1_score 0.33000
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', decoder=1, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 566
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
using wavelet scattering transform
y_features shape after scatting (7650, 9685) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([7650, 186]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 4136.42773 accuracy= 0.61490 time= 0.06988
Epoch: 0011 train_loss= 107.75541 accuracy= 0.61752 time= 0.05234
Epoch: 0021 train_loss= 63.14216 accuracy= 0.64418 time= 0.05353
Epoch: 0031 train_loss= 35.19025 accuracy= 0.63608 time= 0.05212
Epoch: 0041 train_loss= 19.23570 accuracy= 0.65046 time= 0.05250
Epoch: 0051 train_loss= 9.47322 accuracy= 0.66458 time= 0.05216
Epoch: 0061 train_loss= 4.57195 accuracy= 0.63085 time= 0.05228
Epoch: 0071 train_loss= 2.47059 accuracy= 0.61176 time= 0.05211
Epoch: 0081 train_loss= 1.55348 accuracy= 0.59869 time= 0.05192
Epoch: 0091 train_loss= 1.06498 accuracy= 0.59477 time= 0.05230
Epoch: 0101 train_loss= 0.82567 accuracy= 0.59346 time= 0.05230
Epoch: 0111 train_loss= 0.66990 accuracy= 0.59294 time= 0.05174
Epoch: 0121 train_loss= 0.56480 accuracy= 0.59294 time= 0.05199
Epoch: 0131 train_loss= 0.48303 accuracy= 0.59242 time= 0.05213
Epoch: 0141 train_loss= 0.43549 accuracy= 0.59216 time= 0.05197
Epoch: 0151 train_loss= 0.39227 accuracy= 0.59242 time= 0.05214
Epoch: 0161 train_loss= 0.36499 accuracy= 0.59190 time= 0.05192
Epoch: 0171 train_loss= 0.34014 accuracy= 0.59163 time= 0.05206
Epoch: 0181 train_loss= 0.32101 accuracy= 0.59163 time= 0.05214
Epoch: 0191 train_loss= 0.30451 accuracy= 0.59242 time= 0.05210
Epoch: 0201 train_loss= 0.29249 accuracy= 0.59111 time= 0.05193
Epoch: 0211 train_loss= 0.27999 accuracy= 0.59111 time= 0.05215
Epoch: 0221 train_loss= 0.27032 accuracy= 0.59111 time= 0.05193
Epoch: 0231 train_loss= 0.26412 accuracy= 0.59085 time= 0.05208
Epoch: 0241 train_loss= 0.25613 accuracy= 0.59085 time= 0.05220
Epoch: 0251 train_loss= 0.25121 accuracy= 0.59085 time= 0.05187
Epoch: 0261 train_loss= 0.24410 accuracy= 0.59085 time= 0.05197
Epoch: 0271 train_loss= 0.24156 accuracy= 0.59111 time= 0.05218
Epoch: 0281 train_loss= 0.23839 accuracy= 0.59111 time= 0.05228
Epoch: 0291 train_loss= 0.23359 accuracy= 0.59111 time= 0.05214

accuracy 0.59085
auc 0.41746
f1_score 0.13056
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', decoder=0, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 640
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
using wavelet scattering transform
y_features shape after scatting (7650, 9685) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([7650, 186]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 29.28820 accuracy= 0.62144 time= 0.07439
Epoch: 0011 train_loss= 17.71955 accuracy= 0.65935 time= 0.05498
Epoch: 0021 train_loss= 16.13671 accuracy= 0.66458 time= 0.05493
Epoch: 0031 train_loss= 15.71082 accuracy= 0.66431 time= 0.05493
Epoch: 0041 train_loss= 15.35547 accuracy= 0.66562 time= 0.05491
Epoch: 0051 train_loss= 15.06432 accuracy= 0.66536 time= 0.05496
Epoch: 0061 train_loss= 14.79297 accuracy= 0.66641 time= 0.05701
Epoch: 0071 train_loss= 14.48986 accuracy= 0.66693 time= 0.05836
Epoch: 0081 train_loss= 14.13888 accuracy= 0.66980 time= 0.05527
Epoch: 0091 train_loss= 13.58054 accuracy= 0.66562 time= 0.05504
Epoch: 0101 train_loss= 13.35822 accuracy= 0.66510 time= 0.05577
Epoch: 0111 train_loss= 13.16015 accuracy= 0.66693 time= 0.05527
Epoch: 0121 train_loss= 17.51722 accuracy= 0.66248 time= 0.05550
Epoch: 0131 train_loss= 12.89347 accuracy= 0.66745 time= 0.05531
Epoch: 0141 train_loss= 12.74323 accuracy= 0.67085 time= 0.05536
Epoch: 0151 train_loss= 12.61969 accuracy= 0.67425 time= 0.05554
Epoch: 0161 train_loss= 12.51643 accuracy= 0.67556 time= 0.05558
Epoch: 0171 train_loss= 12.54177 accuracy= 0.67346 time= 0.05468
Epoch: 0181 train_loss= 12.44483 accuracy= 0.67268 time= 0.05522
Epoch: 0191 train_loss= 12.35394 accuracy= 0.67660 time= 0.05664
Epoch: 0201 train_loss= 12.27975 accuracy= 0.67765 time= 0.05522
Epoch: 0211 train_loss= 12.20664 accuracy= 0.67974 time= 0.05530
Epoch: 0221 train_loss= 12.14112 accuracy= 0.68026 time= 0.05524
Epoch: 0231 train_loss= 12.32820 accuracy= 0.67582 time= 0.05517
Epoch: 0241 train_loss= 12.17366 accuracy= 0.67425 time= 0.05534
Epoch: 0251 train_loss= 12.08639 accuracy= 0.67895 time= 0.05523
Epoch: 0261 train_loss= 12.01279 accuracy= 0.68000 time= 0.05539
Epoch: 0271 train_loss= 11.95539 accuracy= 0.68078 time= 0.05551
Epoch: 0281 train_loss= 11.90601 accuracy= 0.68026 time= 0.05524
Epoch: 0291 train_loss= 11.86348 accuracy= 0.68288 time= 0.05536

accuracy 0.68340
auc 0.58007
f1_score 0.32722
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', decoder=1, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 724
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
using wavelet scattering transform
y_features shape after scatting (7650, 9685) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 90 steps!

y_features shape after FMS torch.Size([7650, 186]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 62.30907 accuracy= 0.62797 time= 0.07128
Epoch: 0011 train_loss= 1.51752 accuracy= 0.60078 time= 0.05179
Epoch: 0021 train_loss= 0.86917 accuracy= 0.59503 time= 0.05141
Epoch: 0031 train_loss= 0.56696 accuracy= 0.59503 time= 0.05160
Epoch: 0041 train_loss= 0.42351 accuracy= 0.59294 time= 0.05144
Epoch: 0051 train_loss= 0.34890 accuracy= 0.59137 time= 0.05161
Epoch: 0061 train_loss= 0.30612 accuracy= 0.59111 time= 0.05165
Epoch: 0071 train_loss= 0.28156 accuracy= 0.59033 time= 0.05138
Epoch: 0081 train_loss= 0.26352 accuracy= 0.59085 time= 0.05148
Epoch: 0091 train_loss= 0.25026 accuracy= 0.59059 time= 0.05151
Epoch: 0101 train_loss= 0.24037 accuracy= 0.59111 time= 0.05170
Epoch: 0111 train_loss= 0.23286 accuracy= 0.59111 time= 0.05166
Epoch: 0121 train_loss= 0.22707 accuracy= 0.59085 time= 0.05130
Epoch: 0131 train_loss= 0.22210 accuracy= 0.59111 time= 0.05149
Epoch: 0141 train_loss= 0.21903 accuracy= 0.59111 time= 0.05139
Epoch: 0151 train_loss= 0.21537 accuracy= 0.59111 time= 0.05158
Epoch: 0161 train_loss= 0.21424 accuracy= 0.59085 time= 0.05156
Epoch: 0171 train_loss= 0.21261 accuracy= 0.59085 time= 0.05147
Epoch: 0181 train_loss= 0.21056 accuracy= 0.59059 time= 0.05155
Epoch: 0191 train_loss= 0.20998 accuracy= 0.59085 time= 0.05151
Epoch: 0201 train_loss= 0.20893 accuracy= 0.59111 time= 0.05185
Epoch: 0211 train_loss= 0.20807 accuracy= 0.59085 time= 0.05153
Epoch: 0221 train_loss= 0.20714 accuracy= 0.59059 time= 0.05193
Epoch: 0231 train_loss= 0.20647 accuracy= 0.59059 time= 0.05169
Epoch: 0241 train_loss= 0.20587 accuracy= 0.59033 time= 0.05186
Epoch: 0251 train_loss= 0.20505 accuracy= 0.59059 time= 0.05164
Epoch: 0261 train_loss= 0.20462 accuracy= 0.59033 time= 0.05179
Epoch: 0271 train_loss= 0.20447 accuracy= 0.59059 time= 0.05173
Epoch: 0281 train_loss= 0.20415 accuracy= 0.59033 time= 0.05198
Epoch: 0291 train_loss= 0.20399 accuracy= 0.59033 time= 0.05175

accuracy 0.59033
auc 0.42352
f1_score 0.12944
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', decoder=0, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 11
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
using wavelet scattering transform
y_features shape after scatting (7650, 9685) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 58 steps!

y_features shape after FMS torch.Size([7650, 186]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 29.24900 accuracy= 0.62196 time= 0.07530
Epoch: 0011 train_loss= 19.46027 accuracy= 0.65281 time= 0.05462
Epoch: 0021 train_loss= 14.69281 accuracy= 0.66301 time= 0.05416
Epoch: 0031 train_loss= 14.17315 accuracy= 0.66431 time= 0.05436
Epoch: 0041 train_loss= 13.94771 accuracy= 0.66431 time= 0.05443
Epoch: 0051 train_loss= 13.75076 accuracy= 0.66431 time= 0.05415
Epoch: 0061 train_loss= 13.57833 accuracy= 0.66431 time= 0.05582
Epoch: 0071 train_loss= 13.42804 accuracy= 0.66431 time= 0.05434
Epoch: 0081 train_loss= 13.29744 accuracy= 0.66431 time= 0.05435
Epoch: 0091 train_loss= 13.18414 accuracy= 0.66405 time= 0.05446
Epoch: 0101 train_loss= 13.08595 accuracy= 0.66405 time= 0.05437
Epoch: 0111 train_loss= 13.00088 accuracy= 0.66379 time= 0.05436
Epoch: 0121 train_loss= 12.92723 accuracy= 0.66379 time= 0.05463
Epoch: 0131 train_loss= 12.86347 accuracy= 0.66379 time= 0.05442
Epoch: 0141 train_loss= 12.80830 accuracy= 0.66379 time= 0.05407
Epoch: 0151 train_loss= 12.76058 accuracy= 0.66353 time= 0.05451
Epoch: 0161 train_loss= 12.71934 accuracy= 0.66379 time= 0.05457
Epoch: 0171 train_loss= 12.68373 accuracy= 0.66379 time= 0.05449
Epoch: 0181 train_loss= 12.65301 accuracy= 0.66353 time= 0.05466
Epoch: 0191 train_loss= 12.62653 accuracy= 0.66379 time= 0.05458
Epoch: 0201 train_loss= 12.60374 accuracy= 0.66405 time= 0.05435
Epoch: 0211 train_loss= 12.58414 accuracy= 0.66405 time= 0.05437
Epoch: 0221 train_loss= 12.56732 accuracy= 0.66405 time= 0.05453
Epoch: 0231 train_loss= 12.55290 accuracy= 0.66405 time= 0.05465
Epoch: 0241 train_loss= 12.54055 accuracy= 0.66431 time= 0.05461
Epoch: 0251 train_loss= 12.53000 accuracy= 0.66431 time= 0.05465
Epoch: 0261 train_loss= 12.52100 accuracy= 0.66431 time= 0.05456
Epoch: 0271 train_loss= 12.51333 accuracy= 0.66405 time= 0.05455
Epoch: 0281 train_loss= 12.50681 accuracy= 0.66405 time= 0.05481
Epoch: 0291 train_loss= 12.50128 accuracy= 0.66405 time= 0.05478

accuracy 0.66405
auc 0.50238
f1_score 0.28611
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', decoder=1, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 174
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
using wavelet scattering transform
y_features shape after scatting (7650, 9685) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([7650, 186]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 2405.86499 accuracy= 0.62092 time= 0.04331
Epoch: 0011 train_loss= 69.95794 accuracy= 0.63451 time= 0.03147
Epoch: 0021 train_loss= 40.19129 accuracy= 0.64497 time= 0.03134
Epoch: 0031 train_loss= 22.82531 accuracy= 0.65020 time= 0.03136
Epoch: 0041 train_loss= 13.22784 accuracy= 0.65856 time= 0.03218
Epoch: 0051 train_loss= 7.28637 accuracy= 0.63085 time= 0.03148
Epoch: 0061 train_loss= 4.21470 accuracy= 0.62614 time= 0.03166
Epoch: 0071 train_loss= 2.58147 accuracy= 0.60863 time= 0.03282
Epoch: 0081 train_loss= 1.65543 accuracy= 0.60105 time= 0.03126
Epoch: 0091 train_loss= 1.19922 accuracy= 0.59582 time= 0.03177
Epoch: 0101 train_loss= 0.86034 accuracy= 0.59346 time= 0.03136
Epoch: 0111 train_loss= 0.67007 accuracy= 0.59346 time= 0.03176
Epoch: 0121 train_loss= 0.56577 accuracy= 0.59320 time= 0.03148
Epoch: 0131 train_loss= 0.47923 accuracy= 0.59163 time= 0.03174
Epoch: 0141 train_loss= 0.42637 accuracy= 0.59163 time= 0.03138
Epoch: 0151 train_loss= 0.37585 accuracy= 0.59137 time= 0.03124
Epoch: 0161 train_loss= 0.33761 accuracy= 0.59163 time= 0.03161
Epoch: 0171 train_loss= 0.31451 accuracy= 0.59137 time= 0.03121
Epoch: 0181 train_loss= 0.29825 accuracy= 0.59137 time= 0.03191
Epoch: 0191 train_loss= 0.28189 accuracy= 0.59111 time= 0.03132
Epoch: 0201 train_loss= 0.26770 accuracy= 0.59085 time= 0.03162
Epoch: 0211 train_loss= 0.26151 accuracy= 0.59085 time= 0.03149
Epoch: 0221 train_loss= 0.25151 accuracy= 0.59111 time= 0.03145
Epoch: 0231 train_loss= 0.24835 accuracy= 0.59111 time= 0.03161
Epoch: 0241 train_loss= 0.23715 accuracy= 0.59085 time= 0.03127
Epoch: 0251 train_loss= 0.23288 accuracy= 0.59085 time= 0.03194
Epoch: 0261 train_loss= 0.22863 accuracy= 0.59085 time= 0.03129
Epoch: 0271 train_loss= 0.22781 accuracy= 0.59085 time= 0.03184
Epoch: 0281 train_loss= 0.22295 accuracy= 0.59059 time= 0.03128
Epoch: 0291 train_loss= 0.21981 accuracy= 0.59085 time= 0.03163

accuracy 0.59085
auc 0.41760
f1_score 0.13056
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', decoder=0, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 403
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
using wavelet scattering transform
y_features shape after scatting (7650, 9685) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([7650, 186]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 29.45266 accuracy= 0.61935 time= 0.04870
Epoch: 0011 train_loss= 21.10027 accuracy= 0.65882 time= 0.03472
Epoch: 0021 train_loss= 17.65347 accuracy= 0.66275 time= 0.03490
Epoch: 0031 train_loss= 16.34162 accuracy= 0.66379 time= 0.03535
Epoch: 0041 train_loss= 15.84020 accuracy= 0.66275 time= 0.03531
Epoch: 0051 train_loss= 14.95025 accuracy= 0.66510 time= 0.03482
Epoch: 0061 train_loss= 14.26312 accuracy= 0.66641 time= 0.03546
Epoch: 0071 train_loss= 13.98436 accuracy= 0.66667 time= 0.03505
Epoch: 0081 train_loss= 13.71926 accuracy= 0.66797 time= 0.03514
Epoch: 0091 train_loss= 13.46406 accuracy= 0.66954 time= 0.03564
Epoch: 0101 train_loss= 13.23247 accuracy= 0.66928 time= 0.03480
Epoch: 0111 train_loss= 13.03208 accuracy= 0.67320 time= 0.03573
Epoch: 0121 train_loss= 12.85311 accuracy= 0.67451 time= 0.03458
Epoch: 0131 train_loss= 12.67485 accuracy= 0.67686 time= 0.03527
Epoch: 0141 train_loss= 12.54298 accuracy= 0.67791 time= 0.03556
Epoch: 0151 train_loss= 12.44592 accuracy= 0.67869 time= 0.03716
Epoch: 0161 train_loss= 12.34087 accuracy= 0.67974 time= 0.03575
Epoch: 0171 train_loss= 12.23710 accuracy= 0.68078 time= 0.03494
Epoch: 0181 train_loss= 12.15524 accuracy= 0.68288 time= 0.03492
Epoch: 0191 train_loss= 12.11127 accuracy= 0.68288 time= 0.03578
Epoch: 0201 train_loss= 12.02365 accuracy= 0.68183 time= 0.03457
Epoch: 0211 train_loss= 11.93716 accuracy= 0.68157 time= 0.03528
Epoch: 0221 train_loss= 11.87123 accuracy= 0.68392 time= 0.03555
Epoch: 0231 train_loss= 11.86373 accuracy= 0.68366 time= 0.03546
Epoch: 0241 train_loss= 11.77490 accuracy= 0.68497 time= 0.03536
Epoch: 0251 train_loss= 11.71987 accuracy= 0.68444 time= 0.03516
Epoch: 0261 train_loss= 11.67926 accuracy= 0.68706 time= 0.03503
Epoch: 0271 train_loss= 11.64490 accuracy= 0.68941 time= 0.03582
Epoch: 0281 train_loss= 11.59491 accuracy= 0.68601 time= 0.03503
Epoch: 0291 train_loss= 11.59754 accuracy= 0.68732 time= 0.03585

accuracy 0.68810
auc 0.58649
f1_score 0.33722
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', decoder=1, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 438
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
using wavelet scattering transform
y_features shape after scatting (7650, 9685) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 89 steps!

y_features shape after FMS torch.Size([7650, 186]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 25.23149 accuracy= 0.62065 time= 0.05070
Epoch: 0011 train_loss= 1.91375 accuracy= 0.62405 time= 0.03198
Epoch: 0021 train_loss= 0.63136 accuracy= 0.60784 time= 0.03138
Epoch: 0031 train_loss= 0.42936 accuracy= 0.60444 time= 0.03215
Epoch: 0041 train_loss= 0.34850 accuracy= 0.59660 time= 0.03143
Epoch: 0051 train_loss= 0.30299 accuracy= 0.59503 time= 0.03174
Epoch: 0061 train_loss= 0.27472 accuracy= 0.59346 time= 0.03141
Epoch: 0071 train_loss= 0.25602 accuracy= 0.59294 time= 0.03152
Epoch: 0081 train_loss= 0.24392 accuracy= 0.59294 time= 0.03136
Epoch: 0091 train_loss= 0.23472 accuracy= 0.59216 time= 0.03153
Epoch: 0101 train_loss= 0.22827 accuracy= 0.59085 time= 0.03164
Epoch: 0111 train_loss= 0.22339 accuracy= 0.59111 time= 0.03134
Epoch: 0121 train_loss= 0.21942 accuracy= 0.59137 time= 0.03200
Epoch: 0131 train_loss= 0.21638 accuracy= 0.59111 time= 0.03134
Epoch: 0141 train_loss= 0.21391 accuracy= 0.59085 time= 0.03192
Epoch: 0151 train_loss= 0.21207 accuracy= 0.59085 time= 0.03130
Epoch: 0161 train_loss= 0.21050 accuracy= 0.59059 time= 0.03175
Epoch: 0171 train_loss= 0.20904 accuracy= 0.59085 time= 0.03125
Epoch: 0181 train_loss= 0.20802 accuracy= 0.59059 time= 0.03169
Epoch: 0191 train_loss= 0.20707 accuracy= 0.59033 time= 0.03152
Epoch: 0201 train_loss= 0.20623 accuracy= 0.59033 time= 0.03163
Epoch: 0211 train_loss= 0.20547 accuracy= 0.59033 time= 0.03182
Epoch: 0221 train_loss= 0.20472 accuracy= 0.59085 time= 0.03139
Epoch: 0231 train_loss= 0.20430 accuracy= 0.59059 time= 0.03170
Epoch: 0241 train_loss= 0.20392 accuracy= 0.59059 time= 0.03175
Epoch: 0251 train_loss= 0.20332 accuracy= 0.59033 time= 0.03156
Epoch: 0261 train_loss= 0.20283 accuracy= 0.59059 time= 0.03204
Epoch: 0271 train_loss= 0.20258 accuracy= 0.59033 time= 0.03158
Epoch: 0281 train_loss= 0.20208 accuracy= 0.59085 time= 0.03145
Epoch: 0291 train_loss= 0.20175 accuracy= 0.59059 time= 0.03225

accuracy 0.59059
auc 0.40452
f1_score 0.13000
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', decoder=0, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 132
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
using wavelet scattering transform
y_features shape after scatting (7650, 9685) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 58 steps!

y_features shape after FMS torch.Size([7650, 186]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 29.04452 accuracy= 0.61830 time= 0.05835
Epoch: 0011 train_loss= 17.36393 accuracy= 0.65621 time= 0.03427
Epoch: 0021 train_loss= 14.46380 accuracy= 0.66431 time= 0.03419
Epoch: 0031 train_loss= 14.16881 accuracy= 0.66431 time= 0.03459
Epoch: 0041 train_loss= 13.94254 accuracy= 0.66431 time= 0.03450
Epoch: 0051 train_loss= 13.74366 accuracy= 0.66431 time= 0.03449
Epoch: 0061 train_loss= 13.57024 accuracy= 0.66431 time= 0.03448
Epoch: 0071 train_loss= 13.41968 accuracy= 0.66431 time= 0.03447
Epoch: 0081 train_loss= 13.28928 accuracy= 0.66405 time= 0.03445
Epoch: 0091 train_loss= 13.17645 accuracy= 0.66405 time= 0.03444
Epoch: 0101 train_loss= 13.07886 accuracy= 0.66405 time= 0.03460
Epoch: 0111 train_loss= 12.99445 accuracy= 0.66379 time= 0.03464
Epoch: 0121 train_loss= 12.92145 accuracy= 0.66379 time= 0.03503
Epoch: 0131 train_loss= 12.85831 accuracy= 0.66379 time= 0.03478
Epoch: 0141 train_loss= 12.80372 accuracy= 0.66379 time= 0.03459
Epoch: 0151 train_loss= 12.75654 accuracy= 0.66353 time= 0.03439
Epoch: 0161 train_loss= 12.71579 accuracy= 0.66379 time= 0.03441
Epoch: 0171 train_loss= 12.68061 accuracy= 0.66379 time= 0.03447
Epoch: 0181 train_loss= 12.65027 accuracy= 0.66379 time= 0.03444
Epoch: 0191 train_loss= 12.62414 accuracy= 0.66405 time= 0.03444
Epoch: 0201 train_loss= 12.60166 accuracy= 0.66405 time= 0.03443
Epoch: 0211 train_loss= 12.58233 accuracy= 0.66405 time= 0.03456
Epoch: 0221 train_loss= 12.56575 accuracy= 0.66431 time= 0.03456
Epoch: 0231 train_loss= 12.55154 accuracy= 0.66431 time= 0.03486
Epoch: 0241 train_loss= 12.53937 accuracy= 0.66431 time= 0.03487
Epoch: 0251 train_loss= 12.52899 accuracy= 0.66431 time= 0.03476
Epoch: 0261 train_loss= 12.52012 accuracy= 0.66405 time= 0.03486
Epoch: 0271 train_loss= 12.51258 accuracy= 0.66405 time= 0.03481
Epoch: 0281 train_loss= 12.50616 accuracy= 0.66405 time= 0.03470
Epoch: 0291 train_loss= 12.50072 accuracy= 0.66405 time= 0.03465

accuracy 0.66405
auc 0.50237
f1_score 0.28611
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=3, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', decoder=1, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 469
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
using wavelet scattering transform
y_features shape after scatting (7650, 9685) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([7650, 186]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 10062.77344 accuracy= 0.61647 time= 0.07302
Epoch: 0011 train_loss= 4535.80908 accuracy= 0.62980 time= 0.05029
Epoch: 0021 train_loss= 3297.77612 accuracy= 0.63085 time= 0.05054
Epoch: 0031 train_loss= 2408.12817 accuracy= 0.63320 time= 0.05008
Epoch: 0041 train_loss= 1746.39050 accuracy= 0.64157 time= 0.05012
Epoch: 0051 train_loss= 1481.90540 accuracy= 0.62536 time= 0.05326
Epoch: 0061 train_loss= 1282.16785 accuracy= 0.62588 time= 0.05031
Epoch: 0071 train_loss= 1147.33667 accuracy= 0.62458 time= 0.05038
Epoch: 0081 train_loss= 1046.16150 accuracy= 0.62562 time= 0.05021
Epoch: 0091 train_loss= 967.55493 accuracy= 0.62536 time= 0.05010
Epoch: 0101 train_loss= 899.37866 accuracy= 0.62536 time= 0.05019
Epoch: 0111 train_loss= 839.94141 accuracy= 0.62484 time= 0.05048
Epoch: 0121 train_loss= 790.35077 accuracy= 0.62745 time= 0.05042
Epoch: 0131 train_loss= 742.19958 accuracy= 0.62771 time= 0.05000
Epoch: 0141 train_loss= 703.98901 accuracy= 0.61516 time= 0.05007
Epoch: 0151 train_loss= 667.53094 accuracy= 0.62928 time= 0.05026
Epoch: 0161 train_loss= 640.00220 accuracy= 0.62536 time= 0.05038
Epoch: 0171 train_loss= 613.96039 accuracy= 0.63712 time= 0.05014
Epoch: 0181 train_loss= 580.03949 accuracy= 0.61908 time= 0.05012
Epoch: 0191 train_loss= 557.73853 accuracy= 0.62248 time= 0.05019
Epoch: 0201 train_loss= 538.85449 accuracy= 0.62797 time= 0.05042
Epoch: 0211 train_loss= 514.50421 accuracy= 0.63268 time= 0.05040
Epoch: 0221 train_loss= 493.45834 accuracy= 0.62405 time= 0.05021
Epoch: 0231 train_loss= 476.30835 accuracy= 0.63085 time= 0.05008
Epoch: 0241 train_loss= 464.98431 accuracy= 0.62275 time= 0.05031
Epoch: 0251 train_loss= 445.31894 accuracy= 0.62353 time= 0.05054
Epoch: 0261 train_loss= 436.58347 accuracy= 0.62353 time= 0.05011
Epoch: 0271 train_loss= 412.26266 accuracy= 0.62771 time= 0.04991
Epoch: 0281 train_loss= 405.86047 accuracy= 0.62275 time= 0.05010
Epoch: 0291 train_loss= 395.24966 accuracy= 0.62824 time= 0.05018

accuracy 0.62641
auc 0.47975
f1_score 0.20611
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=3, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', decoder=0, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 670
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
using wavelet scattering transform
y_features shape after scatting (7650, 9685) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([7650, 186]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 29.03855 accuracy= 0.62431 time= 0.07225
Epoch: 0011 train_loss= 20.48690 accuracy= 0.65961 time= 0.05385
Epoch: 0021 train_loss= 18.39413 accuracy= 0.66092 time= 0.05382
Epoch: 0031 train_loss= 16.52278 accuracy= 0.66458 time= 0.05429
Epoch: 0041 train_loss= 15.25619 accuracy= 0.66484 time= 0.05382
Epoch: 0051 train_loss= 14.51812 accuracy= 0.66797 time= 0.05403
Epoch: 0061 train_loss= 14.05739 accuracy= 0.66771 time= 0.05379
Epoch: 0071 train_loss= 13.79195 accuracy= 0.66850 time= 0.05389
Epoch: 0081 train_loss= 13.58278 accuracy= 0.66771 time= 0.05406
Epoch: 0091 train_loss= 13.38671 accuracy= 0.66797 time= 0.05400
Epoch: 0101 train_loss= 13.20689 accuracy= 0.66954 time= 0.05416
Epoch: 0111 train_loss= 13.02079 accuracy= 0.66824 time= 0.05408
Epoch: 0121 train_loss= 12.86433 accuracy= 0.67137 time= 0.05422
Epoch: 0131 train_loss= 12.73395 accuracy= 0.67216 time= 0.05399
Epoch: 0141 train_loss= 12.67330 accuracy= 0.67503 time= 0.05415
Epoch: 0151 train_loss= 12.56997 accuracy= 0.67268 time= 0.05434
Epoch: 0161 train_loss= 12.49623 accuracy= 0.67216 time= 0.05412
Epoch: 0171 train_loss= 12.39605 accuracy= 0.67529 time= 0.05424
Epoch: 0181 train_loss= 12.35729 accuracy= 0.67451 time= 0.05432
Epoch: 0191 train_loss= 12.25484 accuracy= 0.67503 time= 0.05406
Epoch: 0201 train_loss= 12.24564 accuracy= 0.67791 time= 0.05431
Epoch: 0211 train_loss= 12.15030 accuracy= 0.68157 time= 0.05414
Epoch: 0221 train_loss= 12.10411 accuracy= 0.67686 time= 0.05413
Epoch: 0231 train_loss= 12.09613 accuracy= 0.67634 time= 0.05412
Epoch: 0241 train_loss= 12.03968 accuracy= 0.67974 time= 0.05441
Epoch: 0251 train_loss= 11.98728 accuracy= 0.68209 time= 0.05391
Epoch: 0261 train_loss= 11.96444 accuracy= 0.68183 time= 0.05410
Epoch: 0271 train_loss= 11.93799 accuracy= 0.68261 time= 0.05436
Epoch: 0281 train_loss= 11.97219 accuracy= 0.68209 time= 0.05414
Epoch: 0291 train_loss= 11.94598 accuracy= 0.68314 time= 0.05427

accuracy 0.68418
auc 0.57836
f1_score 0.32889
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=3, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', decoder=1, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 553
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
using wavelet scattering transform
y_features shape after scatting (7650, 9685) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 90 steps!

y_features shape after FMS torch.Size([7650, 186]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 10077.67578 accuracy= 0.60837 time= 0.07589
Epoch: 0011 train_loss= 4693.59033 accuracy= 0.62510 time= 0.05027
Epoch: 0021 train_loss= 3492.67603 accuracy= 0.64261 time= 0.05041
Epoch: 0031 train_loss= 2544.52979 accuracy= 0.62771 time= 0.05043
Epoch: 0041 train_loss= 1897.52869 accuracy= 0.64471 time= 0.05057
Epoch: 0051 train_loss= 1539.36353 accuracy= 0.63712 time= 0.05043
Epoch: 0061 train_loss= 1325.21814 accuracy= 0.63033 time= 0.05058
Epoch: 0071 train_loss= 1170.23462 accuracy= 0.63686 time= 0.05046
Epoch: 0081 train_loss= 1053.90698 accuracy= 0.62876 time= 0.05048
Epoch: 0091 train_loss= 969.08026 accuracy= 0.63294 time= 0.05040
Epoch: 0101 train_loss= 900.08612 accuracy= 0.63268 time= 0.05043
Epoch: 0111 train_loss= 840.76483 accuracy= 0.63007 time= 0.05019
Epoch: 0121 train_loss= 792.57117 accuracy= 0.62928 time= 0.05052
Epoch: 0131 train_loss= 746.79199 accuracy= 0.62902 time= 0.05053
Epoch: 0141 train_loss= 709.63922 accuracy= 0.62510 time= 0.05054
Epoch: 0151 train_loss= 674.05103 accuracy= 0.62719 time= 0.05053
Epoch: 0161 train_loss= 644.72528 accuracy= 0.62876 time= 0.05075
Epoch: 0171 train_loss= 623.36182 accuracy= 0.62771 time= 0.05030
Epoch: 0181 train_loss= 594.39520 accuracy= 0.62745 time= 0.05045
Epoch: 0191 train_loss= 568.02966 accuracy= 0.62405 time= 0.05036
Epoch: 0201 train_loss= 547.18939 accuracy= 0.62405 time= 0.05059
Epoch: 0211 train_loss= 527.76642 accuracy= 0.61961 time= 0.05233
Epoch: 0221 train_loss= 508.78510 accuracy= 0.61752 time= 0.05039
Epoch: 0231 train_loss= 494.69370 accuracy= 0.63007 time= 0.05039
Epoch: 0241 train_loss= 473.73737 accuracy= 0.61961 time= 0.05052
Epoch: 0251 train_loss= 456.10339 accuracy= 0.62275 time= 0.05042
Epoch: 0261 train_loss= 445.33035 accuracy= 0.62353 time= 0.05030
Epoch: 0271 train_loss= 427.35397 accuracy= 0.62484 time= 0.05041
Epoch: 0281 train_loss= 413.11499 accuracy= 0.62248 time= 0.05037
Epoch: 0291 train_loss= 400.18185 accuracy= 0.62275 time= 0.05039

accuracy 0.61176
auc 0.45156
f1_score 0.17500
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=3, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', decoder=0, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 536
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
using wavelet scattering transform
y_features shape after scatting (7650, 9685) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 58 steps!

y_features shape after FMS torch.Size([7650, 186]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 28.90861 accuracy= 0.62013 time= 0.08180
Epoch: 0011 train_loss= 20.57392 accuracy= 0.66170 time= 0.05342
Epoch: 0021 train_loss= 18.14847 accuracy= 0.66327 time= 0.05360
Epoch: 0031 train_loss= 16.68406 accuracy= 0.66797 time= 0.05379
Epoch: 0041 train_loss= 15.10021 accuracy= 0.67216 time= 0.05389
Epoch: 0051 train_loss= 14.15130 accuracy= 0.67373 time= 0.05374
Epoch: 0061 train_loss= 13.74334 accuracy= 0.67242 time= 0.05384
Epoch: 0071 train_loss= 13.51293 accuracy= 0.67320 time= 0.05370
Epoch: 0081 train_loss= 13.29006 accuracy= 0.67190 time= 0.05374
Epoch: 0091 train_loss= 13.12786 accuracy= 0.67111 time= 0.05366
Epoch: 0101 train_loss= 12.94347 accuracy= 0.67059 time= 0.05389
Epoch: 0111 train_loss= 12.79494 accuracy= 0.67137 time= 0.05357
Epoch: 0121 train_loss= 12.66821 accuracy= 0.67190 time= 0.05392
Epoch: 0131 train_loss= 12.56484 accuracy= 0.67268 time= 0.05355
Epoch: 0141 train_loss= 12.43481 accuracy= 0.67660 time= 0.05364
Epoch: 0151 train_loss= 12.95242 accuracy= 0.66745 time= 0.05357
Epoch: 0161 train_loss= 12.65635 accuracy= 0.67425 time= 0.05367
Epoch: 0171 train_loss= 12.48273 accuracy= 0.67608 time= 0.05350
Epoch: 0181 train_loss= 12.37167 accuracy= 0.67608 time= 0.05381
Epoch: 0191 train_loss= 12.28231 accuracy= 0.67817 time= 0.05392
Epoch: 0201 train_loss= 12.21380 accuracy= 0.67791 time= 0.05398
Epoch: 0211 train_loss= 12.17385 accuracy= 0.67503 time= 0.05364
Epoch: 0221 train_loss= 12.73014 accuracy= 0.66693 time= 0.05375
Epoch: 0231 train_loss= 12.83523 accuracy= 0.67216 time= 0.05363
Epoch: 0241 train_loss= 12.53642 accuracy= 0.67399 time= 0.05540
Epoch: 0251 train_loss= 12.29211 accuracy= 0.67190 time= 0.05395
Epoch: 0261 train_loss= 12.14679 accuracy= 0.67765 time= 0.05407
Epoch: 0271 train_loss= 12.07436 accuracy= 0.67974 time= 0.05379
Epoch: 0281 train_loss= 12.02431 accuracy= 0.67948 time= 0.05396
Epoch: 0291 train_loss= 11.99250 accuracy= 0.67974 time= 0.05391

accuracy 0.66431
auc 0.51964
f1_score 0.28667
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=0, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 269
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
using wavelet scattering transform
y_features shape after scatting (7650, 9685) <class 'numpy.ndarray'>
using PCA to reduce dim
y_features shape after PCA torch.Size([7650, 186]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 10716.14551 feature_loss= 28.95303 structure_loss= 10687.19238 accuracy= 0.61020 accuracy_s= 0.77307 accuracy_f= 0.79033 time= 0.08460
Epoch: 0011 train_loss= 6750.49219 feature_loss= 22.24865 structure_loss= 6728.24365 accuracy= 0.61516 accuracy_s= 0.77359 accuracy_f= 0.80471 time= 0.06135
Epoch: 0021 train_loss= 5042.92920 feature_loss= 19.94240 structure_loss= 5022.98682 accuracy= 0.61752 accuracy_s= 0.77333 accuracy_f= 0.81386 time= 0.06177
Epoch: 0031 train_loss= 4016.64893 feature_loss= 17.68460 structure_loss= 3998.96436 accuracy= 0.62013 accuracy_s= 0.77464 accuracy_f= 0.82771 time= 0.06159
Epoch: 0041 train_loss= 3262.10254 feature_loss= 15.85019 structure_loss= 3246.25244 accuracy= 0.62431 accuracy_s= 0.77569 accuracy_f= 0.82980 time= 0.06271
Epoch: 0051 train_loss= 2696.45020 feature_loss= 14.69718 structure_loss= 2681.75293 accuracy= 0.62745 accuracy_s= 0.77621 accuracy_f= 0.83007 time= 0.06323
Epoch: 0061 train_loss= 2323.57080 feature_loss= 13.99712 structure_loss= 2309.57373 accuracy= 0.63346 accuracy_s= 0.77935 accuracy_f= 0.83216 time= 0.06161
Epoch: 0071 train_loss= 2046.54126 feature_loss= 13.61572 structure_loss= 2032.92554 accuracy= 0.64288 accuracy_s= 0.78092 accuracy_f= 0.83556 time= 0.06206
Epoch: 0081 train_loss= 1823.37646 feature_loss= 13.35984 structure_loss= 1810.01660 accuracy= 0.64732 accuracy_s= 0.78222 accuracy_f= 0.83765 time= 0.06131
Epoch: 0091 train_loss= 1653.01733 feature_loss= 13.15268 structure_loss= 1639.86462 accuracy= 0.64837 accuracy_s= 0.78275 accuracy_f= 0.83817 time= 0.06229
Epoch: 0101 train_loss= 1493.81726 feature_loss= 12.98411 structure_loss= 1480.83313 accuracy= 0.64993 accuracy_s= 0.78275 accuracy_f= 0.83843 time= 0.06123
Epoch: 0111 train_loss= 1358.10571 feature_loss= 12.83031 structure_loss= 1345.27539 accuracy= 0.65020 accuracy_s= 0.78248 accuracy_f= 0.83948 time= 0.06255
Epoch: 0121 train_loss= 1241.18237 feature_loss= 12.71008 structure_loss= 1228.47229 accuracy= 0.64967 accuracy_s= 0.78405 accuracy_f= 0.83948 time= 0.06154
Epoch: 0131 train_loss= 1132.78992 feature_loss= 12.59049 structure_loss= 1120.19946 accuracy= 0.64993 accuracy_s= 0.78484 accuracy_f= 0.83974 time= 0.06227
Epoch: 0141 train_loss= 1045.02820 feature_loss= 12.47590 structure_loss= 1032.55225 accuracy= 0.64915 accuracy_s= 0.78510 accuracy_f= 0.84078 time= 0.06123
Epoch: 0151 train_loss= 978.60199 feature_loss= 12.36842 structure_loss= 966.23358 accuracy= 0.64680 accuracy_s= 0.78431 accuracy_f= 0.84000 time= 0.06235
Epoch: 0161 train_loss= 921.01031 feature_loss= 12.28185 structure_loss= 908.72845 accuracy= 0.64549 accuracy_s= 0.78431 accuracy_f= 0.84052 time= 0.06132
Epoch: 0171 train_loss= 868.26611 feature_loss= 12.21299 structure_loss= 856.05310 accuracy= 0.64575 accuracy_s= 0.78379 accuracy_f= 0.84078 time= 0.06225
Epoch: 0181 train_loss= 816.28333 feature_loss= 12.15109 structure_loss= 804.13226 accuracy= 0.64549 accuracy_s= 0.78275 accuracy_f= 0.84131 time= 0.06158
Epoch: 0191 train_loss= 776.37665 feature_loss= 12.08596 structure_loss= 764.29071 accuracy= 0.64418 accuracy_s= 0.78379 accuracy_f= 0.84314 time= 0.06267
Epoch: 0201 train_loss= 733.37183 feature_loss= 12.03145 structure_loss= 721.34039 accuracy= 0.64471 accuracy_s= 0.78484 accuracy_f= 0.84497 time= 0.06133
Epoch: 0211 train_loss= 702.55255 feature_loss= 11.99950 structure_loss= 690.55304 accuracy= 0.64444 accuracy_s= 0.78614 accuracy_f= 0.84392 time= 0.06127
Epoch: 0221 train_loss= 672.58838 feature_loss= 11.97353 structure_loss= 660.61487 accuracy= 0.64261 accuracy_s= 0.78771 accuracy_f= 0.84392 time= 0.06129
Epoch: 0231 train_loss= 644.25317 feature_loss= 11.93071 structure_loss= 632.32245 accuracy= 0.64497 accuracy_s= 0.78719 accuracy_f= 0.84601 time= 0.06167
Epoch: 0241 train_loss= 617.48627 feature_loss= 11.90232 structure_loss= 605.58392 accuracy= 0.64863 accuracy_s= 0.78641 accuracy_f= 0.84732 time= 0.06225
Epoch: 0251 train_loss= 600.67896 feature_loss= 11.85756 structure_loss= 588.82141 accuracy= 0.65961 accuracy_s= 0.78562 accuracy_f= 0.84680 time= 0.06162
Epoch: 0261 train_loss= 580.18298 feature_loss= 11.84916 structure_loss= 568.33380 accuracy= 0.65778 accuracy_s= 0.78536 accuracy_f= 0.84732 time= 0.06258
Epoch: 0271 train_loss= 556.53699 feature_loss= 11.81466 structure_loss= 544.72235 accuracy= 0.66092 accuracy_s= 0.78431 accuracy_f= 0.84732 time= 0.06163
Epoch: 0281 train_loss= 540.06793 feature_loss= 11.78635 structure_loss= 528.28156 accuracy= 0.66353 accuracy_s= 0.78536 accuracy_f= 0.84889 time= 0.06157
Epoch: 0291 train_loss= 520.68024 feature_loss= 11.77310 structure_loss= 508.90714 accuracy= 0.65699 accuracy_s= 0.78379 accuracy_f= 0.84837 time= 0.06280

accuracy 0.65856
accuracy_s 0.78405
accuracy_f 0.84915
auc 0.54840
f1_score 0.27444
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=0, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 759
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
using wavelet scattering transform
y_features shape after scatting (7650, 9685) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 57 steps!
y_features shape after FMS torch.Size([7650, 186]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 10203.87305 feature_loss= 29.17846 structure_loss= 10174.69434 accuracy= 0.61098 accuracy_s= 0.77490 accuracy_f= 0.79137 time= 0.08318
Epoch: 0011 train_loss= 5580.92432 feature_loss= 22.36662 structure_loss= 5558.55762 accuracy= 0.61595 accuracy_s= 0.77229 accuracy_f= 0.80680 time= 0.06157
Epoch: 0021 train_loss= 4022.33276 feature_loss= 19.91459 structure_loss= 4002.41821 accuracy= 0.62248 accuracy_s= 0.77229 accuracy_f= 0.81516 time= 0.06191
Epoch: 0031 train_loss= 3126.42261 feature_loss= 17.71719 structure_loss= 3108.70532 accuracy= 0.63059 accuracy_s= 0.77699 accuracy_f= 0.82118 time= 0.06128
Epoch: 0041 train_loss= 2503.03979 feature_loss= 15.91033 structure_loss= 2487.12939 accuracy= 0.63399 accuracy_s= 0.78092 accuracy_f= 0.82954 time= 0.06216
Epoch: 0051 train_loss= 2053.55615 feature_loss= 14.91471 structure_loss= 2038.64148 accuracy= 0.64157 accuracy_s= 0.78693 accuracy_f= 0.83111 time= 0.06171
Epoch: 0061 train_loss= 1770.60681 feature_loss= 14.32156 structure_loss= 1756.28528 accuracy= 0.64837 accuracy_s= 0.79503 accuracy_f= 0.83007 time= 0.06178
Epoch: 0071 train_loss= 1584.55579 feature_loss= 13.87321 structure_loss= 1570.68262 accuracy= 0.65647 accuracy_s= 0.79451 accuracy_f= 0.83373 time= 0.06477
Epoch: 0081 train_loss= 1433.46338 feature_loss= 13.58100 structure_loss= 1419.88232 accuracy= 0.65987 accuracy_s= 0.79660 accuracy_f= 0.83791 time= 0.06136
Epoch: 0091 train_loss= 1310.06909 feature_loss= 13.34803 structure_loss= 1296.72107 accuracy= 0.66170 accuracy_s= 0.79791 accuracy_f= 0.83791 time= 0.06167
Epoch: 0101 train_loss= 1197.50366 feature_loss= 13.13451 structure_loss= 1184.36914 accuracy= 0.66353 accuracy_s= 0.79765 accuracy_f= 0.83843 time= 0.06194
Epoch: 0111 train_loss= 1115.61584 feature_loss= 12.88741 structure_loss= 1102.72839 accuracy= 0.66222 accuracy_s= 0.79817 accuracy_f= 0.83922 time= 0.06133
Epoch: 0121 train_loss= 1047.81006 feature_loss= 12.70950 structure_loss= 1035.10059 accuracy= 0.65752 accuracy_s= 0.79739 accuracy_f= 0.83922 time= 0.06256
Epoch: 0131 train_loss= 990.10168 feature_loss= 12.58916 structure_loss= 977.51251 accuracy= 0.65987 accuracy_s= 0.79739 accuracy_f= 0.84052 time= 0.06141
Epoch: 0141 train_loss= 934.45105 feature_loss= 12.50261 structure_loss= 921.94843 accuracy= 0.65961 accuracy_s= 0.79712 accuracy_f= 0.84052 time= 0.06271
Epoch: 0151 train_loss= 880.69421 feature_loss= 12.39317 structure_loss= 868.30103 accuracy= 0.65987 accuracy_s= 0.79712 accuracy_f= 0.84209 time= 0.06172
Epoch: 0161 train_loss= 838.56635 feature_loss= 12.29525 structure_loss= 826.27112 accuracy= 0.65804 accuracy_s= 0.79634 accuracy_f= 0.84052 time= 0.06179
Epoch: 0171 train_loss= 800.46576 feature_loss= 12.22560 structure_loss= 788.24017 accuracy= 0.65882 accuracy_s= 0.79582 accuracy_f= 0.84183 time= 0.06214
Epoch: 0181 train_loss= 758.87103 feature_loss= 12.16003 structure_loss= 746.71100 accuracy= 0.65699 accuracy_s= 0.79582 accuracy_f= 0.84105 time= 0.06145
Epoch: 0191 train_loss= 727.30133 feature_loss= 12.10409 structure_loss= 715.19727 accuracy= 0.65255 accuracy_s= 0.79503 accuracy_f= 0.84314 time= 0.06309
Epoch: 0201 train_loss= 691.79620 feature_loss= 12.05561 structure_loss= 679.74060 accuracy= 0.65020 accuracy_s= 0.79503 accuracy_f= 0.84340 time= 0.06161
Epoch: 0211 train_loss= 664.15100 feature_loss= 12.00582 structure_loss= 652.14520 accuracy= 0.64889 accuracy_s= 0.79373 accuracy_f= 0.84471 time= 0.06197
Epoch: 0221 train_loss= 628.51556 feature_loss= 11.99138 structure_loss= 616.52417 accuracy= 0.65203 accuracy_s= 0.79477 accuracy_f= 0.84575 time= 0.06238
Epoch: 0231 train_loss= 608.71881 feature_loss= 11.93761 structure_loss= 596.78119 accuracy= 0.64758 accuracy_s= 0.79373 accuracy_f= 0.84601 time= 0.06138
Epoch: 0241 train_loss= 587.64825 feature_loss= 11.91570 structure_loss= 575.73254 accuracy= 0.64967 accuracy_s= 0.79216 accuracy_f= 0.84627 time= 0.06259
Epoch: 0251 train_loss= 562.76306 feature_loss= 11.89755 structure_loss= 550.86548 accuracy= 0.64967 accuracy_s= 0.78954 accuracy_f= 0.84680 time= 0.06161
Epoch: 0261 train_loss= 538.42389 feature_loss= 11.86574 structure_loss= 526.55817 accuracy= 0.64418 accuracy_s= 0.79033 accuracy_f= 0.84627 time= 0.06269
Epoch: 0271 train_loss= 519.69031 feature_loss= 11.84242 structure_loss= 507.84790 accuracy= 0.64209 accuracy_s= 0.78745 accuracy_f= 0.84784 time= 0.06184
Epoch: 0281 train_loss= 498.53656 feature_loss= 11.81907 structure_loss= 486.71750 accuracy= 0.64340 accuracy_s= 0.78719 accuracy_f= 0.84758 time= 0.06169
Epoch: 0291 train_loss= 485.24548 feature_loss= 11.80282 structure_loss= 473.44266 accuracy= 0.66039 accuracy_s= 0.78484 accuracy_f= 0.84810 time= 0.06130

accuracy 0.64784
accuracy_s 0.78379
accuracy_f 0.84941
auc 0.54487
f1_score 0.25167
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=1, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 861
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
using wavelet scattering transform
y_features shape after scatting (7650, 9685) <class 'numpy.ndarray'>
using PCA to reduce dim
y_features shape after PCA torch.Size([7650, 186]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 4316.32178 feature_loss= 29.33846 structure_loss= 4286.98340 accuracy= 0.62222 accuracy_s= 0.78013 accuracy_f= 0.79425 time= 0.10749
Epoch: 0011 train_loss= 123.99402 feature_loss= 20.33368 structure_loss= 103.66035 accuracy= 0.63059 accuracy_s= 0.78092 accuracy_f= 0.81046 time= 0.08363
Epoch: 0021 train_loss= 75.02555 feature_loss= 17.67373 structure_loss= 57.35182 accuracy= 0.63974 accuracy_s= 0.78379 accuracy_f= 0.81333 time= 0.08300
Epoch: 0031 train_loss= 45.10555 feature_loss= 15.50798 structure_loss= 29.59756 accuracy= 0.65621 accuracy_s= 0.78536 accuracy_f= 0.81569 time= 0.08285
Epoch: 0041 train_loss= 28.15775 feature_loss= 14.75051 structure_loss= 13.40724 accuracy= 0.65569 accuracy_s= 0.78797 accuracy_f= 0.81752 time= 0.08279
Epoch: 0051 train_loss= 20.12255 feature_loss= 14.42668 structure_loss= 5.69588 accuracy= 0.65438 accuracy_s= 0.78719 accuracy_f= 0.81725 time= 0.08274
Epoch: 0061 train_loss= 16.96386 feature_loss= 14.15956 structure_loss= 2.80429 accuracy= 0.65542 accuracy_s= 0.78092 accuracy_f= 0.81778 time= 0.08266
Epoch: 0071 train_loss= 15.34878 feature_loss= 13.80504 structure_loss= 1.54374 accuracy= 0.65412 accuracy_s= 0.77595 accuracy_f= 0.81725 time= 0.08314
Epoch: 0081 train_loss= 14.69544 feature_loss= 13.65985 structure_loss= 1.03559 accuracy= 0.65542 accuracy_s= 0.77072 accuracy_f= 0.81752 time= 0.08306
Epoch: 0091 train_loss= 14.24079 feature_loss= 13.42638 structure_loss= 0.81441 accuracy= 0.65647 accuracy_s= 0.76863 accuracy_f= 0.81752 time= 0.08296
Epoch: 0101 train_loss= 13.80156 feature_loss= 13.15055 structure_loss= 0.65102 accuracy= 0.65778 accuracy_s= 0.76758 accuracy_f= 0.81752 time= 0.08296
Epoch: 0111 train_loss= 13.56106 feature_loss= 13.03502 structure_loss= 0.52603 accuracy= 0.65961 accuracy_s= 0.76706 accuracy_f= 0.81725 time= 0.08306
Epoch: 0121 train_loss= 13.43017 feature_loss= 12.96152 structure_loss= 0.46865 accuracy= 0.66013 accuracy_s= 0.76627 accuracy_f= 0.81725 time= 0.08310
Epoch: 0131 train_loss= 13.30675 feature_loss= 12.89400 structure_loss= 0.41274 accuracy= 0.66013 accuracy_s= 0.76627 accuracy_f= 0.81725 time= 0.08323
Epoch: 0141 train_loss= 13.20749 feature_loss= 12.83130 structure_loss= 0.37618 accuracy= 0.66065 accuracy_s= 0.76575 accuracy_f= 0.81725 time= 0.08669
Epoch: 0151 train_loss= 13.12871 feature_loss= 12.78240 structure_loss= 0.34631 accuracy= 0.66092 accuracy_s= 0.76575 accuracy_f= 0.81725 time= 0.08391
Epoch: 0161 train_loss= 13.06222 feature_loss= 12.73974 structure_loss= 0.32248 accuracy= 0.66092 accuracy_s= 0.76549 accuracy_f= 0.81725 time= 0.08365
Epoch: 0171 train_loss= 13.00491 feature_loss= 12.70257 structure_loss= 0.30234 accuracy= 0.66170 accuracy_s= 0.76549 accuracy_f= 0.81725 time= 0.08309
Epoch: 0181 train_loss= 12.95811 feature_loss= 12.67025 structure_loss= 0.28786 accuracy= 0.66170 accuracy_s= 0.76523 accuracy_f= 0.81725 time= 0.08289
Epoch: 0191 train_loss= 12.91625 feature_loss= 12.64220 structure_loss= 0.27406 accuracy= 0.66170 accuracy_s= 0.76497 accuracy_f= 0.81725 time= 0.08328
Epoch: 0201 train_loss= 12.88326 feature_loss= 12.61788 structure_loss= 0.26537 accuracy= 0.66196 accuracy_s= 0.76497 accuracy_f= 0.81725 time= 0.08366
Epoch: 0211 train_loss= 12.85304 feature_loss= 12.59684 structure_loss= 0.25620 accuracy= 0.66222 accuracy_s= 0.76497 accuracy_f= 0.81725 time= 0.08344
Epoch: 0221 train_loss= 12.82642 feature_loss= 12.57866 structure_loss= 0.24775 accuracy= 0.66248 accuracy_s= 0.76497 accuracy_f= 0.81699 time= 0.08365
Epoch: 0231 train_loss= 12.80612 feature_loss= 12.56299 structure_loss= 0.24313 accuracy= 0.66222 accuracy_s= 0.76497 accuracy_f= 0.81699 time= 0.08397
Epoch: 0241 train_loss= 12.78530 feature_loss= 12.54949 structure_loss= 0.23581 accuracy= 0.66248 accuracy_s= 0.76497 accuracy_f= 0.81699 time= 0.08338
Epoch: 0251 train_loss= 12.77095 feature_loss= 12.53789 structure_loss= 0.23306 accuracy= 0.66222 accuracy_s= 0.76497 accuracy_f= 0.81699 time= 0.08350
Epoch: 0261 train_loss= 12.75661 feature_loss= 12.52794 structure_loss= 0.22867 accuracy= 0.66248 accuracy_s= 0.76497 accuracy_f= 0.81699 time= 0.08346
Epoch: 0271 train_loss= 12.74682 feature_loss= 12.51942 structure_loss= 0.22741 accuracy= 0.66248 accuracy_s= 0.76471 accuracy_f= 0.81699 time= 0.08361
Epoch: 0281 train_loss= 12.73481 feature_loss= 12.51213 structure_loss= 0.22267 accuracy= 0.66248 accuracy_s= 0.76471 accuracy_f= 0.81699 time= 0.08360
Epoch: 0291 train_loss= 12.72691 feature_loss= 12.50592 structure_loss= 0.22100 accuracy= 0.66248 accuracy_s= 0.76471 accuracy_f= 0.81699 time= 0.08358

accuracy 0.66275
accuracy_s 0.76471
accuracy_f 0.81699
auc 0.49750
f1_score 0.28333
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=1, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 84
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
using wavelet scattering transform
y_features shape after scatting (7650, 9685) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 58 steps!
y_features shape after FMS torch.Size([7650, 186]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 73.95361 feature_loss= 29.21722 structure_loss= 44.73639 accuracy= 0.61203 accuracy_s= 0.78144 accuracy_f= 0.79425 time= 0.10730
Epoch: 0011 train_loss= 21.09382 feature_loss= 19.65359 structure_loss= 1.44023 accuracy= 0.64654 accuracy_s= 0.76837 accuracy_f= 0.82144 time= 0.08297
Epoch: 0021 train_loss= 15.47324 feature_loss= 14.70867 structure_loss= 0.76457 accuracy= 0.65752 accuracy_s= 0.76758 accuracy_f= 0.81699 time= 0.08305
Epoch: 0031 train_loss= 14.70025 feature_loss= 14.17067 structure_loss= 0.52958 accuracy= 0.66092 accuracy_s= 0.76837 accuracy_f= 0.81752 time= 0.08269
Epoch: 0041 train_loss= 14.36443 feature_loss= 13.94656 structure_loss= 0.41788 accuracy= 0.66118 accuracy_s= 0.76758 accuracy_f= 0.81752 time= 0.08327
Epoch: 0051 train_loss= 14.10211 feature_loss= 13.74908 structure_loss= 0.35304 accuracy= 0.66144 accuracy_s= 0.76680 accuracy_f= 0.81752 time= 0.08253
Epoch: 0061 train_loss= 13.89178 feature_loss= 13.57631 structure_loss= 0.31548 accuracy= 0.66196 accuracy_s= 0.76575 accuracy_f= 0.81752 time= 0.08291
Epoch: 0071 train_loss= 13.71407 feature_loss= 13.42585 structure_loss= 0.28822 accuracy= 0.66222 accuracy_s= 0.76497 accuracy_f= 0.81752 time= 0.08267
Epoch: 0081 train_loss= 13.56127 feature_loss= 13.29518 structure_loss= 0.26609 accuracy= 0.66275 accuracy_s= 0.76497 accuracy_f= 0.81752 time= 0.08301
Epoch: 0091 train_loss= 13.43572 feature_loss= 13.18190 structure_loss= 0.25383 accuracy= 0.66275 accuracy_s= 0.76497 accuracy_f= 0.81752 time= 0.08288
Epoch: 0101 train_loss= 13.32716 feature_loss= 13.08376 structure_loss= 0.24340 accuracy= 0.66275 accuracy_s= 0.76471 accuracy_f= 0.81725 time= 0.08342
Epoch: 0111 train_loss= 13.23350 feature_loss= 12.99880 structure_loss= 0.23470 accuracy= 0.66275 accuracy_s= 0.76471 accuracy_f= 0.81725 time= 0.08356
Epoch: 0121 train_loss= 13.15423 feature_loss= 12.92525 structure_loss= 0.22897 accuracy= 0.66327 accuracy_s= 0.76471 accuracy_f= 0.81725 time= 0.08323
Epoch: 0131 train_loss= 13.08579 feature_loss= 12.86162 structure_loss= 0.22417 accuracy= 0.66353 accuracy_s= 0.76471 accuracy_f= 0.81725 time= 0.08316
Epoch: 0141 train_loss= 13.02709 feature_loss= 12.80659 structure_loss= 0.22051 accuracy= 0.66327 accuracy_s= 0.76471 accuracy_f= 0.81725 time= 0.08350
Epoch: 0151 train_loss= 12.97529 feature_loss= 12.75901 structure_loss= 0.21628 accuracy= 0.66327 accuracy_s= 0.76471 accuracy_f= 0.81725 time= 0.08384
Epoch: 0161 train_loss= 12.93277 feature_loss= 12.71790 structure_loss= 0.21486 accuracy= 0.66301 accuracy_s= 0.76471 accuracy_f= 0.81725 time= 0.08355
Epoch: 0171 train_loss= 12.89455 feature_loss= 12.68242 structure_loss= 0.21213 accuracy= 0.66327 accuracy_s= 0.76471 accuracy_f= 0.81725 time= 0.08288
Epoch: 0181 train_loss= 12.86301 feature_loss= 12.65182 structure_loss= 0.21118 accuracy= 0.66327 accuracy_s= 0.76471 accuracy_f= 0.81725 time= 0.08253
Epoch: 0191 train_loss= 12.83514 feature_loss= 12.62546 structure_loss= 0.20968 accuracy= 0.66327 accuracy_s= 0.76471 accuracy_f= 0.81725 time= 0.08353
Epoch: 0201 train_loss= 12.81115 feature_loss= 12.60278 structure_loss= 0.20837 accuracy= 0.66327 accuracy_s= 0.76471 accuracy_f= 0.81725 time= 0.08368
Epoch: 0211 train_loss= 12.79080 feature_loss= 12.58328 structure_loss= 0.20752 accuracy= 0.66327 accuracy_s= 0.76471 accuracy_f= 0.81699 time= 0.08378
Epoch: 0221 train_loss= 12.77290 feature_loss= 12.56655 structure_loss= 0.20635 accuracy= 0.66327 accuracy_s= 0.76471 accuracy_f= 0.81699 time= 0.08371
Epoch: 0231 train_loss= 12.75844 feature_loss= 12.55221 structure_loss= 0.20623 accuracy= 0.66327 accuracy_s= 0.76471 accuracy_f= 0.81699 time= 0.08325
Epoch: 0241 train_loss= 12.74525 feature_loss= 12.53994 structure_loss= 0.20532 accuracy= 0.66301 accuracy_s= 0.76471 accuracy_f= 0.81699 time= 0.08365
Epoch: 0251 train_loss= 12.73382 feature_loss= 12.52946 structure_loss= 0.20436 accuracy= 0.66301 accuracy_s= 0.76471 accuracy_f= 0.81699 time= 0.08362
Epoch: 0261 train_loss= 12.72499 feature_loss= 12.52052 structure_loss= 0.20448 accuracy= 0.66301 accuracy_s= 0.76471 accuracy_f= 0.81699 time= 0.08362
Epoch: 0271 train_loss= 12.71698 feature_loss= 12.51290 structure_loss= 0.20408 accuracy= 0.66301 accuracy_s= 0.76471 accuracy_f= 0.81699 time= 0.08396
Epoch: 0281 train_loss= 12.71015 feature_loss= 12.50644 structure_loss= 0.20371 accuracy= 0.66301 accuracy_s= 0.76471 accuracy_f= 0.81699 time= 0.08356
Epoch: 0291 train_loss= 12.70416 feature_loss= 12.50095 structure_loss= 0.20321 accuracy= 0.66301 accuracy_s= 0.76471 accuracy_f= 0.81699 time= 0.08335

accuracy 0.66301
accuracy_s 0.76471
accuracy_f 0.81699
auc 0.49844
f1_score 0.28389
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=2, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 711
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
using wavelet scattering transform
y_features shape after scatting (7650, 9685) <class 'numpy.ndarray'>
using PCA to reduce dim
y_features shape after PCA torch.Size([7650, 186]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 2899.70337 feature_loss= 29.38943 structure_loss= 2870.31396 accuracy= 0.62039 accuracy_s= 0.78170 accuracy_f= 0.79399 time= 0.08287
Epoch: 0011 train_loss= 92.90757 feature_loss= 20.60984 structure_loss= 72.29773 accuracy= 0.63817 accuracy_s= 0.77987 accuracy_f= 0.81020 time= 0.06409
Epoch: 0021 train_loss= 60.79714 feature_loss= 17.79753 structure_loss= 42.99961 accuracy= 0.63974 accuracy_s= 0.77987 accuracy_f= 0.81438 time= 0.06296
Epoch: 0031 train_loss= 41.68545 feature_loss= 15.89566 structure_loss= 25.78978 accuracy= 0.65412 accuracy_s= 0.77856 accuracy_f= 0.81621 time= 0.06309
Epoch: 0041 train_loss= 30.09569 feature_loss= 15.05121 structure_loss= 15.04449 accuracy= 0.65307 accuracy_s= 0.77882 accuracy_f= 0.81699 time= 0.06413
Epoch: 0051 train_loss= 23.46906 feature_loss= 14.63124 structure_loss= 8.83782 accuracy= 0.65386 accuracy_s= 0.77908 accuracy_f= 0.81699 time= 0.06319
Epoch: 0061 train_loss= 19.62917 feature_loss= 14.31947 structure_loss= 5.30971 accuracy= 0.65464 accuracy_s= 0.77987 accuracy_f= 0.81752 time= 0.06301
Epoch: 0071 train_loss= 17.34331 feature_loss= 14.10622 structure_loss= 3.23709 accuracy= 0.65542 accuracy_s= 0.78092 accuracy_f= 0.81778 time= 0.06357
Epoch: 0081 train_loss= 16.01437 feature_loss= 13.91738 structure_loss= 2.09698 accuracy= 0.65647 accuracy_s= 0.77647 accuracy_f= 0.81778 time= 0.06466
Epoch: 0091 train_loss= 15.18891 feature_loss= 13.75094 structure_loss= 1.43797 accuracy= 0.65778 accuracy_s= 0.77176 accuracy_f= 0.81778 time= 0.06376
Epoch: 0101 train_loss= 14.64761 feature_loss= 13.60430 structure_loss= 1.04332 accuracy= 0.65778 accuracy_s= 0.77046 accuracy_f= 0.81830 time= 0.06259
Epoch: 0111 train_loss= 14.26317 feature_loss= 13.45725 structure_loss= 0.80592 accuracy= 0.65830 accuracy_s= 0.76915 accuracy_f= 0.81830 time= 0.06255
Epoch: 0121 train_loss= 13.98454 feature_loss= 13.35660 structure_loss= 0.62795 accuracy= 0.66039 accuracy_s= 0.76784 accuracy_f= 0.81830 time= 0.06309
Epoch: 0131 train_loss= 13.79339 feature_loss= 13.25557 structure_loss= 0.53782 accuracy= 0.66118 accuracy_s= 0.76758 accuracy_f= 0.81856 time= 0.06404
Epoch: 0141 train_loss= 13.64711 feature_loss= 13.17858 structure_loss= 0.46852 accuracy= 0.66065 accuracy_s= 0.76654 accuracy_f= 0.81856 time= 0.06312
Epoch: 0151 train_loss= 13.51222 feature_loss= 13.10462 structure_loss= 0.40760 accuracy= 0.66196 accuracy_s= 0.76654 accuracy_f= 0.81882 time= 0.06304
Epoch: 0161 train_loss= 13.39585 feature_loss= 13.03494 structure_loss= 0.36091 accuracy= 0.66196 accuracy_s= 0.76601 accuracy_f= 0.81882 time= 0.06429
Epoch: 0171 train_loss= 13.31876 feature_loss= 12.98316 structure_loss= 0.33560 accuracy= 0.66248 accuracy_s= 0.76575 accuracy_f= 0.81856 time= 0.06321
Epoch: 0181 train_loss= 13.25253 feature_loss= 12.94109 structure_loss= 0.31144 accuracy= 0.66248 accuracy_s= 0.76575 accuracy_f= 0.81882 time= 0.06316
Epoch: 0191 train_loss= 13.19534 feature_loss= 12.89826 structure_loss= 0.29707 accuracy= 0.66222 accuracy_s= 0.76549 accuracy_f= 0.81882 time= 0.06402
Epoch: 0201 train_loss= 12.96220 feature_loss= 12.67833 structure_loss= 0.28387 accuracy= 0.66327 accuracy_s= 0.76549 accuracy_f= 0.81882 time= 0.06406
Epoch: 0211 train_loss= 12.91010 feature_loss= 12.64452 structure_loss= 0.26558 accuracy= 0.66379 accuracy_s= 0.76549 accuracy_f= 0.81856 time= 0.06290
Epoch: 0221 train_loss= 12.88716 feature_loss= 12.62659 structure_loss= 0.26057 accuracy= 0.66431 accuracy_s= 0.76549 accuracy_f= 0.81856 time= 0.06419
Epoch: 0231 train_loss= 12.85335 feature_loss= 12.60049 structure_loss= 0.25285 accuracy= 0.66431 accuracy_s= 0.76523 accuracy_f= 0.81856 time= 0.06403
Epoch: 0241 train_loss= 12.83294 feature_loss= 12.58081 structure_loss= 0.25213 accuracy= 0.66431 accuracy_s= 0.76523 accuracy_f= 0.81882 time= 0.06248
Epoch: 0251 train_loss= 12.80386 feature_loss= 12.56352 structure_loss= 0.24034 accuracy= 0.66431 accuracy_s= 0.76523 accuracy_f= 0.81908 time= 0.06312
Epoch: 0261 train_loss= 12.78885 feature_loss= 12.55107 structure_loss= 0.23778 accuracy= 0.66484 accuracy_s= 0.76523 accuracy_f= 0.81882 time= 0.06402
Epoch: 0271 train_loss= 12.76648 feature_loss= 12.53501 structure_loss= 0.23147 accuracy= 0.66458 accuracy_s= 0.76523 accuracy_f= 0.81908 time= 0.06330
Epoch: 0281 train_loss= 12.75422 feature_loss= 12.52213 structure_loss= 0.23209 accuracy= 0.66458 accuracy_s= 0.76497 accuracy_f= 0.81856 time= 0.06345
Epoch: 0291 train_loss= 12.74648 feature_loss= 12.51357 structure_loss= 0.23291 accuracy= 0.66431 accuracy_s= 0.76497 accuracy_f= 0.81882 time= 0.06394

accuracy 0.66458
accuracy_s 0.76497
accuracy_f 0.81882
auc 0.50684
f1_score 0.28722
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=2, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 689
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
using wavelet scattering transform
y_features shape after scatting (7650, 9685) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 58 steps!
y_features shape after FMS torch.Size([7650, 186]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 55.96875 feature_loss= 28.92219 structure_loss= 27.04656 accuracy= 0.61595 accuracy_s= 0.78510 accuracy_f= 0.79346 time= 0.08433
Epoch: 0011 train_loss= 19.68958 feature_loss= 17.98328 structure_loss= 1.70629 accuracy= 0.64732 accuracy_s= 0.77072 accuracy_f= 0.81359 time= 0.06360
Epoch: 0021 train_loss= 15.19474 feature_loss= 14.53260 structure_loss= 0.66214 accuracy= 0.66065 accuracy_s= 0.76941 accuracy_f= 0.81725 time= 0.06396
Epoch: 0031 train_loss= 14.62905 feature_loss= 14.16954 structure_loss= 0.45951 accuracy= 0.66092 accuracy_s= 0.76993 accuracy_f= 0.81752 time= 0.06366
Epoch: 0041 train_loss= 14.30943 feature_loss= 13.94272 structure_loss= 0.36670 accuracy= 0.66170 accuracy_s= 0.76863 accuracy_f= 0.81752 time= 0.06413
Epoch: 0051 train_loss= 14.05740 feature_loss= 13.74330 structure_loss= 0.31410 accuracy= 0.66248 accuracy_s= 0.76863 accuracy_f= 0.81752 time= 0.06280
Epoch: 0061 train_loss= 13.85138 feature_loss= 13.56943 structure_loss= 0.28195 accuracy= 0.66248 accuracy_s= 0.76784 accuracy_f= 0.81752 time= 0.06301
Epoch: 0071 train_loss= 13.67927 feature_loss= 13.41853 structure_loss= 0.26074 accuracy= 0.66275 accuracy_s= 0.76680 accuracy_f= 0.81752 time= 0.06373
Epoch: 0081 train_loss= 13.53503 feature_loss= 13.28788 structure_loss= 0.24715 accuracy= 0.66275 accuracy_s= 0.76523 accuracy_f= 0.81752 time= 0.06362
Epoch: 0091 train_loss= 13.41141 feature_loss= 13.17490 structure_loss= 0.23651 accuracy= 0.66275 accuracy_s= 0.76523 accuracy_f= 0.81752 time= 0.06379
Epoch: 0101 train_loss= 13.30649 feature_loss= 13.07722 structure_loss= 0.22927 accuracy= 0.66327 accuracy_s= 0.76523 accuracy_f= 0.81725 time= 0.06366
Epoch: 0111 train_loss= 13.21746 feature_loss= 12.99278 structure_loss= 0.22468 accuracy= 0.66327 accuracy_s= 0.76523 accuracy_f= 0.81725 time= 0.06354
Epoch: 0121 train_loss= 13.14005 feature_loss= 12.91979 structure_loss= 0.22026 accuracy= 0.66327 accuracy_s= 0.76523 accuracy_f= 0.81725 time= 0.06323
Epoch: 0131 train_loss= 13.07416 feature_loss= 12.85669 structure_loss= 0.21747 accuracy= 0.66327 accuracy_s= 0.76497 accuracy_f= 0.81725 time= 0.06355
Epoch: 0141 train_loss= 13.01714 feature_loss= 12.80217 structure_loss= 0.21497 accuracy= 0.66327 accuracy_s= 0.76497 accuracy_f= 0.81725 time= 0.06317
Epoch: 0151 train_loss= 12.96788 feature_loss= 12.75506 structure_loss= 0.21282 accuracy= 0.66327 accuracy_s= 0.76471 accuracy_f= 0.81725 time= 0.06377
Epoch: 0161 train_loss= 12.92558 feature_loss= 12.71440 structure_loss= 0.21118 accuracy= 0.66327 accuracy_s= 0.76471 accuracy_f= 0.81725 time= 0.06278
Epoch: 0171 train_loss= 12.88917 feature_loss= 12.67931 structure_loss= 0.20986 accuracy= 0.66327 accuracy_s= 0.76471 accuracy_f= 0.81725 time= 0.06297
Epoch: 0181 train_loss= 12.85788 feature_loss= 12.64907 structure_loss= 0.20880 accuracy= 0.66327 accuracy_s= 0.76471 accuracy_f= 0.81725 time= 0.06346
Epoch: 0191 train_loss= 12.83090 feature_loss= 12.62303 structure_loss= 0.20787 accuracy= 0.66327 accuracy_s= 0.76471 accuracy_f= 0.81725 time= 0.06374
Epoch: 0201 train_loss= 12.80759 feature_loss= 12.60064 structure_loss= 0.20695 accuracy= 0.66327 accuracy_s= 0.76471 accuracy_f= 0.81725 time= 0.06305
Epoch: 0211 train_loss= 12.78775 feature_loss= 12.58140 structure_loss= 0.20635 accuracy= 0.66327 accuracy_s= 0.76471 accuracy_f= 0.81699 time= 0.06326
Epoch: 0221 train_loss= 12.77077 feature_loss= 12.56490 structure_loss= 0.20586 accuracy= 0.66327 accuracy_s= 0.76471 accuracy_f= 0.81699 time= 0.06359
Epoch: 0231 train_loss= 12.75583 feature_loss= 12.55077 structure_loss= 0.20506 accuracy= 0.66301 accuracy_s= 0.76471 accuracy_f= 0.81699 time= 0.06394
Epoch: 0241 train_loss= 12.74359 feature_loss= 12.53868 structure_loss= 0.20491 accuracy= 0.66301 accuracy_s= 0.76471 accuracy_f= 0.81699 time= 0.06383
Epoch: 0251 train_loss= 12.73272 feature_loss= 12.52836 structure_loss= 0.20437 accuracy= 0.66301 accuracy_s= 0.76471 accuracy_f= 0.81699 time= 0.06298
Epoch: 0261 train_loss= 12.72358 feature_loss= 12.51956 structure_loss= 0.20402 accuracy= 0.66327 accuracy_s= 0.76471 accuracy_f= 0.81699 time= 0.06315
Epoch: 0271 train_loss= 12.71577 feature_loss= 12.51207 structure_loss= 0.20370 accuracy= 0.66327 accuracy_s= 0.76471 accuracy_f= 0.81699 time= 0.06388
Epoch: 0281 train_loss= 12.70899 feature_loss= 12.50571 structure_loss= 0.20328 accuracy= 0.66301 accuracy_s= 0.76471 accuracy_f= 0.81699 time= 0.06378
Epoch: 0291 train_loss= 12.70330 feature_loss= 12.50032 structure_loss= 0.20298 accuracy= 0.66301 accuracy_s= 0.76471 accuracy_f= 0.81699 time= 0.06289

accuracy 0.66301
accuracy_s 0.76471
accuracy_f 0.81699
auc 0.49809
f1_score 0.28389
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=3, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 999
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
using wavelet scattering transform
y_features shape after scatting (7650, 9685) <class 'numpy.ndarray'>
using PCA to reduce dim
y_features shape after PCA torch.Size([7650, 186]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 10219.77148 feature_loss= 28.54258 structure_loss= 10191.22852 accuracy= 0.61176 accuracy_s= 0.77490 accuracy_f= 0.79529 time= 0.10537
Epoch: 0011 train_loss= 4644.08887 feature_loss= 20.40755 structure_loss= 4623.68115 accuracy= 0.61856 accuracy_s= 0.77203 accuracy_f= 0.81333 time= 0.08292
Epoch: 0021 train_loss= 3378.23291 feature_loss= 18.53004 structure_loss= 3359.70288 accuracy= 0.63974 accuracy_s= 0.77438 accuracy_f= 0.81725 time= 0.08225
Epoch: 0031 train_loss= 2421.65967 feature_loss= 16.94597 structure_loss= 2404.71362 accuracy= 0.63922 accuracy_s= 0.77412 accuracy_f= 0.82144 time= 0.08261
Epoch: 0041 train_loss= 1753.19580 feature_loss= 15.55818 structure_loss= 1737.63757 accuracy= 0.65778 accuracy_s= 0.78039 accuracy_f= 0.82824 time= 0.08268
Epoch: 0051 train_loss= 1455.37207 feature_loss= 14.46087 structure_loss= 1440.91125 accuracy= 0.64889 accuracy_s= 0.77542 accuracy_f= 0.82928 time= 0.08213
Epoch: 0061 train_loss= 1282.22546 feature_loss= 14.16354 structure_loss= 1268.06189 accuracy= 0.65176 accuracy_s= 0.77882 accuracy_f= 0.82484 time= 0.08245
Epoch: 0071 train_loss= 1148.65454 feature_loss= 13.94537 structure_loss= 1134.70911 accuracy= 0.65490 accuracy_s= 0.77516 accuracy_f= 0.82771 time= 0.08260
Epoch: 0081 train_loss= 1050.69824 feature_loss= 13.73133 structure_loss= 1036.96692 accuracy= 0.65647 accuracy_s= 0.77647 accuracy_f= 0.83059 time= 0.08248
Epoch: 0091 train_loss= 972.19336 feature_loss= 13.53560 structure_loss= 958.65778 accuracy= 0.66092 accuracy_s= 0.77647 accuracy_f= 0.83085 time= 0.08236
Epoch: 0101 train_loss= 894.01678 feature_loss= 13.36893 structure_loss= 880.64783 accuracy= 0.65882 accuracy_s= 0.77464 accuracy_f= 0.83242 time= 0.08257
Epoch: 0111 train_loss= 844.90295 feature_loss= 13.19254 structure_loss= 831.71039 accuracy= 0.65961 accuracy_s= 0.77386 accuracy_f= 0.83373 time= 0.08349
Epoch: 0121 train_loss= 789.67310 feature_loss= 13.04553 structure_loss= 776.62756 accuracy= 0.66301 accuracy_s= 0.77621 accuracy_f= 0.83451 time= 0.08269
Epoch: 0131 train_loss= 748.01892 feature_loss= 12.91609 structure_loss= 735.10284 accuracy= 0.66092 accuracy_s= 0.77569 accuracy_f= 0.83608 time= 0.08225
Epoch: 0141 train_loss= 704.88519 feature_loss= 12.78091 structure_loss= 692.10431 accuracy= 0.65987 accuracy_s= 0.77516 accuracy_f= 0.83556 time= 0.08209
Epoch: 0151 train_loss= 669.84186 feature_loss= 12.64801 structure_loss= 657.19385 accuracy= 0.66379 accuracy_s= 0.77490 accuracy_f= 0.83843 time= 0.08315
Epoch: 0161 train_loss= 642.18225 feature_loss= 12.52751 structure_loss= 629.65472 accuracy= 0.66327 accuracy_s= 0.77542 accuracy_f= 0.83817 time= 0.08283
Epoch: 0171 train_loss= 611.13379 feature_loss= 12.41186 structure_loss= 598.72192 accuracy= 0.66144 accuracy_s= 0.77333 accuracy_f= 0.83922 time= 0.08232
Epoch: 0181 train_loss= 590.85504 feature_loss= 12.34873 structure_loss= 578.50629 accuracy= 0.66458 accuracy_s= 0.77386 accuracy_f= 0.84000 time= 0.08236
Epoch: 0191 train_loss= 564.09393 feature_loss= 12.27333 structure_loss= 551.82062 accuracy= 0.66588 accuracy_s= 0.77647 accuracy_f= 0.83948 time= 0.08301
Epoch: 0201 train_loss= 547.00757 feature_loss= 12.22824 structure_loss= 534.77930 accuracy= 0.66588 accuracy_s= 0.77542 accuracy_f= 0.83974 time= 0.08307
Epoch: 0211 train_loss= 523.70703 feature_loss= 12.79486 structure_loss= 510.91220 accuracy= 0.65203 accuracy_s= 0.77438 accuracy_f= 0.82170 time= 0.08276
Epoch: 0221 train_loss= 504.62534 feature_loss= 12.61229 structure_loss= 492.01306 accuracy= 0.65961 accuracy_s= 0.77438 accuracy_f= 0.82954 time= 0.08286
Epoch: 0231 train_loss= 486.79156 feature_loss= 12.42633 structure_loss= 474.36523 accuracy= 0.65778 accuracy_s= 0.77255 accuracy_f= 0.83425 time= 0.08266
Epoch: 0241 train_loss= 474.94254 feature_loss= 12.26795 structure_loss= 462.67459 accuracy= 0.65882 accuracy_s= 0.77176 accuracy_f= 0.83791 time= 0.08296
Epoch: 0251 train_loss= 460.39496 feature_loss= 12.18183 structure_loss= 448.21313 accuracy= 0.66196 accuracy_s= 0.77542 accuracy_f= 0.83974 time= 0.08252
Epoch: 0261 train_loss= 442.28571 feature_loss= 12.12640 structure_loss= 430.15930 accuracy= 0.66275 accuracy_s= 0.77412 accuracy_f= 0.84000 time= 0.08309
Epoch: 0271 train_loss= 427.48398 feature_loss= 12.07608 structure_loss= 415.40790 accuracy= 0.66484 accuracy_s= 0.77229 accuracy_f= 0.84183 time= 0.08307
Epoch: 0281 train_loss= 421.25586 feature_loss= 12.02971 structure_loss= 409.22614 accuracy= 0.66510 accuracy_s= 0.77778 accuracy_f= 0.84340 time= 0.08293
Epoch: 0291 train_loss= 413.20984 feature_loss= 11.98441 structure_loss= 401.22543 accuracy= 0.66928 accuracy_s= 0.77569 accuracy_f= 0.84444 time= 0.08313

accuracy 0.65647
accuracy_s 0.77412
accuracy_f 0.83974
auc 0.52749
f1_score 0.27000
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=3, beta=1, clique_size=30, cuda=False, dataset='amazon_electronics_photo', device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 613
Found existing ad data, loading...
feature_dim: 745 hidden1_dim: 372 hidden2_dim: 186 nodes_num: 7650 anomaly_num: 1800

Start modeling
using wavelet scattering transform
y_features shape after scatting (7650, 9685) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 58 steps!
y_features shape after FMS torch.Size([7650, 186]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 10146.23340 feature_loss= 28.88331 structure_loss= 10117.34961 accuracy= 0.60810 accuracy_s= 0.77333 accuracy_f= 0.79268 time= 0.10577
Epoch: 0011 train_loss= 4799.66064 feature_loss= 20.69159 structure_loss= 4778.96924 accuracy= 0.62379 accuracy_s= 0.77307 accuracy_f= 0.81359 time= 0.08041
Epoch: 0021 train_loss= 3501.13452 feature_loss= 18.42172 structure_loss= 3482.71289 accuracy= 0.64837 accuracy_s= 0.77307 accuracy_f= 0.81908 time= 0.08241
Epoch: 0031 train_loss= 2608.71240 feature_loss= 16.91000 structure_loss= 2591.80249 accuracy= 0.63922 accuracy_s= 0.77595 accuracy_f= 0.82824 time= 0.08199
Epoch: 0041 train_loss= 1972.60767 feature_loss= 15.38825 structure_loss= 1957.21936 accuracy= 0.63869 accuracy_s= 0.78013 accuracy_f= 0.83059 time= 0.08182
Epoch: 0051 train_loss= 1592.00146 feature_loss= 14.47992 structure_loss= 1577.52148 accuracy= 0.63477 accuracy_s= 0.78144 accuracy_f= 0.83033 time= 0.08256
Epoch: 0061 train_loss= 1373.74927 feature_loss= 14.01793 structure_loss= 1359.73132 accuracy= 0.62667 accuracy_s= 0.77882 accuracy_f= 0.83111 time= 0.08189
Epoch: 0071 train_loss= 1188.08362 feature_loss= 13.79226 structure_loss= 1174.29138 accuracy= 0.65935 accuracy_s= 0.78536 accuracy_f= 0.83059 time= 0.08237
Epoch: 0081 train_loss= 1073.89111 feature_loss= 13.57707 structure_loss= 1060.31409 accuracy= 0.66039 accuracy_s= 0.78405 accuracy_f= 0.83268 time= 0.08254
Epoch: 0091 train_loss= 984.63452 feature_loss= 13.39766 structure_loss= 971.23688 accuracy= 0.66536 accuracy_s= 0.78222 accuracy_f= 0.83608 time= 0.08184
Epoch: 0101 train_loss= 911.50903 feature_loss= 13.23710 structure_loss= 898.27191 accuracy= 0.66092 accuracy_s= 0.78092 accuracy_f= 0.83608 time= 0.08240
Epoch: 0111 train_loss= 838.95105 feature_loss= 13.08979 structure_loss= 825.86127 accuracy= 0.66118 accuracy_s= 0.78039 accuracy_f= 0.83686 time= 0.08209
Epoch: 0121 train_loss= 783.11511 feature_loss= 12.96917 structure_loss= 770.14594 accuracy= 0.66510 accuracy_s= 0.78248 accuracy_f= 0.83791 time= 0.08254
Epoch: 0131 train_loss= 732.99420 feature_loss= 12.83437 structure_loss= 720.15985 accuracy= 0.66379 accuracy_s= 0.77987 accuracy_f= 0.83895 time= 0.08188
Epoch: 0141 train_loss= 695.17914 feature_loss= 12.73330 structure_loss= 682.44586 accuracy= 0.66092 accuracy_s= 0.77908 accuracy_f= 0.83843 time= 0.08257
Epoch: 0151 train_loss= 664.35791 feature_loss= 12.65505 structure_loss= 651.70288 accuracy= 0.66275 accuracy_s= 0.77804 accuracy_f= 0.83817 time= 0.08253
Epoch: 0161 train_loss= 629.28845 feature_loss= 12.56423 structure_loss= 616.72424 accuracy= 0.66484 accuracy_s= 0.77830 accuracy_f= 0.84026 time= 0.08209
Epoch: 0171 train_loss= 600.03625 feature_loss= 12.50580 structure_loss= 587.53046 accuracy= 0.66379 accuracy_s= 0.77725 accuracy_f= 0.84157 time= 0.08280
Epoch: 0181 train_loss= 570.96777 feature_loss= 12.44711 structure_loss= 558.52069 accuracy= 0.66379 accuracy_s= 0.77569 accuracy_f= 0.84000 time= 0.08215
Epoch: 0191 train_loss= 543.96112 feature_loss= 12.40319 structure_loss= 531.55792 accuracy= 0.66379 accuracy_s= 0.77673 accuracy_f= 0.84131 time= 0.08190
Epoch: 0201 train_loss= 526.60443 feature_loss= 12.35141 structure_loss= 514.25305 accuracy= 0.66353 accuracy_s= 0.77490 accuracy_f= 0.84235 time= 0.08305
Epoch: 0211 train_loss= 503.00443 feature_loss= 12.29775 structure_loss= 490.70667 accuracy= 0.66118 accuracy_s= 0.77333 accuracy_f= 0.84288 time= 0.08212
Epoch: 0221 train_loss= 488.02017 feature_loss= 12.32570 structure_loss= 475.69446 accuracy= 0.66484 accuracy_s= 0.77412 accuracy_f= 0.84444 time= 0.08266
Epoch: 0231 train_loss= 466.20023 feature_loss= 12.24070 structure_loss= 453.95953 accuracy= 0.66458 accuracy_s= 0.77412 accuracy_f= 0.84392 time= 0.08292
Epoch: 0241 train_loss= 450.65637 feature_loss= 12.18580 structure_loss= 438.47058 accuracy= 0.66170 accuracy_s= 0.77359 accuracy_f= 0.84235 time= 0.08257
Epoch: 0251 train_loss= 429.36252 feature_loss= 12.17279 structure_loss= 417.18973 accuracy= 0.66353 accuracy_s= 0.77359 accuracy_f= 0.84209 time= 0.08260
Epoch: 0261 train_loss= 417.91327 feature_loss= 12.17237 structure_loss= 405.74091 accuracy= 0.66405 accuracy_s= 0.77255 accuracy_f= 0.84288 time= 0.08250
Epoch: 0271 train_loss= 398.46576 feature_loss= 12.10693 structure_loss= 386.35883 accuracy= 0.66405 accuracy_s= 0.77281 accuracy_f= 0.84654 time= 0.08291
Epoch: 0281 train_loss= 384.78421 feature_loss= 12.07794 structure_loss= 372.70627 accuracy= 0.66353 accuracy_s= 0.77046 accuracy_f= 0.84680 time= 0.08374
Epoch: 0291 train_loss= 372.13168 feature_loss= 12.09214 structure_loss= 360.03955 accuracy= 0.66275 accuracy_s= 0.77072 accuracy_f= 0.84288 time= 0.08237

accuracy 0.66275
accuracy_s 0.76993
accuracy_f 0.84418
auc 0.53691
f1_score 0.28333
Job finished!
amazon_electronics_photo job finished!
amazon_electronics_computers experiment with fixed GCN hidden layer



Initializing normal onedecoder
Namespace(alpha=1, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', decoder=0, device=device(type='cuda'), dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 103
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
Epoch: 0001 train_loss= 24.03436 accuracy= 0.66841 time= 0.14559
Epoch: 0011 train_loss= 16.00389 accuracy= 0.67917 time= 0.12790
Epoch: 0021 train_loss= 15.23602 accuracy= 0.67728 time= 0.12817
Epoch: 0031 train_loss= 14.81781 accuracy= 0.67525 time= 0.12670
Epoch: 0041 train_loss= 14.46455 accuracy= 0.67525 time= 0.12763
Epoch: 0051 train_loss= 14.17272 accuracy= 0.67525 time= 0.12683
Epoch: 0061 train_loss= 13.90479 accuracy= 0.67801 time= 0.12609
Epoch: 0071 train_loss= 13.65330 accuracy= 0.68019 time= 0.12847
Epoch: 0081 train_loss= 13.43906 accuracy= 0.68048 time= 0.12857
Epoch: 0091 train_loss= 13.26025 accuracy= 0.68237 time= 0.12817
Epoch: 0101 train_loss= 13.08939 accuracy= 0.68485 time= 0.12714
Epoch: 0111 train_loss= 12.94070 accuracy= 0.69299 time= 0.12616
Epoch: 0121 train_loss= 12.79049 accuracy= 0.69183 time= 0.12729
Epoch: 0131 train_loss= 12.67985 accuracy= 0.69255 time= 0.12837
Epoch: 0141 train_loss= 12.57313 accuracy= 0.68935 time= 0.12897
Epoch: 0151 train_loss= 12.45573 accuracy= 0.69750 time= 0.12604
Epoch: 0161 train_loss= 12.36369 accuracy= 0.69532 time= 0.12721
Epoch: 0171 train_loss= 12.28383 accuracy= 0.70259 time= 0.12679
Epoch: 0181 train_loss= 12.20078 accuracy= 0.70230 time= 0.12669
Epoch: 0191 train_loss= 12.15900 accuracy= 0.69983 time= 0.12640
Epoch: 0201 train_loss= 12.09774 accuracy= 0.70157 time= 0.12747
Epoch: 0211 train_loss= 12.02531 accuracy= 0.70375 time= 0.12710
Epoch: 0221 train_loss= 11.99371 accuracy= 0.70608 time= 0.12680
Epoch: 0231 train_loss= 11.92605 accuracy= 0.70492 time= 0.12692
Epoch: 0241 train_loss= 11.89306 accuracy= 0.70724 time= 0.12784
Epoch: 0251 train_loss= 11.88114 accuracy= 0.70841 time= 0.12764
Epoch: 0261 train_loss= 11.82811 accuracy= 0.70564 time= 0.12817
Epoch: 0271 train_loss= 11.80913 accuracy= 0.70564 time= 0.12779
Epoch: 0281 train_loss= 11.78947 accuracy= 0.71175 time= 0.12652
Epoch: 0291 train_loss= 11.76373 accuracy= 0.70913 time= 0.12772

accuracy 0.70782
auc 0.58010
f1_score 0.33033
Job finished!



Initializing normal onedecoder
Namespace(alpha=1, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', decoder=1, device=device(type='cuda'), dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 97
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
Epoch: 0001 train_loss= 6283.00049 accuracy= 0.62711 time= 0.13692
Epoch: 0011 train_loss= 1233.47632 accuracy= 0.62376 time= 0.11958
Epoch: 0021 train_loss= 714.15100 accuracy= 0.62871 time= 0.12031
Epoch: 0031 train_loss= 541.55603 accuracy= 0.61460 time= 0.12006
Epoch: 0041 train_loss= 411.83673 accuracy= 0.61562 time= 0.11953
Epoch: 0051 train_loss= 385.35947 accuracy= 0.61504 time= 0.11874
Epoch: 0061 train_loss= 369.08270 accuracy= 0.64994 time= 0.11844
Epoch: 0071 train_loss= 362.59317 accuracy= 0.64849 time= 0.11911
Epoch: 0081 train_loss= 351.21265 accuracy= 0.64994 time= 0.11976
Epoch: 0091 train_loss= 339.19678 accuracy= 0.65736 time= 0.11962
Epoch: 0101 train_loss= 336.16046 accuracy= 0.64718 time= 0.11959
Epoch: 0111 train_loss= 328.96585 accuracy= 0.65125 time= 0.11835
Epoch: 0121 train_loss= 322.01453 accuracy= 0.65125 time= 0.11952
Epoch: 0131 train_loss= 318.48798 accuracy= 0.65183 time= 0.11989
Epoch: 0141 train_loss= 310.55289 accuracy= 0.64994 time= 0.11939
Epoch: 0151 train_loss= 307.66315 accuracy= 0.65169 time= 0.11985
Epoch: 0161 train_loss= 297.59732 accuracy= 0.64616 time= 0.11964
Epoch: 0171 train_loss= 290.44339 accuracy= 0.65009 time= 0.11885
Epoch: 0181 train_loss= 287.56378 accuracy= 0.65198 time= 0.11900
Epoch: 0191 train_loss= 286.20020 accuracy= 0.65140 time= 0.11919
Epoch: 0201 train_loss= 278.20697 accuracy= 0.65445 time= 0.11985
Epoch: 0211 train_loss= 272.32681 accuracy= 0.65067 time= 0.11954
Epoch: 0221 train_loss= 267.41388 accuracy= 0.65009 time= 0.11943
Epoch: 0231 train_loss= 259.01520 accuracy= 0.64616 time= 0.11943
Epoch: 0241 train_loss= 256.03427 accuracy= 0.64369 time= 0.11902
Epoch: 0251 train_loss= 251.16183 accuracy= 0.64602 time= 0.11882
Epoch: 0261 train_loss= 245.70816 accuracy= 0.64732 time= 0.11930
Epoch: 0271 train_loss= 239.38356 accuracy= 0.64689 time= 0.11990
Epoch: 0281 train_loss= 237.07857 accuracy= 0.64761 time= 0.11999
Epoch: 0291 train_loss= 232.35727 accuracy= 0.64296 time= 0.11982

accuracy 0.64791
auc 0.46936
f1_score 0.19300
Job finished!



Initializing normal twodecoders
Namespace(alpha=1, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', device=device(type='cuda'), dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 699
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
Epoch: 0001 train_loss= 6693.14355 feature_loss= 24.57589 structure_loss= 6668.56787 accuracy= 0.62784 accuracy_s= 0.79014 accuracy_f= 0.81908 time= 0.24544
Epoch: 0011 train_loss= 1236.04810 feature_loss= 16.96909 structure_loss= 1219.07898 accuracy= 0.62318 accuracy_s= 0.78403 accuracy_f= 0.82868 time= 0.21030
Epoch: 0021 train_loss= 725.82941 feature_loss= 15.65106 structure_loss= 710.17834 accuracy= 0.62784 accuracy_s= 0.78229 accuracy_f= 0.83057 time= 0.20903
Epoch: 0031 train_loss= 540.93439 feature_loss= 14.97653 structure_loss= 525.95782 accuracy= 0.61620 accuracy_s= 0.78214 accuracy_f= 0.83188 time= 0.21141
Epoch: 0041 train_loss= 434.74265 feature_loss= 14.51255 structure_loss= 420.23010 accuracy= 0.64791 accuracy_s= 0.78287 accuracy_f= 0.83508 time= 0.20946
Epoch: 0051 train_loss= 403.87921 feature_loss= 14.20602 structure_loss= 389.67319 accuracy= 0.66027 accuracy_s= 0.78374 accuracy_f= 0.83610 time= 0.21021
Epoch: 0061 train_loss= 393.09250 feature_loss= 13.96754 structure_loss= 379.12497 accuracy= 0.66943 accuracy_s= 0.79029 accuracy_f= 0.83682 time= 0.20898
Epoch: 0071 train_loss= 380.12637 feature_loss= 13.76095 structure_loss= 366.36542 accuracy= 0.67118 accuracy_s= 0.79276 accuracy_f= 0.83770 time= 0.20965
Epoch: 0081 train_loss= 373.15646 feature_loss= 13.57501 structure_loss= 359.58145 accuracy= 0.67307 accuracy_s= 0.79727 accuracy_f= 0.83755 time= 0.21167
Epoch: 0091 train_loss= 363.62991 feature_loss= 13.40790 structure_loss= 350.22202 accuracy= 0.67219 accuracy_s= 0.79945 accuracy_f= 0.83842 time= 0.21069
Epoch: 0101 train_loss= 359.44580 feature_loss= 13.26259 structure_loss= 346.18320 accuracy= 0.67248 accuracy_s= 0.79945 accuracy_f= 0.83930 time= 0.20962
Epoch: 0111 train_loss= 348.48853 feature_loss= 13.13237 structure_loss= 335.35614 accuracy= 0.67277 accuracy_s= 0.79974 accuracy_f= 0.83944 time= 0.21009
Epoch: 0121 train_loss= 343.79675 feature_loss= 13.03529 structure_loss= 330.76144 accuracy= 0.67307 accuracy_s= 0.80148 accuracy_f= 0.83973 time= 0.20828
Epoch: 0131 train_loss= 334.02423 feature_loss= 12.92250 structure_loss= 321.10175 accuracy= 0.67234 accuracy_s= 0.80134 accuracy_f= 0.83973 time= 0.20939
Epoch: 0141 train_loss= 328.65152 feature_loss= 12.82544 structure_loss= 315.82608 accuracy= 0.67350 accuracy_s= 0.79887 accuracy_f= 0.83988 time= 0.21061
Epoch: 0151 train_loss= 326.55731 feature_loss= 12.73714 structure_loss= 313.82019 accuracy= 0.67379 accuracy_s= 0.79945 accuracy_f= 0.84104 time= 0.20911
Epoch: 0161 train_loss= 317.05374 feature_loss= 12.64553 structure_loss= 304.40820 accuracy= 0.67379 accuracy_s= 0.80076 accuracy_f= 0.84104 time= 0.20993
Epoch: 0171 train_loss= 311.65317 feature_loss= 12.57576 structure_loss= 299.07739 accuracy= 0.67365 accuracy_s= 0.79857 accuracy_f= 0.84191 time= 0.21075
Epoch: 0181 train_loss= 308.50620 feature_loss= 12.50164 structure_loss= 296.00455 accuracy= 0.67510 accuracy_s= 0.80119 accuracy_f= 0.84206 time= 0.20972
Epoch: 0191 train_loss= 300.45935 feature_loss= 12.45125 structure_loss= 288.00809 accuracy= 0.67554 accuracy_s= 0.79930 accuracy_f= 0.84308 time= 0.21127
Epoch: 0201 train_loss= 296.56802 feature_loss= 12.40912 structure_loss= 284.15891 accuracy= 0.67467 accuracy_s= 0.79756 accuracy_f= 0.84264 time= 0.21063
Epoch: 0211 train_loss= 289.25137 feature_loss= 12.35815 structure_loss= 276.89322 accuracy= 0.67728 accuracy_s= 0.79974 accuracy_f= 0.84439 time= 0.21024
Epoch: 0221 train_loss= 282.56015 feature_loss= 12.32570 structure_loss= 270.23444 accuracy= 0.67539 accuracy_s= 0.79872 accuracy_f= 0.84570 time= 0.21018
Epoch: 0231 train_loss= 280.19366 feature_loss= 12.28370 structure_loss= 267.90997 accuracy= 0.68165 accuracy_s= 0.80134 accuracy_f= 0.84657 time= 0.21258
Epoch: 0241 train_loss= 277.08765 feature_loss= 12.25409 structure_loss= 264.83356 accuracy= 0.67917 accuracy_s= 0.80119 accuracy_f= 0.84628 time= 0.21066
Epoch: 0251 train_loss= 270.01071 feature_loss= 12.22578 structure_loss= 257.78494 accuracy= 0.67990 accuracy_s= 0.79639 accuracy_f= 0.84526 time= 0.21012
Epoch: 0261 train_loss= 263.33472 feature_loss= 12.21167 structure_loss= 251.12305 accuracy= 0.68150 accuracy_s= 0.80221 accuracy_f= 0.84788 time= 0.21266
Epoch: 0271 train_loss= 259.03152 feature_loss= 12.17661 structure_loss= 246.85492 accuracy= 0.68426 accuracy_s= 0.79988 accuracy_f= 0.84642 time= 0.21075
Epoch: 0281 train_loss= 254.32358 feature_loss= 12.17701 structure_loss= 242.14658 accuracy= 0.68019 accuracy_s= 0.79654 accuracy_f= 0.84613 time= 0.21009
Epoch: 0291 train_loss= 251.38083 feature_loss= 12.15872 structure_loss= 239.22211 accuracy= 0.67946 accuracy_s= 0.80119 accuracy_f= 0.84686 time= 0.21179

accuracy 0.68106
accuracy_s 0.79945
accuracy_f 0.84802
auc 0.56486
f1_score 0.26900
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', decoder=1, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 61
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
using wavelet scattering transform
y_features shape after scatting (13752, 9971) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([13752, 191]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 11964.86816 accuracy= 0.65736 time= 0.10258
Epoch: 0011 train_loss= 7064.08301 accuracy= 0.63903 time= 0.08058
Epoch: 0021 train_loss= 5247.47412 accuracy= 0.63191 time= 0.08075
Epoch: 0031 train_loss= 4178.55225 accuracy= 0.63074 time= 0.08076
Epoch: 0041 train_loss= 3468.48120 accuracy= 0.63438 time= 0.08086
Epoch: 0051 train_loss= 2972.16382 accuracy= 0.63453 time= 0.08080
Epoch: 0061 train_loss= 2635.55103 accuracy= 0.63642 time= 0.08161
Epoch: 0071 train_loss= 2388.90430 accuracy= 0.63831 time= 0.08109
Epoch: 0081 train_loss= 2196.78418 accuracy= 0.64078 time= 0.08072
Epoch: 0091 train_loss= 2029.12537 accuracy= 0.64078 time= 0.08050
Epoch: 0101 train_loss= 1887.78320 accuracy= 0.64151 time= 0.08099
Epoch: 0111 train_loss= 1756.60950 accuracy= 0.64383 time= 0.08058
Epoch: 0121 train_loss= 1639.72156 accuracy= 0.64136 time= 0.08074
Epoch: 0131 train_loss= 1535.84741 accuracy= 0.64209 time= 0.08045
Epoch: 0141 train_loss= 1451.39392 accuracy= 0.64180 time= 0.08072
Epoch: 0151 train_loss= 1376.98425 accuracy= 0.64325 time= 0.08101
Epoch: 0161 train_loss= 1313.21802 accuracy= 0.64252 time= 0.08080
Epoch: 0171 train_loss= 1256.14807 accuracy= 0.64296 time= 0.08078
Epoch: 0181 train_loss= 1200.26624 accuracy= 0.64340 time= 0.07975
Epoch: 0191 train_loss= 1154.33557 accuracy= 0.64311 time= 0.08050
Epoch: 0201 train_loss= 1107.45093 accuracy= 0.64252 time= 0.08092
Epoch: 0211 train_loss= 1068.41223 accuracy= 0.64340 time= 0.08087
Epoch: 0221 train_loss= 1024.14771 accuracy= 0.64311 time= 0.08133
Epoch: 0231 train_loss= 992.92773 accuracy= 0.64412 time= 0.08091
Epoch: 0241 train_loss= 955.09198 accuracy= 0.64412 time= 0.08087
Epoch: 0251 train_loss= 919.42047 accuracy= 0.64471 time= 0.08055
Epoch: 0261 train_loss= 891.17316 accuracy= 0.64427 time= 0.08097
Epoch: 0271 train_loss= 857.38354 accuracy= 0.64500 time= 0.08142
Epoch: 0281 train_loss= 831.01227 accuracy= 0.64442 time= 0.08061
Epoch: 0291 train_loss= 803.70648 accuracy= 0.64529 time= 0.08066

accuracy 0.64456
auc 0.48097
f1_score 0.18533
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', decoder=0, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 313
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
using wavelet scattering transform
y_features shape after scatting (13752, 9971) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([13752, 191]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 27.55266 accuracy= 0.64034 time= 0.11328
Epoch: 0011 train_loss= 21.41609 accuracy= 0.67045 time= 0.08846
Epoch: 0021 train_loss= 19.27056 accuracy= 0.67656 time= 0.08834
Epoch: 0031 train_loss= 16.66891 accuracy= 0.67874 time= 0.08839
Epoch: 0041 train_loss= 15.12188 accuracy= 0.68106 time= 0.08571
Epoch: 0051 train_loss= 14.42293 accuracy= 0.68368 time= 0.08613
Epoch: 0061 train_loss= 14.02640 accuracy= 0.68514 time= 0.08613
Epoch: 0071 train_loss= 13.78930 accuracy= 0.68572 time= 0.08640
Epoch: 0081 train_loss= 13.57244 accuracy= 0.68717 time= 0.08591
Epoch: 0091 train_loss= 13.30707 accuracy= 0.68790 time= 0.08631
Epoch: 0101 train_loss= 13.11760 accuracy= 0.68848 time= 0.08991
Epoch: 0111 train_loss= 12.96742 accuracy= 0.68805 time= 0.08602
Epoch: 0121 train_loss= 12.82674 accuracy= 0.68950 time= 0.08648
Epoch: 0131 train_loss= 12.71251 accuracy= 0.68906 time= 0.08641
Epoch: 0141 train_loss= 12.62045 accuracy= 0.68965 time= 0.08627
Epoch: 0151 train_loss= 12.53052 accuracy= 0.69110 time= 0.08619
Epoch: 0161 train_loss= 12.45665 accuracy= 0.69212 time= 0.08676
Epoch: 0171 train_loss= 12.38547 accuracy= 0.69284 time= 0.08707
Epoch: 0181 train_loss= 12.32635 accuracy= 0.69299 time= 0.08743
Epoch: 0191 train_loss= 12.27467 accuracy= 0.69314 time= 0.08783
Epoch: 0201 train_loss= 12.22295 accuracy= 0.69648 time= 0.08746
Epoch: 0211 train_loss= 12.18308 accuracy= 0.69604 time= 0.08731
Epoch: 0221 train_loss= 12.15145 accuracy= 0.69808 time= 0.08786
Epoch: 0231 train_loss= 12.11139 accuracy= 0.69793 time= 0.08807
Epoch: 0241 train_loss= 12.08252 accuracy= 0.70070 time= 0.08762
Epoch: 0251 train_loss= 12.05819 accuracy= 0.69997 time= 0.08684
Epoch: 0261 train_loss= 12.04281 accuracy= 0.69953 time= 0.08689
Epoch: 0271 train_loss= 12.01154 accuracy= 0.70041 time= 0.08745
Epoch: 0281 train_loss= 11.98133 accuracy= 0.70303 time= 0.08761
Epoch: 0291 train_loss= 11.96016 accuracy= 0.70303 time= 0.08800

accuracy 0.70230
auc 0.56928
f1_score 0.31767
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', decoder=1, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 140
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
using wavelet scattering transform
y_features shape after scatting (13752, 9971) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 40 steps!

y_features shape after FMS torch.Size([13752, 191]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 12036.82812 accuracy= 0.66099 time= 0.10134
Epoch: 0011 train_loss= 7338.69043 accuracy= 0.64282 time= 0.07980
Epoch: 0021 train_loss= 5224.46289 accuracy= 0.63351 time= 0.08054
Epoch: 0031 train_loss= 4110.94531 accuracy= 0.63089 time= 0.07929
Epoch: 0041 train_loss= 3402.68164 accuracy= 0.62987 time= 0.07872
Epoch: 0051 train_loss= 2897.28174 accuracy= 0.63074 time= 0.07973
Epoch: 0061 train_loss= 2562.59375 accuracy= 0.63365 time= 0.08037
Epoch: 0071 train_loss= 2285.39819 accuracy= 0.63627 time= 0.08001
Epoch: 0081 train_loss= 2065.87964 accuracy= 0.63714 time= 0.07878
Epoch: 0091 train_loss= 1876.76538 accuracy= 0.63743 time= 0.08021
Epoch: 0101 train_loss= 1707.28333 accuracy= 0.63787 time= 0.08028
Epoch: 0111 train_loss= 1563.23743 accuracy= 0.63933 time= 0.07974
Epoch: 0121 train_loss= 1437.58240 accuracy= 0.64078 time= 0.07889
Epoch: 0131 train_loss= 1326.45544 accuracy= 0.64107 time= 0.07971
Epoch: 0141 train_loss= 1225.21484 accuracy= 0.64063 time= 0.08045
Epoch: 0151 train_loss= 1127.55298 accuracy= 0.63947 time= 0.07850
Epoch: 0161 train_loss= 1043.02905 accuracy= 0.63845 time= 0.07841
Epoch: 0171 train_loss= 961.63116 accuracy= 0.63802 time= 0.07910
Epoch: 0181 train_loss= 901.98132 accuracy= 0.63773 time= 0.07958
Epoch: 0191 train_loss= 845.49878 accuracy= 0.63758 time= 0.08005
Epoch: 0201 train_loss= 783.72662 accuracy= 0.63598 time= 0.07860
Epoch: 0211 train_loss= 738.45239 accuracy= 0.63642 time= 0.07874
Epoch: 0221 train_loss= 694.25354 accuracy= 0.63671 time= 0.07998
Epoch: 0231 train_loss= 652.92700 accuracy= 0.63743 time= 0.07977
Epoch: 0241 train_loss= 618.49023 accuracy= 0.64034 time= 0.07878
Epoch: 0251 train_loss= 588.32782 accuracy= 0.64049 time= 0.07909
Epoch: 0261 train_loss= 559.60254 accuracy= 0.64165 time= 0.07972
Epoch: 0271 train_loss= 528.43353 accuracy= 0.64354 time= 0.08053
Epoch: 0281 train_loss= 507.03995 accuracy= 0.64543 time= 0.07933
Epoch: 0291 train_loss= 485.80457 accuracy= 0.64427 time= 0.08035

accuracy 0.64456
auc 0.50820
f1_score 0.18533
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', decoder=0, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 17
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
using wavelet scattering transform
y_features shape after scatting (13752, 9971) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 40 steps!

y_features shape after FMS torch.Size([13752, 191]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 28.07869 accuracy= 0.63874 time= 0.12116
Epoch: 0011 train_loss= 21.87185 accuracy= 0.66899 time= 0.08624
Epoch: 0021 train_loss= 20.00978 accuracy= 0.67219 time= 0.08854
Epoch: 0031 train_loss= 18.63739 accuracy= 0.67627 time= 0.08672
Epoch: 0041 train_loss= 16.98698 accuracy= 0.67976 time= 0.08652
Epoch: 0051 train_loss= 15.43613 accuracy= 0.67961 time= 0.08541
Epoch: 0061 train_loss= 14.66437 accuracy= 0.68194 time= 0.08408
Epoch: 0071 train_loss= 14.33777 accuracy= 0.68266 time= 0.08418
Epoch: 0081 train_loss= 14.00635 accuracy= 0.68237 time= 0.08787
Epoch: 0091 train_loss= 13.75724 accuracy= 0.68237 time= 0.08702
Epoch: 0101 train_loss= 13.57117 accuracy= 0.68368 time= 0.08503
Epoch: 0111 train_loss= 13.42411 accuracy= 0.68485 time= 0.08564
Epoch: 0121 train_loss= 13.30947 accuracy= 0.68557 time= 0.08687
Epoch: 0131 train_loss= 13.19756 accuracy= 0.68557 time= 0.08576
Epoch: 0141 train_loss= 13.08666 accuracy= 0.68834 time= 0.08524
Epoch: 0151 train_loss= 12.95872 accuracy= 0.68921 time= 0.08683
Epoch: 0161 train_loss= 12.84385 accuracy= 0.69066 time= 0.08529
Epoch: 0171 train_loss= 12.71002 accuracy= 0.69255 time= 0.08702
Epoch: 0181 train_loss= 12.59062 accuracy= 0.69299 time= 0.08648
Epoch: 0191 train_loss= 12.49930 accuracy= 0.69386 time= 0.08666
Epoch: 0201 train_loss= 12.42126 accuracy= 0.69386 time= 0.08751
Epoch: 0211 train_loss= 12.32388 accuracy= 0.69357 time= 0.08695
Epoch: 0221 train_loss= 12.27011 accuracy= 0.69575 time= 0.08578
Epoch: 0231 train_loss= 12.22191 accuracy= 0.69503 time= 0.08788
Epoch: 0241 train_loss= 12.17594 accuracy= 0.69721 time= 0.08676
Epoch: 0251 train_loss= 12.13155 accuracy= 0.69779 time= 0.08572
Epoch: 0261 train_loss= 12.09665 accuracy= 0.69837 time= 0.08567
Epoch: 0271 train_loss= 12.06552 accuracy= 0.69895 time= 0.08665
Epoch: 0281 train_loss= 12.03948 accuracy= 0.69939 time= 0.08624
Epoch: 0291 train_loss= 12.02376 accuracy= 0.70012 time= 0.08583

accuracy 0.69939
auc 0.56618
f1_score 0.31100
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', decoder=1, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 663
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
using wavelet scattering transform
y_features shape after scatting (13752, 9971) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([13752, 191]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 3784.75757 accuracy= 0.66434 time= 0.17958
Epoch: 0011 train_loss= 123.67747 accuracy= 0.63714 time= 0.14574
Epoch: 0021 train_loss= 67.85686 accuracy= 0.63598 time= 0.14627
Epoch: 0031 train_loss= 24.98379 accuracy= 0.63569 time= 0.14581
Epoch: 0041 train_loss= 6.47402 accuracy= 0.64965 time= 0.14490
Epoch: 0051 train_loss= 2.02172 accuracy= 0.62289 time= 0.14550
Epoch: 0061 train_loss= 0.83513 accuracy= 0.62115 time= 0.14582
Epoch: 0071 train_loss= 0.51006 accuracy= 0.61998 time= 0.14510
Epoch: 0081 train_loss= 0.37824 accuracy= 0.61896 time= 0.14766
Epoch: 0091 train_loss= 0.33008 accuracy= 0.61736 time= 0.14537
Epoch: 0101 train_loss= 0.30171 accuracy= 0.61693 time= 0.14728
Epoch: 0111 train_loss= 0.28051 accuracy= 0.61678 time= 0.14580
Epoch: 0121 train_loss= 0.26403 accuracy= 0.61533 time= 0.14617
Epoch: 0131 train_loss= 0.25435 accuracy= 0.61533 time= 0.14581
Epoch: 0141 train_loss= 0.24293 accuracy= 0.61489 time= 0.14611
Epoch: 0151 train_loss= 0.23574 accuracy= 0.61489 time= 0.14602
Epoch: 0161 train_loss= 0.22974 accuracy= 0.61489 time= 0.14589
Epoch: 0171 train_loss= 0.22457 accuracy= 0.61504 time= 0.14591
Epoch: 0181 train_loss= 0.22156 accuracy= 0.61518 time= 0.14649
Epoch: 0191 train_loss= 0.21804 accuracy= 0.61504 time= 0.14597
Epoch: 0201 train_loss= 0.21535 accuracy= 0.61504 time= 0.14606
Epoch: 0211 train_loss= 0.21168 accuracy= 0.61504 time= 0.14624
Epoch: 0221 train_loss= 0.21059 accuracy= 0.61504 time= 0.14662
Epoch: 0231 train_loss= 0.20885 accuracy= 0.61504 time= 0.14677
Epoch: 0241 train_loss= 0.20644 accuracy= 0.61533 time= 0.14626
Epoch: 0251 train_loss= 0.20553 accuracy= 0.61518 time= 0.14682
Epoch: 0261 train_loss= 0.20449 accuracy= 0.61518 time= 0.14677
Epoch: 0271 train_loss= 0.20404 accuracy= 0.61518 time= 0.14623
Epoch: 0281 train_loss= 0.20238 accuracy= 0.61518 time= 0.14654
Epoch: 0291 train_loss= 0.20164 accuracy= 0.61518 time= 0.14655

accuracy 0.61518
auc 0.36189
f1_score 0.11800
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', decoder=0, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 443
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
using wavelet scattering transform
y_features shape after scatting (13752, 9971) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([13752, 191]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 28.09623 accuracy= 0.63729 time= 0.18568
Epoch: 0011 train_loss= 18.62989 accuracy= 0.67583 time= 0.15310
Epoch: 0021 train_loss= 16.56777 accuracy= 0.67801 time= 0.15431
Epoch: 0031 train_loss= 15.32613 accuracy= 0.67845 time= 0.15345
Epoch: 0041 train_loss= 14.95928 accuracy= 0.67874 time= 0.15321
Epoch: 0051 train_loss= 14.72170 accuracy= 0.67917 time= 0.15367
Epoch: 0061 train_loss= 14.50847 accuracy= 0.67917 time= 0.15382
Epoch: 0071 train_loss= 18.07309 accuracy= 0.66987 time= 0.14984
Epoch: 0081 train_loss= 14.17993 accuracy= 0.67903 time= 0.15412
Epoch: 0091 train_loss= 14.01776 accuracy= 0.67990 time= 0.15380
Epoch: 0101 train_loss= 13.87348 accuracy= 0.68005 time= 0.15502
Epoch: 0111 train_loss= 13.70891 accuracy= 0.68019 time= 0.15437
Epoch: 0121 train_loss= 13.55256 accuracy= 0.68034 time= 0.15426
Epoch: 0131 train_loss= 13.35782 accuracy= 0.67946 time= 0.15493
Epoch: 0141 train_loss= 13.26556 accuracy= 0.67917 time= 0.15478
Epoch: 0151 train_loss= 13.18248 accuracy= 0.67932 time= 0.15453
Epoch: 0161 train_loss= 13.12334 accuracy= 0.67961 time= 0.15369
Epoch: 0171 train_loss= 13.07418 accuracy= 0.67946 time= 0.15529
Epoch: 0181 train_loss= 13.03166 accuracy= 0.67903 time= 0.15398
Epoch: 0191 train_loss= 12.99422 accuracy= 0.67961 time= 0.15383
Epoch: 0201 train_loss= 12.95637 accuracy= 0.67976 time= 0.15353
Epoch: 0211 train_loss= 12.92404 accuracy= 0.68034 time= 0.15424
Epoch: 0221 train_loss= 12.89619 accuracy= 0.68048 time= 0.15402
Epoch: 0231 train_loss= 12.87162 accuracy= 0.68106 time= 0.15450
Epoch: 0241 train_loss= 12.84846 accuracy= 0.68106 time= 0.15464
Epoch: 0251 train_loss= 13.05559 accuracy= 0.68106 time= 0.15473
Epoch: 0261 train_loss= 12.82318 accuracy= 0.68063 time= 0.15434
Epoch: 0271 train_loss= 12.80679 accuracy= 0.68034 time= 0.15447
Epoch: 0281 train_loss= 12.78829 accuracy= 0.68121 time= 0.15502
Epoch: 0291 train_loss= 12.76720 accuracy= 0.68136 time= 0.15461

accuracy 0.68150
auc 0.51994
f1_score 0.27000
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', decoder=1, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 964
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
using wavelet scattering transform
y_features shape after scatting (13752, 9971) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 41 steps!

y_features shape after FMS torch.Size([13752, 191]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 40.61416 accuracy= 0.66608 time= 0.18146
Epoch: 0011 train_loss= 0.94931 accuracy= 0.61911 time= 0.14520
Epoch: 0021 train_loss= 0.48910 accuracy= 0.61635 time= 0.14506
Epoch: 0031 train_loss= 0.30030 accuracy= 0.61504 time= 0.14582
Epoch: 0041 train_loss= 0.23671 accuracy= 0.61533 time= 0.14566
Epoch: 0051 train_loss= 0.21449 accuracy= 0.61533 time= 0.14541
Epoch: 0061 train_loss= 0.20695 accuracy= 0.61489 time= 0.14559
Epoch: 0071 train_loss= 0.20339 accuracy= 0.61504 time= 0.14619
Epoch: 0081 train_loss= 0.20172 accuracy= 0.61489 time= 0.14568
Epoch: 0091 train_loss= 0.20085 accuracy= 0.61475 time= 0.14581
Epoch: 0101 train_loss= 0.20023 accuracy= 0.61504 time= 0.14620
Epoch: 0111 train_loss= 0.19974 accuracy= 0.61489 time= 0.14658
Epoch: 0121 train_loss= 0.19920 accuracy= 0.61504 time= 0.14600
Epoch: 0131 train_loss= 0.19902 accuracy= 0.61489 time= 0.14630
Epoch: 0141 train_loss= 0.19872 accuracy= 0.61489 time= 0.14627
Epoch: 0151 train_loss= 0.19849 accuracy= 0.61475 time= 0.14632
Epoch: 0161 train_loss= 0.19823 accuracy= 0.61475 time= 0.14596
Epoch: 0171 train_loss= 0.19806 accuracy= 0.61460 time= 0.14588
Epoch: 0181 train_loss= 0.19789 accuracy= 0.61475 time= 0.14616
Epoch: 0191 train_loss= 0.19779 accuracy= 0.61475 time= 0.14649
Epoch: 0201 train_loss= 0.19762 accuracy= 0.61475 time= 0.14586
Epoch: 0211 train_loss= 0.19759 accuracy= 0.61460 time= 0.14642
Epoch: 0221 train_loss= 0.19750 accuracy= 0.61489 time= 0.14713
Epoch: 0231 train_loss= 0.19740 accuracy= 0.61489 time= 0.14698
Epoch: 0241 train_loss= 0.19740 accuracy= 0.61489 time= 0.14754
Epoch: 0251 train_loss= 0.19732 accuracy= 0.61489 time= 0.14675
Epoch: 0261 train_loss= 0.19730 accuracy= 0.61475 time= 0.14628
Epoch: 0271 train_loss= 0.19722 accuracy= 0.61475 time= 0.14743
Epoch: 0281 train_loss= 0.19723 accuracy= 0.61489 time= 0.14753
Epoch: 0291 train_loss= 0.19716 accuracy= 0.61489 time= 0.14626

accuracy 0.61489
auc 0.36370
f1_score 0.11733
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', decoder=0, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 654
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
using wavelet scattering transform
y_features shape after scatting (13752, 9971) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 47 steps!

y_features shape after FMS torch.Size([13752, 191]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 28.07421 accuracy= 0.63525 time= 0.18891
Epoch: 0011 train_loss= 18.42350 accuracy= 0.67205 time= 0.15261
Epoch: 0021 train_loss= 14.91207 accuracy= 0.67685 time= 0.15172
Epoch: 0031 train_loss= 14.35930 accuracy= 0.67830 time= 0.15052
Epoch: 0041 train_loss= 14.11747 accuracy= 0.67816 time= 0.15214
Epoch: 0051 train_loss= 13.92547 accuracy= 0.67816 time= 0.15210
Epoch: 0061 train_loss= 13.75576 accuracy= 0.67816 time= 0.15169
Epoch: 0071 train_loss= 13.60652 accuracy= 0.67816 time= 0.15243
Epoch: 0081 train_loss= 13.47579 accuracy= 0.67816 time= 0.15255
Epoch: 0091 train_loss= 13.36159 accuracy= 0.67801 time= 0.15242
Epoch: 0101 train_loss= 13.26202 accuracy= 0.67801 time= 0.15234
Epoch: 0111 train_loss= 13.17533 accuracy= 0.67801 time= 0.15262
Epoch: 0121 train_loss= 13.09993 accuracy= 0.67816 time= 0.15340
Epoch: 0131 train_loss= 13.03438 accuracy= 0.67816 time= 0.15310
Epoch: 0141 train_loss= 12.97747 accuracy= 0.67816 time= 0.15339
Epoch: 0151 train_loss= 12.92808 accuracy= 0.67801 time= 0.15256
Epoch: 0161 train_loss= 12.88527 accuracy= 0.67816 time= 0.15336
Epoch: 0171 train_loss= 12.84819 accuracy= 0.67801 time= 0.15353
Epoch: 0181 train_loss= 12.81612 accuracy= 0.67801 time= 0.15359
Epoch: 0191 train_loss= 12.78841 accuracy= 0.67801 time= 0.15290
Epoch: 0201 train_loss= 12.76450 accuracy= 0.67801 time= 0.15356
Epoch: 0211 train_loss= 12.74390 accuracy= 0.67772 time= 0.15192
Epoch: 0221 train_loss= 12.72619 accuracy= 0.67772 time= 0.15308
Epoch: 0231 train_loss= 12.71097 accuracy= 0.67787 time= 0.15274
Epoch: 0241 train_loss= 12.69792 accuracy= 0.67787 time= 0.15348
Epoch: 0251 train_loss= 12.68675 accuracy= 0.67787 time= 0.15340
Epoch: 0261 train_loss= 12.67721 accuracy= 0.67787 time= 0.15323
Epoch: 0271 train_loss= 12.66908 accuracy= 0.67801 time= 0.15220
Epoch: 0281 train_loss= 12.66215 accuracy= 0.67816 time= 0.15399
Epoch: 0291 train_loss= 12.65627 accuracy= 0.67816 time= 0.15369

accuracy 0.67830
auc 0.49443
f1_score 0.26267
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', decoder=1, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 165
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
using wavelet scattering transform
y_features shape after scatting (13752, 9971) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([13752, 191]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 3321.29688 accuracy= 0.66434 time= 0.10263
Epoch: 0011 train_loss= 93.28440 accuracy= 0.63933 time= 0.08117
Epoch: 0021 train_loss= 51.70084 accuracy= 0.63933 time= 0.08138
Epoch: 0031 train_loss= 23.02707 accuracy= 0.63991 time= 0.08092
Epoch: 0041 train_loss= 8.68553 accuracy= 0.64761 time= 0.08117
Epoch: 0051 train_loss= 3.44388 accuracy= 0.63860 time= 0.08087
Epoch: 0061 train_loss= 1.50972 accuracy= 0.62391 time= 0.08075
Epoch: 0071 train_loss= 0.86416 accuracy= 0.62187 time= 0.08077
Epoch: 0081 train_loss= 0.55302 accuracy= 0.62071 time= 0.08112
Epoch: 0091 train_loss= 0.40059 accuracy= 0.61838 time= 0.08114
Epoch: 0101 train_loss= 0.31440 accuracy= 0.61795 time= 0.08107
Epoch: 0111 train_loss= 0.27350 accuracy= 0.61722 time= 0.08119
Epoch: 0121 train_loss= 0.25175 accuracy= 0.61678 time= 0.08131
Epoch: 0131 train_loss= 0.23674 accuracy= 0.61620 time= 0.08114
Epoch: 0141 train_loss= 0.22913 accuracy= 0.61547 time= 0.08087
Epoch: 0151 train_loss= 0.22286 accuracy= 0.61547 time= 0.08084
Epoch: 0161 train_loss= 0.22466 accuracy= 0.61504 time= 0.08101
Epoch: 0171 train_loss= 0.21892 accuracy= 0.61475 time= 0.08086
Epoch: 0181 train_loss= 0.22095 accuracy= 0.61489 time= 0.08441
Epoch: 0191 train_loss= 0.21121 accuracy= 0.61460 time= 0.08119
Epoch: 0201 train_loss= 0.20800 accuracy= 0.61460 time= 0.08111
Epoch: 0211 train_loss= 0.20492 accuracy= 0.61446 time= 0.08117
Epoch: 0221 train_loss= 0.20459 accuracy= 0.61446 time= 0.08116
Epoch: 0231 train_loss= 0.20332 accuracy= 0.61460 time= 0.08125
Epoch: 0241 train_loss= 0.20253 accuracy= 0.61460 time= 0.08127
Epoch: 0251 train_loss= 0.21081 accuracy= 0.61460 time= 0.08092
Epoch: 0261 train_loss= 0.20550 accuracy= 0.61431 time= 0.08093
Epoch: 0271 train_loss= 0.20312 accuracy= 0.61446 time= 0.08095
Epoch: 0281 train_loss= 0.20196 accuracy= 0.61460 time= 0.08113
Epoch: 0291 train_loss= 0.20073 accuracy= 0.61446 time= 0.08102

accuracy 0.61431
auc 0.36211
f1_score 0.11600
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', decoder=0, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 261
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
using wavelet scattering transform
y_features shape after scatting (13752, 9971) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([13752, 191]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 28.51215 accuracy= 0.63642 time= 0.11435
Epoch: 0011 train_loss= 19.08119 accuracy= 0.67481 time= 0.08894
Epoch: 0021 train_loss= 17.40448 accuracy= 0.67714 time= 0.08683
Epoch: 0031 train_loss= 15.93516 accuracy= 0.67859 time= 0.08706
Epoch: 0041 train_loss= 15.47716 accuracy= 0.67845 time= 0.08990
Epoch: 0051 train_loss= 15.19609 accuracy= 0.67845 time= 0.08786
Epoch: 0061 train_loss= 14.94552 accuracy= 0.67976 time= 0.08840
Epoch: 0071 train_loss= 14.70980 accuracy= 0.68092 time= 0.08944
Epoch: 0081 train_loss= 14.48781 accuracy= 0.68121 time= 0.08831
Epoch: 0091 train_loss= 14.27651 accuracy= 0.68252 time= 0.08764
Epoch: 0101 train_loss= 14.06750 accuracy= 0.68354 time= 0.09002
Epoch: 0111 train_loss= 13.87959 accuracy= 0.68397 time= 0.08969
Epoch: 0121 train_loss= 13.73314 accuracy= 0.68397 time= 0.09073
Epoch: 0131 train_loss= 13.31803 accuracy= 0.68339 time= 0.09013
Epoch: 0141 train_loss= 13.15124 accuracy= 0.68819 time= 0.09031
Epoch: 0151 train_loss= 12.98203 accuracy= 0.69081 time= 0.08870
Epoch: 0161 train_loss= 12.85982 accuracy= 0.69226 time= 0.08864
Epoch: 0171 train_loss= 12.72676 accuracy= 0.69212 time= 0.08843
Epoch: 0181 train_loss= 12.62196 accuracy= 0.69604 time= 0.08984
Epoch: 0191 train_loss= 12.54736 accuracy= 0.69677 time= 0.09154
Epoch: 0201 train_loss= 12.46288 accuracy= 0.69953 time= 0.08857
Epoch: 0211 train_loss= 12.39144 accuracy= 0.70012 time= 0.09099
Epoch: 0221 train_loss= 12.35065 accuracy= 0.70128 time= 0.08864
Epoch: 0231 train_loss= 12.30916 accuracy= 0.70026 time= 0.08896
Epoch: 0241 train_loss= 12.24862 accuracy= 0.69997 time= 0.09160
Epoch: 0251 train_loss= 12.20023 accuracy= 0.70055 time= 0.08819
Epoch: 0261 train_loss= 12.17300 accuracy= 0.70143 time= 0.09093
Epoch: 0271 train_loss= 12.16841 accuracy= 0.70230 time= 0.09103
Epoch: 0281 train_loss= 12.11239 accuracy= 0.70273 time= 0.08924
Epoch: 0291 train_loss= 12.06519 accuracy= 0.70433 time= 0.08949

accuracy 0.70215
auc 0.57877
f1_score 0.31733
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', decoder=1, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 833
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
using wavelet scattering transform
y_features shape after scatting (13752, 9971) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 40 steps!

y_features shape after FMS torch.Size([13752, 191]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 26.32721 accuracy= 0.66652 time= 0.11509
Epoch: 0011 train_loss= 4.91346 accuracy= 0.66405 time= 0.08122
Epoch: 0021 train_loss= 1.11636 accuracy= 0.65445 time= 0.08202
Epoch: 0031 train_loss= 0.49376 accuracy= 0.63802 time= 0.08084
Epoch: 0041 train_loss= 0.32319 accuracy= 0.63016 time= 0.08100
Epoch: 0051 train_loss= 0.25701 accuracy= 0.62158 time= 0.08061
Epoch: 0061 train_loss= 0.22901 accuracy= 0.61853 time= 0.08126
Epoch: 0071 train_loss= 0.21669 accuracy= 0.61707 time= 0.08138
Epoch: 0081 train_loss= 0.21062 accuracy= 0.61649 time= 0.08124
Epoch: 0091 train_loss= 0.20729 accuracy= 0.61635 time= 0.08080
Epoch: 0101 train_loss= 0.20525 accuracy= 0.61620 time= 0.08155
Epoch: 0111 train_loss= 0.20378 accuracy= 0.61649 time= 0.08210
Epoch: 0121 train_loss= 0.20262 accuracy= 0.61664 time= 0.08122
Epoch: 0131 train_loss= 0.20164 accuracy= 0.61591 time= 0.08118
Epoch: 0141 train_loss= 0.20078 accuracy= 0.61562 time= 0.08116
Epoch: 0151 train_loss= 0.20009 accuracy= 0.61547 time= 0.08121
Epoch: 0161 train_loss= 0.19946 accuracy= 0.61533 time= 0.08112
Epoch: 0171 train_loss= 0.19907 accuracy= 0.61533 time= 0.08064
Epoch: 0181 train_loss= 0.19865 accuracy= 0.61547 time= 0.08134
Epoch: 0191 train_loss= 0.19836 accuracy= 0.61533 time= 0.08195
Epoch: 0201 train_loss= 0.19796 accuracy= 0.61547 time= 0.08127
Epoch: 0211 train_loss= 0.19768 accuracy= 0.61576 time= 0.08101
Epoch: 0221 train_loss= 0.19747 accuracy= 0.61591 time= 0.08110
Epoch: 0231 train_loss= 0.19723 accuracy= 0.61576 time= 0.08118
Epoch: 0241 train_loss= 0.19708 accuracy= 0.61562 time= 0.08110
Epoch: 0251 train_loss= 0.19687 accuracy= 0.61576 time= 0.08097
Epoch: 0261 train_loss= 0.19675 accuracy= 0.61576 time= 0.08109
Epoch: 0271 train_loss= 0.19659 accuracy= 0.61576 time= 0.08114
Epoch: 0281 train_loss= 0.19650 accuracy= 0.61576 time= 0.07983
Epoch: 0291 train_loss= 0.19636 accuracy= 0.61591 time= 0.08124

accuracy 0.61562
auc 0.35732
f1_score 0.11900
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', decoder=0, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 701
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
using wavelet scattering transform
y_features shape after scatting (13752, 9971) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 40 steps!

y_features shape after FMS torch.Size([13752, 191]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 27.69361 accuracy= 0.63656 time= 0.12263
Epoch: 0011 train_loss= 17.22990 accuracy= 0.67074 time= 0.08695
Epoch: 0021 train_loss= 14.97332 accuracy= 0.67641 time= 0.08653
Epoch: 0031 train_loss= 14.36415 accuracy= 0.67830 time= 0.08637
Epoch: 0041 train_loss= 14.13297 accuracy= 0.67830 time= 0.08784
Epoch: 0051 train_loss= 13.93722 accuracy= 0.67830 time= 0.08738
Epoch: 0061 train_loss= 13.76486 accuracy= 0.67816 time= 0.08620
Epoch: 0071 train_loss= 13.61343 accuracy= 0.67816 time= 0.08632
Epoch: 0081 train_loss= 13.46479 accuracy= 0.67801 time= 0.08670
Epoch: 0091 train_loss= 13.35127 accuracy= 0.67816 time= 0.08751
Epoch: 0101 train_loss= 13.25255 accuracy= 0.67816 time= 0.08819
Epoch: 0111 train_loss= 13.16676 accuracy= 0.67816 time= 0.08623
Epoch: 0121 train_loss= 13.09223 accuracy= 0.67816 time= 0.08624
Epoch: 0131 train_loss= 13.02754 accuracy= 0.67816 time= 0.08602
Epoch: 0141 train_loss= 12.97140 accuracy= 0.67816 time= 0.08840
Epoch: 0151 train_loss= 12.92272 accuracy= 0.67801 time= 0.08739
Epoch: 0161 train_loss= 12.88054 accuracy= 0.67816 time= 0.08809
Epoch: 0171 train_loss= 12.84403 accuracy= 0.67801 time= 0.08726
Epoch: 0181 train_loss= 12.81247 accuracy= 0.67801 time= 0.08867
Epoch: 0191 train_loss= 12.78521 accuracy= 0.67801 time= 0.08824
Epoch: 0201 train_loss= 12.76170 accuracy= 0.67801 time= 0.08841
Epoch: 0211 train_loss= 12.74146 accuracy= 0.67801 time= 0.08797
Epoch: 0221 train_loss= 12.72405 accuracy= 0.67787 time= 0.08822
Epoch: 0231 train_loss= 12.70911 accuracy= 0.67787 time= 0.08849
Epoch: 0241 train_loss= 12.69630 accuracy= 0.67787 time= 0.08723
Epoch: 0251 train_loss= 12.68535 accuracy= 0.67787 time= 0.08743
Epoch: 0261 train_loss= 12.67600 accuracy= 0.67801 time= 0.08882
Epoch: 0271 train_loss= 12.66803 accuracy= 0.67801 time= 0.08693
Epoch: 0281 train_loss= 12.66124 accuracy= 0.67816 time= 0.08898
Epoch: 0291 train_loss= 12.65549 accuracy= 0.67816 time= 0.08803

accuracy 0.67830
auc 0.49443
f1_score 0.26267
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=3, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', decoder=1, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 716
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
using wavelet scattering transform
y_features shape after scatting (13752, 9971) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([13752, 191]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 11333.03711 accuracy= 0.66259 time= 0.17543
Epoch: 0011 train_loss= 5507.40674 accuracy= 0.63074 time= 0.14447
Epoch: 0021 train_loss= 4154.59814 accuracy= 0.62260 time= 0.14498
Epoch: 0031 train_loss= 3194.56177 accuracy= 0.62420 time= 0.14515
Epoch: 0041 train_loss= 2468.46484 accuracy= 0.62696 time= 0.14418
Epoch: 0051 train_loss= 2038.13477 accuracy= 0.63424 time= 0.14385
Epoch: 0061 train_loss= 1838.37439 accuracy= 0.63613 time= 0.14479
Epoch: 0071 train_loss= 1679.09583 accuracy= 0.64412 time= 0.14693
Epoch: 0081 train_loss= 1552.54370 accuracy= 0.64820 time= 0.14428
Epoch: 0091 train_loss= 1447.05212 accuracy= 0.64092 time= 0.14412
Epoch: 0101 train_loss= 1362.97339 accuracy= 0.64252 time= 0.14452
Epoch: 0111 train_loss= 1279.01294 accuracy= 0.64282 time= 0.14403
Epoch: 0121 train_loss= 1203.89783 accuracy= 0.64267 time= 0.14422
Epoch: 0131 train_loss= 1144.69678 accuracy= 0.64514 time= 0.14480
Epoch: 0141 train_loss= 1088.87903 accuracy= 0.64296 time= 0.14459
Epoch: 0151 train_loss= 1042.23877 accuracy= 0.64369 time= 0.14514
Epoch: 0161 train_loss= 992.95685 accuracy= 0.64383 time= 0.14443
Epoch: 0171 train_loss= 947.85376 accuracy= 0.64602 time= 0.14494
Epoch: 0181 train_loss= 916.08344 accuracy= 0.64165 time= 0.14468
Epoch: 0191 train_loss= 872.33429 accuracy= 0.64223 time= 0.14482
Epoch: 0201 train_loss= 842.71613 accuracy= 0.64296 time= 0.14467
Epoch: 0211 train_loss= 816.17352 accuracy= 0.64223 time= 0.14491
Epoch: 0221 train_loss= 782.56708 accuracy= 0.64325 time= 0.14520
Epoch: 0231 train_loss= 761.33014 accuracy= 0.64151 time= 0.14593
Epoch: 0241 train_loss= 732.35199 accuracy= 0.64514 time= 0.14548
Epoch: 0251 train_loss= 713.92114 accuracy= 0.63976 time= 0.14530
Epoch: 0261 train_loss= 697.05261 accuracy= 0.64238 time= 0.14542
Epoch: 0271 train_loss= 669.58801 accuracy= 0.64325 time= 0.14487
Epoch: 0281 train_loss= 650.37604 accuracy= 0.64631 time= 0.14552
Epoch: 0291 train_loss= 632.83655 accuracy= 0.64122 time= 0.14507

accuracy 0.64660
auc 0.48293
f1_score 0.19000
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=3, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', decoder=0, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 410
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
using wavelet scattering transform
y_features shape after scatting (13752, 9971) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([13752, 191]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 26.83939 accuracy= 0.64616 time= 0.18240
Epoch: 0011 train_loss= 20.46540 accuracy= 0.67277 time= 0.15248
Epoch: 0021 train_loss= 19.00133 accuracy= 0.67437 time= 0.15223
Epoch: 0031 train_loss= 17.03287 accuracy= 0.67394 time= 0.15313
Epoch: 0041 train_loss= 15.45562 accuracy= 0.67379 time= 0.15170
Epoch: 0051 train_loss= 14.62560 accuracy= 0.67743 time= 0.15498
Epoch: 0061 train_loss= 14.10928 accuracy= 0.67946 time= 0.15550
Epoch: 0071 train_loss= 13.77253 accuracy= 0.67830 time= 0.15312
Epoch: 0081 train_loss= 13.54362 accuracy= 0.67961 time= 0.15392
Epoch: 0091 train_loss= 13.35739 accuracy= 0.68063 time= 0.15363
Epoch: 0101 train_loss= 13.19926 accuracy= 0.68121 time= 0.15247
Epoch: 0111 train_loss= 13.04433 accuracy= 0.68281 time= 0.15357
Epoch: 0121 train_loss= 12.91460 accuracy= 0.68601 time= 0.15438
Epoch: 0131 train_loss= 12.78933 accuracy= 0.68615 time= 0.15413
Epoch: 0141 train_loss= 12.68003 accuracy= 0.68703 time= 0.15397
Epoch: 0151 train_loss= 12.58906 accuracy= 0.68732 time= 0.15552
Epoch: 0161 train_loss= 12.51474 accuracy= 0.68761 time= 0.15312
Epoch: 0171 train_loss= 12.81006 accuracy= 0.68863 time= 0.15382
Epoch: 0181 train_loss= 12.72533 accuracy= 0.68106 time= 0.15439
Epoch: 0191 train_loss= 12.49996 accuracy= 0.68383 time= 0.15314
Epoch: 0201 train_loss= 12.39270 accuracy= 0.68310 time= 0.15288
Epoch: 0211 train_loss= 12.31737 accuracy= 0.68528 time= 0.15292
Epoch: 0221 train_loss= 12.26155 accuracy= 0.68732 time= 0.15693
Epoch: 0231 train_loss= 12.21667 accuracy= 0.69052 time= 0.15466
Epoch: 0241 train_loss= 12.33777 accuracy= 0.68005 time= 0.15310
Epoch: 0251 train_loss= 19.51506 accuracy= 0.67176 time= 0.15309
Epoch: 0261 train_loss= 12.36884 accuracy= 0.68136 time= 0.15334
Epoch: 0271 train_loss= 12.23092 accuracy= 0.68543 time= 0.15318
Epoch: 0281 train_loss= 12.16092 accuracy= 0.68805 time= 0.15343
Epoch: 0291 train_loss= 12.08931 accuracy= 0.69052 time= 0.15522

accuracy 0.69343
auc 0.56511
f1_score 0.29733
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=3, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', decoder=1, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 935
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
using wavelet scattering transform
y_features shape after scatting (13752, 9971) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 41 steps!

y_features shape after FMS torch.Size([13752, 191]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 11371.69141 accuracy= 0.66070 time= 0.17502
Epoch: 0011 train_loss= 5588.82227 accuracy= 0.63351 time= 0.14446
Epoch: 0021 train_loss= 4123.82764 accuracy= 0.62958 time= 0.14432
Epoch: 0031 train_loss= 3155.25171 accuracy= 0.62798 time= 0.14430
Epoch: 0041 train_loss= 2413.92700 accuracy= 0.63511 time= 0.14305
Epoch: 0051 train_loss= 1961.74109 accuracy= 0.63947 time= 0.14433
Epoch: 0061 train_loss= 1752.79529 accuracy= 0.64136 time= 0.14395
Epoch: 0071 train_loss= 1551.41101 accuracy= 0.64209 time= 0.14380
Epoch: 0081 train_loss= 1415.96875 accuracy= 0.64543 time= 0.14415
Epoch: 0091 train_loss= 1294.53589 accuracy= 0.64398 time= 0.14411
Epoch: 0101 train_loss= 1184.47375 accuracy= 0.64471 time= 0.14368
Epoch: 0111 train_loss= 1097.09656 accuracy= 0.64063 time= 0.14358
Epoch: 0121 train_loss= 1021.92456 accuracy= 0.64311 time= 0.14396
Epoch: 0131 train_loss= 951.98083 accuracy= 0.64325 time= 0.14397
Epoch: 0141 train_loss= 884.42822 accuracy= 0.64412 time= 0.14399
Epoch: 0151 train_loss= 820.79871 accuracy= 0.63845 time= 0.14421
Epoch: 0161 train_loss= 785.93408 accuracy= 0.64238 time= 0.14394
Epoch: 0171 train_loss= 737.23877 accuracy= 0.63336 time= 0.14431
Epoch: 0181 train_loss= 693.25848 accuracy= 0.63380 time= 0.14520
Epoch: 0191 train_loss= 647.32617 accuracy= 0.62914 time= 0.14471
Epoch: 0201 train_loss= 617.21313 accuracy= 0.63074 time= 0.14461
Epoch: 0211 train_loss= 600.15247 accuracy= 0.63031 time= 0.14455
Epoch: 0221 train_loss= 572.80884 accuracy= 0.63351 time= 0.14427
Epoch: 0231 train_loss= 551.44025 accuracy= 0.62798 time= 0.14491
Epoch: 0241 train_loss= 541.23975 accuracy= 0.64951 time= 0.14538
Epoch: 0251 train_loss= 511.38425 accuracy= 0.63365 time= 0.14458
Epoch: 0261 train_loss= 487.67068 accuracy= 0.63351 time= 0.14471
Epoch: 0271 train_loss= 474.26987 accuracy= 0.62522 time= 0.14538
Epoch: 0281 train_loss= 427.63956 accuracy= 0.62551 time= 0.14516
Epoch: 0291 train_loss= 392.55716 accuracy= 0.64529 time= 0.14555

accuracy 0.62755
auc 0.44059
f1_score 0.14633
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=3, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', decoder=0, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 988
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
using wavelet scattering transform
y_features shape after scatting (13752, 9971) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 41 steps!

y_features shape after FMS torch.Size([13752, 191]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 27.69283 accuracy= 0.64005 time= 0.19011
Epoch: 0011 train_loss= 20.55865 accuracy= 0.67234 time= 0.15232
Epoch: 0021 train_loss= 19.41816 accuracy= 0.67277 time= 0.15072
Epoch: 0031 train_loss= 17.74277 accuracy= 0.67525 time= 0.15127
Epoch: 0041 train_loss= 15.75423 accuracy= 0.67568 time= 0.15134
Epoch: 0051 train_loss= 14.77850 accuracy= 0.67903 time= 0.15144
Epoch: 0061 train_loss= 14.38509 accuracy= 0.67917 time= 0.15128
Epoch: 0071 train_loss= 14.12332 accuracy= 0.68019 time= 0.15081
Epoch: 0081 train_loss= 13.89365 accuracy= 0.67903 time= 0.15155
Epoch: 0091 train_loss= 13.67712 accuracy= 0.68077 time= 0.15340
Epoch: 0101 train_loss= 13.48875 accuracy= 0.68237 time= 0.15206
Epoch: 0111 train_loss= 13.31688 accuracy= 0.68223 time= 0.15067
Epoch: 0121 train_loss= 13.16124 accuracy= 0.68368 time= 0.15204
Epoch: 0131 train_loss= 13.01464 accuracy= 0.68383 time= 0.15208
Epoch: 0141 train_loss= 12.88721 accuracy= 0.68455 time= 0.15251
Epoch: 0151 train_loss= 12.77389 accuracy= 0.68645 time= 0.15273
Epoch: 0161 train_loss= 12.69697 accuracy= 0.68426 time= 0.15300
Epoch: 0171 train_loss= 12.61087 accuracy= 0.68935 time= 0.15273
Epoch: 0181 train_loss= 12.54277 accuracy= 0.69023 time= 0.15380
Epoch: 0191 train_loss= 12.47492 accuracy= 0.68703 time= 0.15216
Epoch: 0201 train_loss= 12.42636 accuracy= 0.68979 time= 0.15287
Epoch: 0211 train_loss= 12.37763 accuracy= 0.68979 time= 0.15285
Epoch: 0221 train_loss= 12.33337 accuracy= 0.69343 time= 0.15112
Epoch: 0231 train_loss= 12.29081 accuracy= 0.69255 time= 0.15303
Epoch: 0241 train_loss= 12.26050 accuracy= 0.69444 time= 0.15270
Epoch: 0251 train_loss= 12.24046 accuracy= 0.69561 time= 0.15349
Epoch: 0261 train_loss= 12.26322 accuracy= 0.69604 time= 0.15167
Epoch: 0271 train_loss= 12.19100 accuracy= 0.69444 time= 0.15395
Epoch: 0281 train_loss= 12.15699 accuracy= 0.69692 time= 0.15435
Epoch: 0291 train_loss= 12.13823 accuracy= 0.69590 time= 0.15174

accuracy 0.69488
auc 0.57015
f1_score 0.30067
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=0, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 533
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
using wavelet scattering transform
y_features shape after scatting (13752, 9971) <class 'numpy.ndarray'>
using PCA to reduce dim
y_features shape after PCA torch.Size([13752, 191]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 11595.58203 feature_loss= 27.29647 structure_loss= 11568.28516 accuracy= 0.65736 accuracy_s= 0.79450 accuracy_f= 0.80832 time= 0.19763
Epoch: 0011 train_loss= 6883.84277 feature_loss= 21.36106 structure_loss= 6862.48193 accuracy= 0.63453 accuracy_s= 0.78665 accuracy_f= 0.82054 time= 0.16295
Epoch: 0021 train_loss= 5165.75977 feature_loss= 19.35629 structure_loss= 5146.40332 accuracy= 0.63089 accuracy_s= 0.78432 accuracy_f= 0.82403 time= 0.16378
Epoch: 0031 train_loss= 4133.88574 feature_loss= 17.08147 structure_loss= 4116.80420 accuracy= 0.63118 accuracy_s= 0.78563 accuracy_f= 0.82737 time= 0.16408
Epoch: 0041 train_loss= 3429.19507 feature_loss= 15.48454 structure_loss= 3413.71045 accuracy= 0.63220 accuracy_s= 0.78592 accuracy_f= 0.83057 time= 0.16538
Epoch: 0051 train_loss= 2906.38477 feature_loss= 14.53088 structure_loss= 2891.85400 accuracy= 0.63685 accuracy_s= 0.78592 accuracy_f= 0.83319 time= 0.16467
Epoch: 0061 train_loss= 2567.47974 feature_loss= 14.14352 structure_loss= 2553.33618 accuracy= 0.63991 accuracy_s= 0.78534 accuracy_f= 0.83479 time= 0.16431
Epoch: 0071 train_loss= 2323.68896 feature_loss= 13.85264 structure_loss= 2309.83643 accuracy= 0.64034 accuracy_s= 0.78563 accuracy_f= 0.83537 time= 0.16546
Epoch: 0081 train_loss= 2129.40137 feature_loss= 13.58998 structure_loss= 2115.81128 accuracy= 0.64063 accuracy_s= 0.78549 accuracy_f= 0.83624 time= 0.16357
Epoch: 0091 train_loss= 1974.03040 feature_loss= 13.39185 structure_loss= 1960.63855 accuracy= 0.63976 accuracy_s= 0.78563 accuracy_f= 0.83726 time= 0.16401
Epoch: 0101 train_loss= 1842.45056 feature_loss= 13.23055 structure_loss= 1829.21997 accuracy= 0.63918 accuracy_s= 0.78592 accuracy_f= 0.83799 time= 0.16352
Epoch: 0111 train_loss= 1726.85425 feature_loss= 13.07903 structure_loss= 1713.77527 accuracy= 0.63918 accuracy_s= 0.78563 accuracy_f= 0.83813 time= 0.16678
Epoch: 0121 train_loss= 1622.95166 feature_loss= 12.92149 structure_loss= 1610.03015 accuracy= 0.63889 accuracy_s= 0.78578 accuracy_f= 0.83973 time= 0.16357
Epoch: 0131 train_loss= 1528.50208 feature_loss= 12.75522 structure_loss= 1515.74683 accuracy= 0.63962 accuracy_s= 0.78650 accuracy_f= 0.84031 time= 0.16374
Epoch: 0141 train_loss= 1436.49878 feature_loss= 12.65274 structure_loss= 1423.84607 accuracy= 0.64107 accuracy_s= 0.78665 accuracy_f= 0.84017 time= 0.16360
Epoch: 0151 train_loss= 1361.72180 feature_loss= 12.56398 structure_loss= 1349.15784 accuracy= 0.64369 accuracy_s= 0.78709 accuracy_f= 0.84075 time= 0.16614
Epoch: 0161 train_loss= 1298.25183 feature_loss= 12.49063 structure_loss= 1285.76123 accuracy= 0.64311 accuracy_s= 0.78767 accuracy_f= 0.84104 time= 0.16530
Epoch: 0171 train_loss= 1241.10657 feature_loss= 12.41379 structure_loss= 1228.69275 accuracy= 0.64282 accuracy_s= 0.78752 accuracy_f= 0.84279 time= 0.16776
Epoch: 0181 train_loss= 1191.12939 feature_loss= 12.34393 structure_loss= 1178.78552 accuracy= 0.64543 accuracy_s= 0.78781 accuracy_f= 0.84235 time= 0.16461
Epoch: 0191 train_loss= 1137.96289 feature_loss= 12.29246 structure_loss= 1125.67041 accuracy= 0.64602 accuracy_s= 0.78825 accuracy_f= 0.84279 time= 0.16524
Epoch: 0201 train_loss= 1101.19287 feature_loss= 12.25112 structure_loss= 1088.94177 accuracy= 0.64616 accuracy_s= 0.78825 accuracy_f= 0.84439 time= 0.16591
Epoch: 0211 train_loss= 1056.41345 feature_loss= 12.20176 structure_loss= 1044.21167 accuracy= 0.64645 accuracy_s= 0.78810 accuracy_f= 0.84453 time= 0.16403
Epoch: 0221 train_loss= 1018.71600 feature_loss= 12.15315 structure_loss= 1006.56287 accuracy= 0.64703 accuracy_s= 0.78839 accuracy_f= 0.84439 time= 0.16705
Epoch: 0231 train_loss= 983.35675 feature_loss= 12.12446 structure_loss= 971.23230 accuracy= 0.64747 accuracy_s= 0.78825 accuracy_f= 0.84511 time= 0.16810
Epoch: 0241 train_loss= 951.40540 feature_loss= 12.08954 structure_loss= 939.31586 accuracy= 0.64572 accuracy_s= 0.78810 accuracy_f= 0.84584 time= 0.16819
Epoch: 0251 train_loss= 917.44739 feature_loss= 12.05431 structure_loss= 905.39307 accuracy= 0.64689 accuracy_s= 0.78825 accuracy_f= 0.84788 time= 0.16711
Epoch: 0261 train_loss= 884.39276 feature_loss= 12.02891 structure_loss= 872.36383 accuracy= 0.64645 accuracy_s= 0.78796 accuracy_f= 0.84715 time= 0.16417
Epoch: 0271 train_loss= 854.71478 feature_loss= 12.00062 structure_loss= 842.71417 accuracy= 0.64631 accuracy_s= 0.78810 accuracy_f= 0.84715 time= 0.16890
Epoch: 0281 train_loss= 828.44684 feature_loss= 11.99027 structure_loss= 816.45654 accuracy= 0.64631 accuracy_s= 0.78825 accuracy_f= 0.84962 time= 0.16786
Epoch: 0291 train_loss= 801.08936 feature_loss= 11.97565 structure_loss= 789.11371 accuracy= 0.64616 accuracy_s= 0.78810 accuracy_f= 0.84889 time= 0.16580

accuracy 0.64761
accuracy_s 0.78825
accuracy_f 0.84860
auc 0.51684
f1_score 0.19233
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=0, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 50
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
using wavelet scattering transform
y_features shape after scatting (13752, 9971) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 47 steps!
y_features shape after FMS torch.Size([13752, 191]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 12186.66016 feature_loss= 28.04187 structure_loss= 12158.61816 accuracy= 0.66114 accuracy_s= 0.79857 accuracy_f= 0.80672 time= 0.20805
Epoch: 0011 train_loss= 7433.97998 feature_loss= 22.09313 structure_loss= 7411.88672 accuracy= 0.64427 accuracy_s= 0.79261 accuracy_f= 0.81981 time= 0.16432
Epoch: 0021 train_loss= 5310.43359 feature_loss= 20.16763 structure_loss= 5290.26611 accuracy= 0.63249 accuracy_s= 0.78592 accuracy_f= 0.82301 time= 0.16291
Epoch: 0031 train_loss= 4153.44434 feature_loss= 18.78938 structure_loss= 4134.65479 accuracy= 0.63016 accuracy_s= 0.78505 accuracy_f= 0.82504 time= 0.16468
Epoch: 0041 train_loss= 3390.64478 feature_loss= 17.14136 structure_loss= 3373.50342 accuracy= 0.62929 accuracy_s= 0.78519 accuracy_f= 0.82679 time= 0.16368
Epoch: 0051 train_loss= 2892.38623 feature_loss= 15.56724 structure_loss= 2876.81909 accuracy= 0.62914 accuracy_s= 0.78549 accuracy_f= 0.82504 time= 0.16236
Epoch: 0061 train_loss= 2552.95972 feature_loss= 14.64260 structure_loss= 2538.31714 accuracy= 0.63045 accuracy_s= 0.78534 accuracy_f= 0.82577 time= 0.16227
Epoch: 0071 train_loss= 2291.79102 feature_loss= 14.29170 structure_loss= 2277.49927 accuracy= 0.63322 accuracy_s= 0.78490 accuracy_f= 0.82548 time= 0.16507
Epoch: 0081 train_loss= 2086.09961 feature_loss= 13.93791 structure_loss= 2072.16162 accuracy= 0.63438 accuracy_s= 0.78505 accuracy_f= 0.82708 time= 0.16387
Epoch: 0091 train_loss= 1903.61926 feature_loss= 13.67992 structure_loss= 1889.93933 accuracy= 0.63540 accuracy_s= 0.78490 accuracy_f= 0.82839 time= 0.16609
Epoch: 0101 train_loss= 1749.64624 feature_loss= 13.50131 structure_loss= 1736.14490 accuracy= 0.63845 accuracy_s= 0.78534 accuracy_f= 0.82955 time= 0.16454
Epoch: 0111 train_loss= 1606.86060 feature_loss= 13.33977 structure_loss= 1593.52087 accuracy= 0.64034 accuracy_s= 0.78534 accuracy_f= 0.83188 time= 0.16467
Epoch: 0121 train_loss= 1483.32983 feature_loss= 13.17725 structure_loss= 1470.15259 accuracy= 0.64049 accuracy_s= 0.78563 accuracy_f= 0.83450 time= 0.16479
Epoch: 0131 train_loss= 1373.87842 feature_loss= 13.03345 structure_loss= 1360.84497 accuracy= 0.64151 accuracy_s= 0.78563 accuracy_f= 0.83697 time= 0.16449
Epoch: 0141 train_loss= 1273.26868 feature_loss= 12.88824 structure_loss= 1260.38049 accuracy= 0.64194 accuracy_s= 0.78578 accuracy_f= 0.83871 time= 0.16345
Epoch: 0151 train_loss= 1182.80396 feature_loss= 12.76216 structure_loss= 1170.04175 accuracy= 0.64107 accuracy_s= 0.78592 accuracy_f= 0.83930 time= 0.16560
Epoch: 0161 train_loss= 1097.29578 feature_loss= 12.65795 structure_loss= 1084.63782 accuracy= 0.64020 accuracy_s= 0.78592 accuracy_f= 0.84031 time= 0.16649
Epoch: 0171 train_loss= 1014.72534 feature_loss= 12.56985 structure_loss= 1002.15552 accuracy= 0.64005 accuracy_s= 0.78592 accuracy_f= 0.84104 time= 0.16502
Epoch: 0181 train_loss= 949.15851 feature_loss= 12.49194 structure_loss= 936.66656 accuracy= 0.64020 accuracy_s= 0.78607 accuracy_f= 0.84279 time= 0.16566
Epoch: 0191 train_loss= 889.08453 feature_loss= 12.42627 structure_loss= 876.65826 accuracy= 0.63933 accuracy_s= 0.78592 accuracy_f= 0.84293 time= 0.16513
Epoch: 0201 train_loss= 832.66461 feature_loss= 12.36656 structure_loss= 820.29803 accuracy= 0.63933 accuracy_s= 0.78636 accuracy_f= 0.84337 time= 0.16384
Epoch: 0211 train_loss= 783.98846 feature_loss= 12.30939 structure_loss= 771.67908 accuracy= 0.63962 accuracy_s= 0.78709 accuracy_f= 0.84410 time= 0.16474
Epoch: 0221 train_loss= 735.59631 feature_loss= 12.25804 structure_loss= 723.33826 accuracy= 0.63889 accuracy_s= 0.78694 accuracy_f= 0.84439 time= 0.16378
Epoch: 0231 train_loss= 693.53790 feature_loss= 12.21935 structure_loss= 681.31854 accuracy= 0.64005 accuracy_s= 0.78723 accuracy_f= 0.84555 time= 0.16806
Epoch: 0241 train_loss= 657.39124 feature_loss= 12.18240 structure_loss= 645.20886 accuracy= 0.63933 accuracy_s= 0.78781 accuracy_f= 0.84497 time= 0.16510
Epoch: 0251 train_loss= 622.27417 feature_loss= 12.14027 structure_loss= 610.13391 accuracy= 0.64107 accuracy_s= 0.78767 accuracy_f= 0.84715 time= 0.16614
Epoch: 0261 train_loss= 593.56268 feature_loss= 12.10677 structure_loss= 581.45593 accuracy= 0.64092 accuracy_s= 0.78796 accuracy_f= 0.84759 time= 0.16583
Epoch: 0271 train_loss= 561.69232 feature_loss= 12.08496 structure_loss= 549.60736 accuracy= 0.64136 accuracy_s= 0.78839 accuracy_f= 0.84817 time= 0.16757
Epoch: 0281 train_loss= 535.75189 feature_loss= 12.06058 structure_loss= 523.69128 accuracy= 0.64311 accuracy_s= 0.78825 accuracy_f= 0.84802 time= 0.16562
Epoch: 0291 train_loss= 511.41000 feature_loss= 12.03818 structure_loss= 499.37183 accuracy= 0.64412 accuracy_s= 0.78839 accuracy_f= 0.84802 time= 0.16444

accuracy 0.64514
accuracy_s 0.78869
accuracy_f 0.84875
auc 0.51487
f1_score 0.18667
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=1, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 422
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
using wavelet scattering transform
y_features shape after scatting (13752, 9971) <class 'numpy.ndarray'>
using PCA to reduce dim
y_features shape after PCA torch.Size([13752, 191]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 4033.83667 feature_loss= 27.97263 structure_loss= 4005.86401 accuracy= 0.66332 accuracy_s= 0.80817 accuracy_f= 0.80948 time= 0.27944
Epoch: 0011 train_loss= 131.10805 feature_loss= 20.38571 structure_loss= 110.72233 accuracy= 0.64209 accuracy_s= 0.78709 accuracy_f= 0.81588 time= 0.23325
Epoch: 0021 train_loss= 72.63768 feature_loss= 16.58936 structure_loss= 56.04832 accuracy= 0.63845 accuracy_s= 0.78578 accuracy_f= 0.82126 time= 0.23221
Epoch: 0031 train_loss= 31.03255 feature_loss= 15.21824 structure_loss= 15.81431 accuracy= 0.65620 accuracy_s= 0.78563 accuracy_f= 0.82315 time= 0.23124
Epoch: 0041 train_loss= 17.51150 feature_loss= 14.71904 structure_loss= 2.79247 accuracy= 0.66536 accuracy_s= 0.78839 accuracy_f= 0.82301 time= 0.23090
Epoch: 0051 train_loss= 15.36357 feature_loss= 14.48822 structure_loss= 0.87535 accuracy= 0.67277 accuracy_s= 0.78679 accuracy_f= 0.82286 time= 0.23008
Epoch: 0061 train_loss= 14.41728 feature_loss= 13.99520 structure_loss= 0.42208 accuracy= 0.67699 accuracy_s= 0.78461 accuracy_f= 0.82286 time= 0.22894
Epoch: 0071 train_loss= 14.08328 feature_loss= 13.75632 structure_loss= 0.32697 accuracy= 0.67787 accuracy_s= 0.78316 accuracy_f= 0.82286 time= 0.23332
Epoch: 0081 train_loss= 13.89978 feature_loss= 13.61884 structure_loss= 0.28094 accuracy= 0.67801 accuracy_s= 0.78243 accuracy_f= 0.82301 time= 0.23103
Epoch: 0091 train_loss= 13.75864 feature_loss= 13.49767 structure_loss= 0.26097 accuracy= 0.67801 accuracy_s= 0.78229 accuracy_f= 0.82315 time= 0.23104
Epoch: 0101 train_loss= 13.63827 feature_loss= 13.39009 structure_loss= 0.24818 accuracy= 0.67772 accuracy_s= 0.78229 accuracy_f= 0.82315 time= 0.23300
Epoch: 0111 train_loss= 13.50803 feature_loss= 13.27042 structure_loss= 0.23761 accuracy= 0.67757 accuracy_s= 0.78214 accuracy_f= 0.82286 time= 0.23235
Epoch: 0121 train_loss= 13.38676 feature_loss= 13.15542 structure_loss= 0.23134 accuracy= 0.67743 accuracy_s= 0.78200 accuracy_f= 0.82272 time= 0.23352
Epoch: 0131 train_loss= 13.28037 feature_loss= 13.05448 structure_loss= 0.22590 accuracy= 0.67728 accuracy_s= 0.78200 accuracy_f= 0.82243 time= 0.23006
Epoch: 0141 train_loss= 13.21062 feature_loss= 12.98979 structure_loss= 0.22082 accuracy= 0.67728 accuracy_s= 0.78200 accuracy_f= 0.82243 time= 0.23128
Epoch: 0151 train_loss= 13.15784 feature_loss= 12.93962 structure_loss= 0.21822 accuracy= 0.67728 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.23159
Epoch: 0161 train_loss= 13.11082 feature_loss= 12.89594 structure_loss= 0.21488 accuracy= 0.67714 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.23160
Epoch: 0171 train_loss= 13.06999 feature_loss= 12.85795 structure_loss= 0.21204 accuracy= 0.67714 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.23092
Epoch: 0181 train_loss= 13.03511 feature_loss= 12.82498 structure_loss= 0.21013 accuracy= 0.67699 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.23221
Epoch: 0191 train_loss= 13.00484 feature_loss= 12.79640 structure_loss= 0.20843 accuracy= 0.67728 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.23190
Epoch: 0201 train_loss= 12.97859 feature_loss= 12.77167 structure_loss= 0.20692 accuracy= 0.67714 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.23185
Epoch: 0211 train_loss= 12.95596 feature_loss= 12.75030 structure_loss= 0.20566 accuracy= 0.67714 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.23058
Epoch: 0221 train_loss= 12.93498 feature_loss= 12.73186 structure_loss= 0.20311 accuracy= 0.67728 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.23174
Epoch: 0231 train_loss= 12.91886 feature_loss= 12.71599 structure_loss= 0.20286 accuracy= 0.67714 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.23356
Epoch: 0241 train_loss= 12.90457 feature_loss= 12.70235 structure_loss= 0.20222 accuracy= 0.67728 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.23306
Epoch: 0251 train_loss= 12.89204 feature_loss= 12.69064 structure_loss= 0.20140 accuracy= 0.67728 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.23352
Epoch: 0261 train_loss= 12.88174 feature_loss= 12.68061 structure_loss= 0.20112 accuracy= 0.67743 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.23198
Epoch: 0271 train_loss= 12.87252 feature_loss= 12.67204 structure_loss= 0.20048 accuracy= 0.67714 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.23335
Epoch: 0281 train_loss= 12.86475 feature_loss= 12.66473 structure_loss= 0.20002 accuracy= 0.67728 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.23367
Epoch: 0291 train_loss= 12.85766 feature_loss= 12.65851 structure_loss= 0.19916 accuracy= 0.67728 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.23307

accuracy 0.67743
accuracy_s 0.78185
accuracy_f 0.82257
auc 0.48902
f1_score 0.26067
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=1, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 91
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
using wavelet scattering transform
y_features shape after scatting (13752, 9971) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 40 steps!
y_features shape after FMS torch.Size([13752, 191]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 75.36220 feature_loss= 27.79862 structure_loss= 47.56358 accuracy= 0.66507 accuracy_s= 0.81094 accuracy_f= 0.80890 time= 0.28021
Epoch: 0011 train_loss= 19.15944 feature_loss= 18.06120 structure_loss= 1.09824 accuracy= 0.66885 accuracy_s= 0.78200 accuracy_f= 0.81879 time= 0.22832
Epoch: 0021 train_loss= 15.29135 feature_loss= 14.73308 structure_loss= 0.55827 accuracy= 0.67132 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.22945
Epoch: 0031 train_loss= 14.67131 feature_loss= 14.33490 structure_loss= 0.33641 accuracy= 0.67685 accuracy_s= 0.78200 accuracy_f= 0.82243 time= 0.22898
Epoch: 0041 train_loss= 14.37316 feature_loss= 14.11911 structure_loss= 0.25405 accuracy= 0.67714 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.23010
Epoch: 0051 train_loss= 14.15242 feature_loss= 13.92725 structure_loss= 0.22517 accuracy= 0.67743 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.22998
Epoch: 0061 train_loss= 13.97058 feature_loss= 13.75761 structure_loss= 0.21297 accuracy= 0.67787 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.23073
Epoch: 0071 train_loss= 13.81542 feature_loss= 13.60837 structure_loss= 0.20705 accuracy= 0.67787 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.23166
Epoch: 0081 train_loss= 13.68230 feature_loss= 13.47758 structure_loss= 0.20472 accuracy= 0.67787 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.23143
Epoch: 0091 train_loss= 13.56638 feature_loss= 13.36328 structure_loss= 0.20310 accuracy= 0.67787 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.23285
Epoch: 0101 train_loss= 13.46555 feature_loss= 13.26359 structure_loss= 0.20196 accuracy= 0.67787 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.23029
Epoch: 0111 train_loss= 13.37790 feature_loss= 13.17676 structure_loss= 0.20114 accuracy= 0.67772 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.23559
Epoch: 0121 train_loss= 13.30168 feature_loss= 13.10121 structure_loss= 0.20047 accuracy= 0.67772 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.23541
Epoch: 0131 train_loss= 13.23559 feature_loss= 13.03553 structure_loss= 0.20006 accuracy= 0.67757 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.23343
Epoch: 0141 train_loss= 13.17798 feature_loss= 12.97848 structure_loss= 0.19950 accuracy= 0.67757 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.23591
Epoch: 0151 train_loss= 13.12832 feature_loss= 12.92897 structure_loss= 0.19935 accuracy= 0.67757 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.23490
Epoch: 0161 train_loss= 13.08509 feature_loss= 12.88604 structure_loss= 0.19905 accuracy= 0.67757 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.23342
Epoch: 0171 train_loss= 13.04765 feature_loss= 12.84887 structure_loss= 0.19879 accuracy= 0.67757 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.23166
Epoch: 0181 train_loss= 13.01513 feature_loss= 12.81670 structure_loss= 0.19842 accuracy= 0.67757 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.23115
Epoch: 0191 train_loss= 12.98731 feature_loss= 12.78892 structure_loss= 0.19839 accuracy= 0.67743 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.23141
Epoch: 0201 train_loss= 12.96307 feature_loss= 12.76494 structure_loss= 0.19814 accuracy= 0.67743 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.23148
Epoch: 0211 train_loss= 12.94239 feature_loss= 12.74428 structure_loss= 0.19811 accuracy= 0.67757 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.23088
Epoch: 0221 train_loss= 12.92442 feature_loss= 12.72651 structure_loss= 0.19791 accuracy= 0.67772 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.23239
Epoch: 0231 train_loss= 12.90907 feature_loss= 12.71125 structure_loss= 0.19782 accuracy= 0.67757 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.23158
Epoch: 0241 train_loss= 12.89589 feature_loss= 12.69816 structure_loss= 0.19773 accuracy= 0.67757 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.23207
Epoch: 0251 train_loss= 12.88457 feature_loss= 12.68696 structure_loss= 0.19761 accuracy= 0.67743 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.23317
Epoch: 0261 train_loss= 12.87499 feature_loss= 12.67739 structure_loss= 0.19760 accuracy= 0.67743 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.23283
Epoch: 0271 train_loss= 12.86668 feature_loss= 12.66923 structure_loss= 0.19745 accuracy= 0.67743 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.23304
Epoch: 0281 train_loss= 12.85970 feature_loss= 12.66228 structure_loss= 0.19741 accuracy= 0.67743 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.23228
Epoch: 0291 train_loss= 12.85372 feature_loss= 12.65638 structure_loss= 0.19734 accuracy= 0.67743 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.23323

accuracy 0.67743
accuracy_s 0.78185
accuracy_f 0.82243
auc 0.48898
f1_score 0.26067
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=2, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 60
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
using wavelet scattering transform
y_features shape after scatting (13752, 9971) <class 'numpy.ndarray'>
using PCA to reduce dim
y_features shape after PCA torch.Size([13752, 191]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 3592.90796 feature_loss= 28.32810 structure_loss= 3564.57983 accuracy= 0.66376 accuracy_s= 0.80817 accuracy_f= 0.80963 time= 0.20294
Epoch: 0011 train_loss= 113.77150 feature_loss= 20.69176 structure_loss= 93.07973 accuracy= 0.63947 accuracy_s= 0.78461 accuracy_f= 0.81530 time= 0.16638
Epoch: 0021 train_loss= 65.83746 feature_loss= 16.69637 structure_loss= 49.14110 accuracy= 0.64238 accuracy_s= 0.78621 accuracy_f= 0.82010 time= 0.16657
Epoch: 0031 train_loss= 35.67891 feature_loss= 15.29730 structure_loss= 20.38161 accuracy= 0.65329 accuracy_s= 0.78810 accuracy_f= 0.82170 time= 0.16600
Epoch: 0041 train_loss= 21.97062 feature_loss= 14.78046 structure_loss= 7.19016 accuracy= 0.66056 accuracy_s= 0.78839 accuracy_f= 0.82243 time= 0.16613
Epoch: 0051 train_loss= 17.26913 feature_loss= 14.47697 structure_loss= 2.79216 accuracy= 0.66405 accuracy_s= 0.78854 accuracy_f= 0.82257 time= 0.16737
Epoch: 0061 train_loss= 15.48356 feature_loss= 14.22804 structure_loss= 1.25552 accuracy= 0.67059 accuracy_s= 0.78781 accuracy_f= 0.82257 time= 0.16632
Epoch: 0071 train_loss= 14.66426 feature_loss= 13.89367 structure_loss= 0.77058 accuracy= 0.67336 accuracy_s= 0.78549 accuracy_f= 0.82257 time= 0.16626
Epoch: 0081 train_loss= 14.05617 feature_loss= 13.51607 structure_loss= 0.54010 accuracy= 0.67597 accuracy_s= 0.78476 accuracy_f= 0.82257 time= 0.16649
Epoch: 0091 train_loss= 13.80159 feature_loss= 13.37214 structure_loss= 0.42946 accuracy= 0.67612 accuracy_s= 0.78403 accuracy_f= 0.82257 time= 0.16771
Epoch: 0101 train_loss= 13.63130 feature_loss= 13.26654 structure_loss= 0.36475 accuracy= 0.67627 accuracy_s= 0.78360 accuracy_f= 0.82243 time= 0.16680
Epoch: 0111 train_loss= 13.50165 feature_loss= 13.18156 structure_loss= 0.32009 accuracy= 0.67641 accuracy_s= 0.78287 accuracy_f= 0.82243 time= 0.16490
Epoch: 0121 train_loss= 13.39608 feature_loss= 13.10715 structure_loss= 0.28893 accuracy= 0.67670 accuracy_s= 0.78243 accuracy_f= 0.82243 time= 0.16628
Epoch: 0131 train_loss= 13.31138 feature_loss= 13.04207 structure_loss= 0.26931 accuracy= 0.67670 accuracy_s= 0.78243 accuracy_f= 0.82243 time= 0.16925
Epoch: 0141 train_loss= 13.23867 feature_loss= 12.98522 structure_loss= 0.25344 accuracy= 0.67685 accuracy_s= 0.78200 accuracy_f= 0.82243 time= 0.16594
Epoch: 0151 train_loss= 13.18020 feature_loss= 12.93564 structure_loss= 0.24456 accuracy= 0.67685 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.16569
Epoch: 0161 train_loss= 13.12729 feature_loss= 12.89247 structure_loss= 0.23482 accuracy= 0.67699 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.16530
Epoch: 0171 train_loss= 13.08197 feature_loss= 12.85493 structure_loss= 0.22704 accuracy= 0.67699 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.16580
Epoch: 0181 train_loss= 13.04916 feature_loss= 12.82234 structure_loss= 0.22683 accuracy= 0.67714 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.16579
Epoch: 0191 train_loss= 13.01189 feature_loss= 12.79409 structure_loss= 0.21779 accuracy= 0.67714 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.16624
Epoch: 0201 train_loss= 12.98424 feature_loss= 12.76965 structure_loss= 0.21459 accuracy= 0.67699 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.16646
Epoch: 0211 train_loss= 12.96058 feature_loss= 12.74854 structure_loss= 0.21204 accuracy= 0.67699 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.16849
Epoch: 0221 train_loss= 12.94051 feature_loss= 12.73033 structure_loss= 0.21017 accuracy= 0.67714 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.16850
Epoch: 0231 train_loss= 12.92576 feature_loss= 12.71466 structure_loss= 0.21110 accuracy= 0.67685 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.16546
Epoch: 0241 train_loss= 12.90702 feature_loss= 12.70119 structure_loss= 0.20584 accuracy= 0.67714 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.16604
Epoch: 0251 train_loss= 12.89241 feature_loss= 12.68964 structure_loss= 0.20278 accuracy= 0.67728 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.16727
Epoch: 0261 train_loss= 12.89054 feature_loss= 12.67974 structure_loss= 0.21079 accuracy= 0.67728 accuracy_s= 0.78185 accuracy_f= 0.82243 time= 0.16698
Epoch: 0271 train_loss= 12.87334 feature_loss= 12.67130 structure_loss= 0.20205 accuracy= 0.67728 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.16689
Epoch: 0281 train_loss= 12.86390 feature_loss= 12.66409 structure_loss= 0.19982 accuracy= 0.67714 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.16688
Epoch: 0291 train_loss= 12.85822 feature_loss= 12.65795 structure_loss= 0.20026 accuracy= 0.67728 accuracy_s= 0.78185 accuracy_f= 0.82257 time= 0.16975

accuracy 0.67714
accuracy_s 0.78185
accuracy_f 0.82243
auc 0.48883
f1_score 0.26000
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=2, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 495
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
using wavelet scattering transform
y_features shape after scatting (13752, 9971) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 56 steps!
y_features shape after FMS torch.Size([13752, 191]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 53.59155 feature_loss= 27.74731 structure_loss= 25.84424 accuracy= 0.66245 accuracy_s= 0.80832 accuracy_f= 0.80846 time= 0.21362
Epoch: 0011 train_loss= 21.76111 feature_loss= 17.14971 structure_loss= 4.61141 accuracy= 0.67787 accuracy_s= 0.81254 accuracy_f= 0.81777 time= 0.16480
Epoch: 0021 train_loss= 15.72106 feature_loss= 14.63754 structure_loss= 1.08352 accuracy= 0.67772 accuracy_s= 0.79683 accuracy_f= 0.82243 time= 0.16346
Epoch: 0031 train_loss= 14.81971 feature_loss= 14.32983 structure_loss= 0.48988 accuracy= 0.67787 accuracy_s= 0.78956 accuracy_f= 0.82243 time= 0.16511
Epoch: 0041 train_loss= 14.43967 feature_loss= 14.11032 structure_loss= 0.32935 accuracy= 0.67728 accuracy_s= 0.78752 accuracy_f= 0.82243 time= 0.16639
Epoch: 0051 train_loss= 14.17904 feature_loss= 13.91599 structure_loss= 0.26305 accuracy= 0.67743 accuracy_s= 0.78563 accuracy_f= 0.82243 time= 0.16484
Epoch: 0061 train_loss= 13.97877 feature_loss= 13.74510 structure_loss= 0.23367 accuracy= 0.67787 accuracy_s= 0.78389 accuracy_f= 0.82243 time= 0.16505
Epoch: 0071 train_loss= 13.81503 feature_loss= 13.59555 structure_loss= 0.21949 accuracy= 0.67787 accuracy_s= 0.78345 accuracy_f= 0.82243 time= 0.16644
Epoch: 0081 train_loss= 13.67753 feature_loss= 13.46509 structure_loss= 0.21244 accuracy= 0.67787 accuracy_s= 0.78330 accuracy_f= 0.82243 time= 0.16618
Epoch: 0091 train_loss= 13.56001 feature_loss= 13.35150 structure_loss= 0.20851 accuracy= 0.67787 accuracy_s= 0.78316 accuracy_f= 0.82243 time= 0.16597
Epoch: 0101 train_loss= 13.45866 feature_loss= 13.25273 structure_loss= 0.20593 accuracy= 0.67787 accuracy_s= 0.78316 accuracy_f= 0.82243 time= 0.16612
Epoch: 0111 train_loss= 13.37129 feature_loss= 13.16690 structure_loss= 0.20439 accuracy= 0.67772 accuracy_s= 0.78301 accuracy_f= 0.82243 time= 0.16723
Epoch: 0121 train_loss= 13.29541 feature_loss= 13.09236 structure_loss= 0.20305 accuracy= 0.67772 accuracy_s= 0.78287 accuracy_f= 0.82243 time= 0.16619
Epoch: 0131 train_loss= 13.22971 feature_loss= 13.02765 structure_loss= 0.20206 accuracy= 0.67757 accuracy_s= 0.78272 accuracy_f= 0.82243 time= 0.16696
Epoch: 0141 train_loss= 13.17262 feature_loss= 12.97150 structure_loss= 0.20112 accuracy= 0.67757 accuracy_s= 0.78272 accuracy_f= 0.82243 time= 0.16843
Epoch: 0151 train_loss= 13.12331 feature_loss= 12.92282 structure_loss= 0.20049 accuracy= 0.67743 accuracy_s= 0.78272 accuracy_f= 0.82243 time= 0.16641
Epoch: 0161 train_loss= 13.08046 feature_loss= 12.88065 structure_loss= 0.19980 accuracy= 0.67757 accuracy_s= 0.78272 accuracy_f= 0.82257 time= 0.16613
Epoch: 0171 train_loss= 13.04340 feature_loss= 12.84415 structure_loss= 0.19925 accuracy= 0.67757 accuracy_s= 0.78272 accuracy_f= 0.82243 time= 0.16808
Epoch: 0181 train_loss= 13.01145 feature_loss= 12.81259 structure_loss= 0.19886 accuracy= 0.67757 accuracy_s= 0.78258 accuracy_f= 0.82257 time= 0.16712
Epoch: 0191 train_loss= 12.98381 feature_loss= 12.78534 structure_loss= 0.19847 accuracy= 0.67757 accuracy_s= 0.78243 accuracy_f= 0.82257 time= 0.16857
Epoch: 0201 train_loss= 12.96001 feature_loss= 12.76183 structure_loss= 0.19818 accuracy= 0.67743 accuracy_s= 0.78243 accuracy_f= 0.82257 time= 0.16738
Epoch: 0211 train_loss= 12.93940 feature_loss= 12.74159 structure_loss= 0.19781 accuracy= 0.67728 accuracy_s= 0.78229 accuracy_f= 0.82257 time= 0.16883
Epoch: 0221 train_loss= 12.92175 feature_loss= 12.72418 structure_loss= 0.19757 accuracy= 0.67728 accuracy_s= 0.78229 accuracy_f= 0.82257 time= 0.16710
Epoch: 0231 train_loss= 12.90650 feature_loss= 12.70924 structure_loss= 0.19726 accuracy= 0.67743 accuracy_s= 0.78214 accuracy_f= 0.82257 time= 0.16603
Epoch: 0241 train_loss= 12.89357 feature_loss= 12.69643 structure_loss= 0.19715 accuracy= 0.67728 accuracy_s= 0.78214 accuracy_f= 0.82243 time= 0.16787
Epoch: 0251 train_loss= 12.88239 feature_loss= 12.68547 structure_loss= 0.19692 accuracy= 0.67743 accuracy_s= 0.78200 accuracy_f= 0.82243 time= 0.16692
Epoch: 0261 train_loss= 12.87283 feature_loss= 12.67611 structure_loss= 0.19672 accuracy= 0.67743 accuracy_s= 0.78200 accuracy_f= 0.82243 time= 0.16659
Epoch: 0271 train_loss= 12.86472 feature_loss= 12.66813 structure_loss= 0.19658 accuracy= 0.67743 accuracy_s= 0.78200 accuracy_f= 0.82257 time= 0.16709
Epoch: 0281 train_loss= 12.85777 feature_loss= 12.66134 structure_loss= 0.19643 accuracy= 0.67743 accuracy_s= 0.78200 accuracy_f= 0.82243 time= 0.16679
Epoch: 0291 train_loss= 12.85190 feature_loss= 12.65558 structure_loss= 0.19632 accuracy= 0.67743 accuracy_s= 0.78200 accuracy_f= 0.82243 time= 0.16706

accuracy 0.67743
accuracy_s 0.78200
accuracy_f 0.82243
auc 0.48891
f1_score 0.26067
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=3, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 784
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
using wavelet scattering transform
y_features shape after scatting (13752, 9971) <class 'numpy.ndarray'>
using PCA to reduce dim
y_features shape after PCA torch.Size([13752, 191]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 11790.38379 feature_loss= 27.12559 structure_loss= 11763.25781 accuracy= 0.65794 accuracy_s= 0.79552 accuracy_f= 0.80788 time= 0.26894
Epoch: 0011 train_loss= 5721.77002 feature_loss= 20.46757 structure_loss= 5701.30225 accuracy= 0.62231 accuracy_s= 0.78854 accuracy_f= 0.81894 time= 0.23221
Epoch: 0021 train_loss= 4326.01611 feature_loss= 19.49723 structure_loss= 4306.51904 accuracy= 0.62173 accuracy_s= 0.78563 accuracy_f= 0.81864 time= 0.22961
Epoch: 0031 train_loss= 3352.19409 feature_loss= 18.25357 structure_loss= 3333.94043 accuracy= 0.61984 accuracy_s= 0.78665 accuracy_f= 0.81835 time= 0.23192
Epoch: 0041 train_loss= 2594.53760 feature_loss= 16.72043 structure_loss= 2577.81714 accuracy= 0.62435 accuracy_s= 0.78767 accuracy_f= 0.81937 time= 0.23137
Epoch: 0051 train_loss= 2110.07104 feature_loss= 15.59887 structure_loss= 2094.47217 accuracy= 0.63889 accuracy_s= 0.78549 accuracy_f= 0.81981 time= 0.23014
Epoch: 0061 train_loss= 1948.36389 feature_loss= 14.85359 structure_loss= 1933.51025 accuracy= 0.63743 accuracy_s= 0.78810 accuracy_f= 0.82083 time= 0.23223
Epoch: 0071 train_loss= 1767.00647 feature_loss= 14.39760 structure_loss= 1752.60889 accuracy= 0.65867 accuracy_s= 0.79101 accuracy_f= 0.82141 time= 0.22944
Epoch: 0081 train_loss= 1641.15063 feature_loss= 14.10001 structure_loss= 1627.05066 accuracy= 0.65939 accuracy_s= 0.78927 accuracy_f= 0.82257 time= 0.23048
Epoch: 0091 train_loss= 1531.68909 feature_loss= 13.85773 structure_loss= 1517.83142 accuracy= 0.65954 accuracy_s= 0.78970 accuracy_f= 0.82286 time= 0.22971
Epoch: 0101 train_loss= 1433.93896 feature_loss= 13.71436 structure_loss= 1420.22461 accuracy= 0.66027 accuracy_s= 0.79043 accuracy_f= 0.82228 time= 0.22923
Epoch: 0111 train_loss= 1347.80383 feature_loss= 13.57916 structure_loss= 1334.22473 accuracy= 0.65954 accuracy_s= 0.78985 accuracy_f= 0.82272 time= 0.23009
Epoch: 0121 train_loss= 1278.84998 feature_loss= 13.46629 structure_loss= 1265.38367 accuracy= 0.66143 accuracy_s= 0.79058 accuracy_f= 0.82228 time= 0.22889
Epoch: 0131 train_loss= 1212.01709 feature_loss= 13.36222 structure_loss= 1198.65491 accuracy= 0.66201 accuracy_s= 0.79101 accuracy_f= 0.82286 time= 0.22896
Epoch: 0141 train_loss= 1152.48206 feature_loss= 13.26456 structure_loss= 1139.21753 accuracy= 0.66041 accuracy_s= 0.79029 accuracy_f= 0.82257 time= 0.22863
Epoch: 0151 train_loss= 1093.84143 feature_loss= 13.20620 structure_loss= 1080.63525 accuracy= 0.66099 accuracy_s= 0.79072 accuracy_f= 0.82243 time= 0.22983
Epoch: 0161 train_loss= 1047.90906 feature_loss= 13.15752 structure_loss= 1034.75159 accuracy= 0.66172 accuracy_s= 0.78985 accuracy_f= 0.82228 time= 0.23073
Epoch: 0171 train_loss= 999.15070 feature_loss= 13.11214 structure_loss= 986.03857 accuracy= 0.66201 accuracy_s= 0.78985 accuracy_f= 0.82286 time= 0.23013
Epoch: 0181 train_loss= 957.19537 feature_loss= 13.07362 structure_loss= 944.12177 accuracy= 0.66172 accuracy_s= 0.78956 accuracy_f= 0.82243 time= 0.23239
Epoch: 0191 train_loss= 918.90277 feature_loss= 13.03636 structure_loss= 905.86639 accuracy= 0.66129 accuracy_s= 0.79101 accuracy_f= 0.82243 time= 0.22995
Epoch: 0201 train_loss= 879.03259 feature_loss= 13.01237 structure_loss= 866.02020 accuracy= 0.66143 accuracy_s= 0.78912 accuracy_f= 0.82243 time= 0.22994
Epoch: 0211 train_loss= 852.89441 feature_loss= 12.98038 structure_loss= 839.91400 accuracy= 0.66143 accuracy_s= 0.79072 accuracy_f= 0.82257 time= 0.23036
Epoch: 0221 train_loss= 825.43842 feature_loss= 12.96067 structure_loss= 812.47772 accuracy= 0.66216 accuracy_s= 0.79029 accuracy_f= 0.82243 time= 0.23107
Epoch: 0231 train_loss= 798.69867 feature_loss= 12.93919 structure_loss= 785.75946 accuracy= 0.66230 accuracy_s= 0.78999 accuracy_f= 0.82257 time= 0.23112
Epoch: 0241 train_loss= 773.01843 feature_loss= 12.92098 structure_loss= 760.09747 accuracy= 0.66129 accuracy_s= 0.79116 accuracy_f= 0.82257 time= 0.23226
Epoch: 0251 train_loss= 748.57214 feature_loss= 12.90691 structure_loss= 735.66522 accuracy= 0.66187 accuracy_s= 0.79058 accuracy_f= 0.82243 time= 0.23168
Epoch: 0261 train_loss= 720.05151 feature_loss= 12.90735 structure_loss= 707.14417 accuracy= 0.66114 accuracy_s= 0.79218 accuracy_f= 0.82228 time= 0.23264
Epoch: 0271 train_loss= 699.20447 feature_loss= 12.88097 structure_loss= 686.32349 accuracy= 0.66099 accuracy_s= 0.78941 accuracy_f= 0.82243 time= 0.23087
Epoch: 0281 train_loss= 678.20374 feature_loss= 12.87182 structure_loss= 665.33191 accuracy= 0.66158 accuracy_s= 0.78970 accuracy_f= 0.82243 time= 0.23127
Epoch: 0291 train_loss= 669.56934 feature_loss= 12.86352 structure_loss= 656.70581 accuracy= 0.66332 accuracy_s= 0.79145 accuracy_f= 0.82243 time= 0.23288

accuracy 0.66172
accuracy_s 0.79043
accuracy_f 0.82228
auc 0.46677
f1_score 0.22467
Job finished!



Initializing scat twodecoders training
Namespace(alpha=1, att=3, beta=1, clique_size=50, cuda=False, dataset='amazon_electronics_computers', device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=30, weight_decay=0.0005)
random seed: 946
Found existing ad data, loading...
feature_dim: 767 hidden1_dim: 383 hidden2_dim: 191 nodes_num: 13752 anomaly_num: 3000

Start modeling
using wavelet scattering transform
y_features shape after scatting (13752, 9971) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 40 steps!
y_features shape after FMS torch.Size([13752, 191]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 12351.15039 feature_loss= 27.57869 structure_loss= 12323.57129 accuracy= 0.66259 accuracy_s= 0.80061 accuracy_f= 0.80730 time= 0.27315
Epoch: 0011 train_loss= 5825.37305 feature_loss= 20.94701 structure_loss= 5804.42627 accuracy= 0.64165 accuracy_s= 0.78738 accuracy_f= 0.81835 time= 0.22929
Epoch: 0021 train_loss= 4311.21338 feature_loss= 19.84476 structure_loss= 4291.36865 accuracy= 0.62536 accuracy_s= 0.78505 accuracy_f= 0.81879 time= 0.22948
Epoch: 0031 train_loss= 3252.20508 feature_loss= 18.83320 structure_loss= 3233.37183 accuracy= 0.62507 accuracy_s= 0.78549 accuracy_f= 0.82083 time= 0.22020
Epoch: 0041 train_loss= 2509.28369 feature_loss= 17.21128 structure_loss= 2492.07251 accuracy= 0.63351 accuracy_s= 0.78563 accuracy_f= 0.81923 time= 0.22859
Epoch: 0051 train_loss= 2039.16943 feature_loss= 15.81121 structure_loss= 2023.35828 accuracy= 0.64223 accuracy_s= 0.78519 accuracy_f= 0.82054 time= 0.22922
Epoch: 0061 train_loss= 1832.92322 feature_loss= 14.90824 structure_loss= 1818.01501 accuracy= 0.64136 accuracy_s= 0.78752 accuracy_f= 0.82024 time= 0.22916
Epoch: 0071 train_loss= 1619.64771 feature_loss= 14.39241 structure_loss= 1605.25525 accuracy= 0.65271 accuracy_s= 0.78781 accuracy_f= 0.82272 time= 0.22950
Epoch: 0081 train_loss= 1476.03845 feature_loss= 14.05619 structure_loss= 1461.98230 accuracy= 0.65198 accuracy_s= 0.78956 accuracy_f= 0.82315 time= 0.22882
Epoch: 0091 train_loss= 1354.62891 feature_loss= 13.88023 structure_loss= 1340.74866 accuracy= 0.65474 accuracy_s= 0.79174 accuracy_f= 0.82301 time= 0.22816
Epoch: 0101 train_loss= 1253.81580 feature_loss= 13.68073 structure_loss= 1240.13501 accuracy= 0.65241 accuracy_s= 0.79145 accuracy_f= 0.82243 time= 0.23148
Epoch: 0111 train_loss= 1162.10730 feature_loss= 13.55512 structure_loss= 1148.55212 accuracy= 0.65852 accuracy_s= 0.79159 accuracy_f= 0.82257 time= 0.23045
Epoch: 0121 train_loss= 1094.36279 feature_loss= 13.43237 structure_loss= 1080.93042 accuracy= 0.65663 accuracy_s= 0.79174 accuracy_f= 0.82243 time= 0.22919
Epoch: 0131 train_loss= 1031.56677 feature_loss= 13.32716 structure_loss= 1018.23956 accuracy= 0.65489 accuracy_s= 0.79145 accuracy_f= 0.82257 time= 0.23030
Epoch: 0141 train_loss= 971.78711 feature_loss= 13.29392 structure_loss= 958.49316 accuracy= 0.65620 accuracy_s= 0.79101 accuracy_f= 0.82286 time= 0.22944
Epoch: 0151 train_loss= 915.13000 feature_loss= 13.20508 structure_loss= 901.92493 accuracy= 0.65867 accuracy_s= 0.79159 accuracy_f= 0.82257 time= 0.23177
Epoch: 0161 train_loss= 860.83569 feature_loss= 13.15532 structure_loss= 847.68036 accuracy= 0.65838 accuracy_s= 0.79159 accuracy_f= 0.82257 time= 0.23143
Epoch: 0171 train_loss= 812.87384 feature_loss= 13.11200 structure_loss= 799.76184 accuracy= 0.65576 accuracy_s= 0.79029 accuracy_f= 0.82243 time= 0.22951
Epoch: 0181 train_loss= 758.12158 feature_loss= 13.07217 structure_loss= 745.04938 accuracy= 0.65430 accuracy_s= 0.79130 accuracy_f= 0.82257 time= 0.22877
Epoch: 0191 train_loss= 723.53973 feature_loss= 13.07209 structure_loss= 710.46765 accuracy= 0.65678 accuracy_s= 0.79130 accuracy_f= 0.82257 time= 0.22926
Epoch: 0201 train_loss= 676.96527 feature_loss= 13.02065 structure_loss= 663.94464 accuracy= 0.64892 accuracy_s= 0.78941 accuracy_f= 0.82243 time= 0.23052
Epoch: 0211 train_loss= 643.01208 feature_loss= 13.07458 structure_loss= 629.93750 accuracy= 0.65532 accuracy_s= 0.78869 accuracy_f= 0.82257 time= 0.23002
Epoch: 0221 train_loss= 607.73053 feature_loss= 12.98527 structure_loss= 594.74524 accuracy= 0.64805 accuracy_s= 0.78854 accuracy_f= 0.82257 time= 0.23113
Epoch: 0231 train_loss= 596.84711 feature_loss= 12.96669 structure_loss= 583.88043 accuracy= 0.65372 accuracy_s= 0.78898 accuracy_f= 0.82257 time= 0.22898
Epoch: 0241 train_loss= 564.96527 feature_loss= 12.93433 structure_loss= 552.03094 accuracy= 0.65474 accuracy_s= 0.78970 accuracy_f= 0.82272 time= 0.23120
Epoch: 0251 train_loss= 540.94659 feature_loss= 13.02944 structure_loss= 527.91718 accuracy= 0.65198 accuracy_s= 0.78665 accuracy_f= 0.82286 time= 0.23002
Epoch: 0261 train_loss= 501.95102 feature_loss= 12.99840 structure_loss= 488.95261 accuracy= 0.65271 accuracy_s= 0.78694 accuracy_f= 0.82330 time= 0.22900
Epoch: 0271 train_loss= 474.79788 feature_loss= 12.93966 structure_loss= 461.85822 accuracy= 0.64776 accuracy_s= 0.78665 accuracy_f= 0.82286 time= 0.23081
Epoch: 0281 train_loss= 445.11234 feature_loss= 12.91462 structure_loss= 432.19772 accuracy= 0.64572 accuracy_s= 0.78592 accuracy_f= 0.82286 time= 0.23062
Epoch: 0291 train_loss= 415.32239 feature_loss= 12.95791 structure_loss= 402.36447 accuracy= 0.64907 accuracy_s= 0.78650 accuracy_f= 0.82330 time= 0.23224

accuracy 0.64674
accuracy_s 0.78461
accuracy_f 0.82301
auc 0.44789
f1_score 0.19033
Job finished!
amazon_electronics_computers job finished!
cora_full experiment with fixed GCN hidden layer



Initializing normal onedecoder
Namespace(alpha=1, beta=1, clique_size=50, cuda=False, dataset='cora_full', decoder=0, device=device(type='cuda'), dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=45, weight_decay=0.0005)
random seed: 122
Found existing ad data, loading...
feature_dim: 8710 hidden1_dim: 4355 hidden2_dim: 2177 nodes_num: 19793 anomaly_num: 4500

Start modeling
Epoch: 0001 train_loss= 78.82912 accuracy= 0.59935 time= 3.14018
Epoch: 0011 train_loss= 8.88928 accuracy= 0.61987 time= 3.18050
Epoch: 0021 train_loss= 3.88543 accuracy= 0.62694 time= 3.17847
Epoch: 0031 train_loss= 3.39465 accuracy= 0.62017 time= 3.14727
Epoch: 0041 train_loss= 2.73375 accuracy= 0.62179 time= 3.15510
Epoch: 0051 train_loss= 2.52631 accuracy= 0.62259 time= 3.19851
Epoch: 0061 train_loss= 2.43921 accuracy= 0.62350 time= 3.16960
Epoch: 0071 train_loss= 2.41455 accuracy= 0.66079 time= 3.15970
Epoch: 0081 train_loss= 2.22849 accuracy= 0.67837 time= 3.16417
Epoch: 0091 train_loss= 2.21169 accuracy= 0.67999 time= 3.19522
Epoch: 0101 train_loss= 2.19832 accuracy= 0.67867 time= 3.20360
Epoch: 0111 train_loss= 2.18619 accuracy= 0.67221 time= 3.19867
Epoch: 0121 train_loss= 2.17985 accuracy= 0.77174 time= 3.23175
Epoch: 0131 train_loss= 2.16291 accuracy= 0.67605 time= 3.23021
Epoch: 0141 train_loss= 2.11886 accuracy= 0.65291 time= 3.21046
Epoch: 0151 train_loss= 1.96011 accuracy= 0.69999 time= 3.18142
Epoch: 0161 train_loss= 2.11582 accuracy= 0.66534 time= 3.16561
Epoch: 0171 train_loss= 2.11066 accuracy= 0.67847 time= 3.18579
Epoch: 0181 train_loss= 2.10922 accuracy= 0.68322 time= 3.17884
Epoch: 0191 train_loss= 2.49517 accuracy= 0.62280 time= 3.19722
Epoch: 0201 train_loss= 2.84916 accuracy= 0.66493 time= 3.17395
Epoch: 0211 train_loss= 2.80895 accuracy= 0.62765 time= 3.17992
Epoch: 0221 train_loss= 2.59452 accuracy= 0.66796 time= 3.18178
Epoch: 0231 train_loss= 2.33139 accuracy= 0.62280 time= 3.15978
Epoch: 0241 train_loss= 1.96085 accuracy= 0.62259 time= 3.16603
Epoch: 0251 train_loss= 1.54064 accuracy= 0.63138 time= 3.16733
Epoch: 0261 train_loss= 1.48310 accuracy= 0.71545 time= 3.21193
Epoch: 0271 train_loss= 1.38715 accuracy= 0.63492 time= 3.17015
Epoch: 0281 train_loss= 1.34377 accuracy= 0.65250 time= 3.17361
Epoch: 0291 train_loss= 1.25405 accuracy= 0.64866 time= 3.18001

accuracy 0.69403
auc 0.61730
f1_score 0.32711
Job finished!



Initializing normal onedecoder
Namespace(alpha=1, beta=1, clique_size=50, cuda=False, dataset='cora_full', decoder=1, device=device(type='cuda'), dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=45, weight_decay=0.0005)
random seed: 62
Found existing ad data, loading...
feature_dim: 8710 hidden1_dim: 4355 hidden2_dim: 2177 nodes_num: 19793 anomaly_num: 4500

Start modeling
Epoch: 0001 train_loss= 62756.10938 accuracy= 0.66998 time= 2.67619
Epoch: 0011 train_loss= 53849.19531 accuracy= 0.60430 time= 2.64073
Epoch: 0021 train_loss= 14980.34668 accuracy= 0.64139 time= 2.61567
Epoch: 0031 train_loss= 11346.51758 accuracy= 0.61006 time= 2.61738
Epoch: 0041 train_loss= 9083.75098 accuracy= 0.63229 time= 2.61715
Epoch: 0051 train_loss= 7701.79980 accuracy= 0.62128 time= 2.62509
Epoch: 0061 train_loss= 6418.32080 accuracy= 0.65372 time= 2.61296
Epoch: 0071 train_loss= 6160.27295 accuracy= 0.61956 time= 2.60235
Epoch: 0081 train_loss= 5933.88525 accuracy= 0.59956 time= 2.60937
Epoch: 0091 train_loss= 5337.68652 accuracy= 0.64412 time= 2.58868
Epoch: 0101 train_loss= 2168.98120 accuracy= 0.74749 time= 2.61507
Epoch: 0111 train_loss= 1239.05566 accuracy= 0.61299 time= 2.60136
Epoch: 0121 train_loss= 878.68121 accuracy= 0.64311 time= 2.57592
Epoch: 0131 train_loss= 317.65939 accuracy= 0.64967 time= 2.58964
Epoch: 0141 train_loss= 190.83611 accuracy= 0.63836 time= 2.58714
Epoch: 0151 train_loss= 135.81238 accuracy= 0.64210 time= 2.60470
Epoch: 0161 train_loss= 104.63570 accuracy= 0.64311 time= 2.59935
Epoch: 0171 train_loss= 81.56304 accuracy= 0.64492 time= 2.59786
Epoch: 0181 train_loss= 66.88951 accuracy= 0.64058 time= 2.59917
Epoch: 0191 train_loss= 55.20640 accuracy= 0.64159 time= 2.60520
Epoch: 0201 train_loss= 51.79211 accuracy= 0.63401 time= 2.59242
Epoch: 0211 train_loss= 43.94157 accuracy= 0.63482 time= 2.59322
Epoch: 0221 train_loss= 39.10242 accuracy= 0.62805 time= 2.59917
Epoch: 0231 train_loss= 34.78233 accuracy= 0.61835 time= 2.60397
Epoch: 0241 train_loss= 32.92276 accuracy= 0.61107 time= 2.60658
Epoch: 0251 train_loss= 32.21091 accuracy= 0.60441 time= 2.59819
Epoch: 0261 train_loss= 30.79021 accuracy= 0.60835 time= 2.59913
Epoch: 0271 train_loss= 29.19567 accuracy= 0.60643 time= 2.59890
Epoch: 0281 train_loss= 28.23385 accuracy= 0.60552 time= 2.59828
Epoch: 0291 train_loss= 27.50721 accuracy= 0.60734 time= 2.60800

accuracy 0.60693
auc 0.25756
f1_score 0.13556
Job finished!



Initializing normal twodecoders
Namespace(alpha=1, beta=1, clique_size=50, cuda=False, dataset='cora_full', device=device(type='cuda'), dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=45, weight_decay=0.0005)
random seed: 934
Found existing ad data, loading...
feature_dim: 8710 hidden1_dim: 4355 hidden2_dim: 2177 nodes_num: 19793 anomaly_num: 4500

Start modeling
Epoch: 0001 train_loss= 63003.41797 feature_loss= 78.68526 structure_loss= 62924.73438 accuracy= 0.67352 accuracy_s= 0.77346 accuracy_f= 0.79791 time= 4.75861
Epoch: 0011 train_loss= 53381.98828 feature_loss= 6.08210 structure_loss= 53375.90625 accuracy= 0.60400 accuracy_s= 0.77467 accuracy_f= 0.79336 time= 4.69461
Epoch: 0021 train_loss= 14823.69043 feature_loss= 3.51461 structure_loss= 14820.17578 accuracy= 0.65766 accuracy_s= 0.79397 accuracy_f= 0.81822 time= 4.70251
Epoch: 0031 train_loss= 11347.25098 feature_loss= 2.96361 structure_loss= 11344.28711 accuracy= 0.60542 accuracy_s= 0.77730 accuracy_f= 0.82741 time= 4.73059
Epoch: 0041 train_loss= 9472.42090 feature_loss= 2.61564 structure_loss= 9469.80566 accuracy= 0.64624 accuracy_s= 0.78780 accuracy_f= 0.81468 time= 4.70539
Epoch: 0051 train_loss= 7495.24023 feature_loss= 2.24953 structure_loss= 7492.99072 accuracy= 0.65119 accuracy_s= 0.79174 accuracy_f= 0.82741 time= 4.70674
Epoch: 0061 train_loss= 6584.46484 feature_loss= 2.07667 structure_loss= 6582.38818 accuracy= 0.67483 accuracy_s= 0.79730 accuracy_f= 0.82418 time= 4.70800
Epoch: 0071 train_loss= 6331.36230 feature_loss= 2.07132 structure_loss= 6329.29102 accuracy= 0.62320 accuracy_s= 0.77992 accuracy_f= 0.82024 time= 4.68477
Epoch: 0081 train_loss= 6225.57910 feature_loss= 2.06208 structure_loss= 6223.51709 accuracy= 0.62512 accuracy_s= 0.77265 accuracy_f= 0.82650 time= 4.66832
Epoch: 0091 train_loss= 6137.22314 feature_loss= 2.05194 structure_loss= 6135.17139 accuracy= 0.60046 accuracy_s= 0.77436 accuracy_f= 0.81933 time= 4.65840
Epoch: 0101 train_loss= 5961.64502 feature_loss= 2.04264 structure_loss= 5959.60254 accuracy= 0.63462 accuracy_s= 0.77517 accuracy_f= 0.79185 time= 4.69288
Epoch: 0111 train_loss= 5557.88672 feature_loss= 2.42994 structure_loss= 5555.45654 accuracy= 0.63553 accuracy_s= 0.77426 accuracy_f= 0.78790 time= 4.71146
Epoch: 0121 train_loss= 4900.12598 feature_loss= 5.51199 structure_loss= 4894.61377 accuracy= 0.59834 accuracy_s= 0.77315 accuracy_f= 0.79276 time= 4.71282
Epoch: 0131 train_loss= 2960.95288 feature_loss= 2.43389 structure_loss= 2958.51904 accuracy= 0.64755 accuracy_s= 0.79831 accuracy_f= 0.78437 time= 4.67582
Epoch: 0141 train_loss= 1810.16028 feature_loss= 2.24195 structure_loss= 1807.91833 accuracy= 0.65816 accuracy_s= 0.79265 accuracy_f= 0.78194 time= 4.68445
Epoch: 0151 train_loss= 303.90424 feature_loss= 2.17774 structure_loss= 301.72650 accuracy= 0.65634 accuracy_s= 0.79326 accuracy_f= 0.78508 time= 4.68767
Epoch: 0161 train_loss= 203.21820 feature_loss= 2.12562 structure_loss= 201.09258 accuracy= 0.65564 accuracy_s= 0.79417 accuracy_f= 0.79255 time= 4.68015
Epoch: 0171 train_loss= 148.37413 feature_loss= 3.06407 structure_loss= 145.31006 accuracy= 0.73213 accuracy_s= 0.79629 accuracy_f= 0.78780 time= 4.65809
Epoch: 0181 train_loss= 111.18887 feature_loss= 2.72117 structure_loss= 108.46770 accuracy= 0.74577 accuracy_s= 0.79447 accuracy_f= 0.78194 time= 4.64679
Epoch: 0191 train_loss= 95.70564 feature_loss= 2.78505 structure_loss= 92.92060 accuracy= 0.69484 accuracy_s= 0.79488 accuracy_f= 0.79700 time= 4.60634
Epoch: 0201 train_loss= 85.22986 feature_loss= 6.81915 structure_loss= 78.41071 accuracy= 0.75992 accuracy_s= 0.79174 accuracy_f= 0.78639 time= 4.63156
Epoch: 0211 train_loss= 75.37995 feature_loss= 3.03428 structure_loss= 72.34567 accuracy= 0.73849 accuracy_s= 0.79195 accuracy_f= 0.78528 time= 4.61855
Epoch: 0221 train_loss= 69.83445 feature_loss= 2.54583 structure_loss= 67.28862 accuracy= 0.65008 accuracy_s= 0.79023 accuracy_f= 0.81670 time= 4.64166
Epoch: 0231 train_loss= 64.18234 feature_loss= 2.67314 structure_loss= 61.50920 accuracy= 0.62088 accuracy_s= 0.78225 accuracy_f= 0.80033 time= 4.59943
Epoch: 0241 train_loss= 58.04381 feature_loss= 2.21918 structure_loss= 55.82463 accuracy= 0.64412 accuracy_s= 0.77992 accuracy_f= 0.80771 time= 4.61184
Epoch: 0251 train_loss= 52.64256 feature_loss= 2.11645 structure_loss= 50.52611 accuracy= 0.64563 accuracy_s= 0.77436 accuracy_f= 0.80700 time= 4.61978
Epoch: 0261 train_loss= 52.50920 feature_loss= 2.08315 structure_loss= 50.42606 accuracy= 0.63805 accuracy_s= 0.77346 accuracy_f= 0.80478 time= 4.60836
Epoch: 0271 train_loss= 48.47710 feature_loss= 1.93263 structure_loss= 46.54447 accuracy= 0.62613 accuracy_s= 0.77285 accuracy_f= 0.80569 time= 4.61239
Epoch: 0281 train_loss= 47.26565 feature_loss= 2.02832 structure_loss= 45.23733 accuracy= 0.64331 accuracy_s= 0.77265 accuracy_f= 0.79892 time= 4.63339
Epoch: 0291 train_loss= 43.52317 feature_loss= 1.79693 structure_loss= 41.72623 accuracy= 0.63937 accuracy_s= 0.77265 accuracy_f= 0.79528 time= 4.63788

accuracy 0.62552
accuracy_s 0.77265
accuracy_f 0.79781
auc 0.37725
f1_score 0.17644
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=50, cuda=False, dataset='cora_full', decoder=1, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=45, weight_decay=0.0005)
random seed: 85
Found existing ad data, loading...
feature_dim: 8710 hidden1_dim: 4355 hidden2_dim: 2177 nodes_num: 19793 anomaly_num: 4500

Start modeling
using wavelet scattering transform
y_features shape after scatting (19793, 113230) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([19793, 2177]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 76753.00000 accuracy= 0.67170 time= 1.40460
Epoch: 0011 train_loss= 30611.27344 accuracy= 0.77558 time= 1.34823
Epoch: 0021 train_loss= 23379.62305 accuracy= 0.64947 time= 1.36145
Epoch: 0031 train_loss= 19450.86719 accuracy= 0.60683 time= 1.35864
Epoch: 0041 train_loss= 16960.51758 accuracy= 0.60764 time= 1.36480
Epoch: 0051 train_loss= 15190.29590 accuracy= 0.60804 time= 1.37386
Epoch: 0061 train_loss= 13826.81152 accuracy= 0.60865 time= 1.38618
Epoch: 0071 train_loss= 12730.66602 accuracy= 0.60875 time= 1.39429
Epoch: 0081 train_loss= 11820.42871 accuracy= 0.60885 time= 1.40041
Epoch: 0091 train_loss= 11036.10156 accuracy= 0.60895 time= 1.40692
Epoch: 0101 train_loss= 10363.46777 accuracy= 0.60885 time= 1.40247
Epoch: 0111 train_loss= 9760.32031 accuracy= 0.60845 time= 1.41090
Epoch: 0121 train_loss= 9232.86035 accuracy= 0.60875 time= 1.41497
Epoch: 0131 train_loss= 8754.28223 accuracy= 0.60915 time= 1.41888
Epoch: 0141 train_loss= 8324.86328 accuracy= 0.60905 time= 1.42411
Epoch: 0151 train_loss= 7928.52246 accuracy= 0.60946 time= 1.42927
Epoch: 0161 train_loss= 7567.85010 accuracy= 0.60956 time= 1.42984
Epoch: 0171 train_loss= 7231.48242 accuracy= 0.60976 time= 1.43714
Epoch: 0181 train_loss= 6922.39062 accuracy= 0.61006 time= 1.43729
Epoch: 0191 train_loss= 6630.91162 accuracy= 0.60976 time= 1.44607
Epoch: 0201 train_loss= 6365.78027 accuracy= 0.60996 time= 1.44717
Epoch: 0211 train_loss= 6112.02148 accuracy= 0.61027 time= 1.44643
Epoch: 0221 train_loss= 5873.31982 accuracy= 0.61017 time= 1.45210
Epoch: 0231 train_loss= 5649.55420 accuracy= 0.61057 time= 1.45444
Epoch: 0241 train_loss= 5437.55176 accuracy= 0.61128 time= 1.46117
Epoch: 0251 train_loss= 5240.25391 accuracy= 0.61107 time= 1.46977
Epoch: 0261 train_loss= 5046.97363 accuracy= 0.61158 time= 1.47363
Epoch: 0271 train_loss= 4861.89990 accuracy= 0.61077 time= 1.48683
Epoch: 0281 train_loss= 4687.79443 accuracy= 0.61138 time= 1.48921
Epoch: 0291 train_loss= 4522.81934 accuracy= 0.61097 time= 1.48645

accuracy 0.61118
auc 0.30189
f1_score 0.14489
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=50, cuda=False, dataset='cora_full', decoder=0, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=45, weight_decay=0.0005)
random seed: 666
Found existing ad data, loading...
feature_dim: 8710 hidden1_dim: 4355 hidden2_dim: 2177 nodes_num: 19793 anomaly_num: 4500

Start modeling
using wavelet scattering transform
y_features shape after scatting (19793, 113230) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([19793, 2177]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 89.43419 accuracy= 0.64503 time= 2.00282
Epoch: 0011 train_loss= 25.50509 accuracy= 0.63149 time= 2.00166
Epoch: 0021 train_loss= 16.72626 accuracy= 0.61542 time= 2.01090
Epoch: 0031 train_loss= 10.18117 accuracy= 0.61966 time= 1.97743
Epoch: 0041 train_loss= 7.64614 accuracy= 0.69737 time= 2.02240
Epoch: 0051 train_loss= 7.18955 accuracy= 0.70141 time= 2.01532
Epoch: 0061 train_loss= 6.14549 accuracy= 0.72728 time= 1.99363
Epoch: 0071 train_loss= 5.19724 accuracy= 0.73930 time= 1.99292
Epoch: 0081 train_loss= 4.26423 accuracy= 0.73597 time= 1.99997
Epoch: 0091 train_loss= 3.32828 accuracy= 0.70959 time= 1.99969
Epoch: 0101 train_loss= 2.58872 accuracy= 0.65392 time= 2.01994
Epoch: 0111 train_loss= 2.19384 accuracy= 0.68019 time= 2.02234
Epoch: 0121 train_loss= 2.07958 accuracy= 0.66422 time= 2.01913
Epoch: 0131 train_loss= 1.83724 accuracy= 0.72637 time= 2.03012
Epoch: 0141 train_loss= 1.86442 accuracy= 0.72334 time= 2.05239
Epoch: 0151 train_loss= 1.94871 accuracy= 0.63341 time= 2.04720
Epoch: 0161 train_loss= 1.88617 accuracy= 0.66251 time= 2.05969
Epoch: 0171 train_loss= 1.87460 accuracy= 0.65735 time= 2.06553
Epoch: 0181 train_loss= 2.01935 accuracy= 0.69555 time= 2.08100
Epoch: 0191 train_loss= 2.56605 accuracy= 0.66837 time= 2.07727
Epoch: 0201 train_loss= 3.58794 accuracy= 0.68160 time= 2.09063
Epoch: 0211 train_loss= 4.01313 accuracy= 0.63714 time= 2.07745
Epoch: 0221 train_loss= 2.85414 accuracy= 0.63159 time= 2.07415
Epoch: 0231 train_loss= 2.30156 accuracy= 0.64068 time= 2.02487
Epoch: 0241 train_loss= 2.15768 accuracy= 0.67554 time= 2.03613
Epoch: 0251 train_loss= 1.77234 accuracy= 0.67655 time= 2.06028
Epoch: 0261 train_loss= 1.94737 accuracy= 0.65099 time= 2.04161
Epoch: 0271 train_loss= 1.76439 accuracy= 0.68059 time= 2.01529
Epoch: 0281 train_loss= 1.77286 accuracy= 0.66776 time= 2.00567
Epoch: 0291 train_loss= 1.74112 accuracy= 0.67382 time= 2.00828

accuracy 0.68312
auc 0.57701
f1_score 0.30311
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=50, cuda=False, dataset='cora_full', decoder=1, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=45, weight_decay=0.0005)
random seed: 662
Found existing ad data, loading...
feature_dim: 8710 hidden1_dim: 4355 hidden2_dim: 2177 nodes_num: 19793 anomaly_num: 4500

Start modeling
using wavelet scattering transform
y_features shape after scatting (19793, 113230) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 51 steps!

y_features shape after FMS torch.Size([19793, 2177]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 66400.42188 accuracy= 0.67716 time= 1.37336
Epoch: 0011 train_loss= 30966.26562 accuracy= 0.67716 time= 1.35646
Epoch: 0021 train_loss= 23904.88281 accuracy= 0.63947 time= 1.36203
Epoch: 0031 train_loss= 20162.88281 accuracy= 0.61118 time= 1.36301
Epoch: 0041 train_loss= 17709.95312 accuracy= 0.60946 time= 1.36304
Epoch: 0051 train_loss= 15922.63281 accuracy= 0.60936 time= 1.37632
Epoch: 0061 train_loss= 14547.72559 accuracy= 0.61017 time= 1.39028
Epoch: 0071 train_loss= 13439.74902 accuracy= 0.61087 time= 1.39731
Epoch: 0081 train_loss= 12505.14258 accuracy= 0.61128 time= 1.39783
Epoch: 0091 train_loss= 11704.16016 accuracy= 0.61057 time= 1.40369
Epoch: 0101 train_loss= 10999.73633 accuracy= 0.61118 time= 1.40636
Epoch: 0111 train_loss= 10362.33594 accuracy= 0.61188 time= 1.41223
Epoch: 0121 train_loss= 9793.87988 accuracy= 0.61198 time= 1.41749
Epoch: 0131 train_loss= 9272.66992 accuracy= 0.61259 time= 1.42235
Epoch: 0141 train_loss= 8793.93945 accuracy= 0.61330 time= 1.42785
Epoch: 0151 train_loss= 8352.74219 accuracy= 0.61310 time= 1.43105
Epoch: 0161 train_loss= 7940.42480 accuracy= 0.61299 time= 1.43703
Epoch: 0171 train_loss= 7546.15430 accuracy= 0.61289 time= 1.43872
Epoch: 0181 train_loss= 7171.93652 accuracy= 0.61239 time= 1.44200
Epoch: 0191 train_loss= 7007.71289 accuracy= 0.61158 time= 1.44619
Epoch: 0201 train_loss= 6570.44141 accuracy= 0.61198 time= 1.45094
Epoch: 0211 train_loss= 6217.08447 accuracy= 0.61097 time= 1.46065
Epoch: 0221 train_loss= 5893.10205 accuracy= 0.61037 time= 1.47461
Epoch: 0231 train_loss= 6962.27686 accuracy= 0.61279 time= 1.48242
Epoch: 0241 train_loss= 5982.61084 accuracy= 0.61188 time= 1.48640
Epoch: 0251 train_loss= 5517.18604 accuracy= 0.61148 time= 1.48894
Epoch: 0261 train_loss= 5171.59180 accuracy= 0.61118 time= 1.48720
Epoch: 0271 train_loss= 4880.63672 accuracy= 0.61057 time= 1.48904
Epoch: 0281 train_loss= 4620.77197 accuracy= 0.61077 time= 1.48724
Epoch: 0291 train_loss= 4384.65039 accuracy= 0.61118 time= 1.48661

accuracy 0.61057
auc 0.29447
f1_score 0.14356
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=0, beta=1, clique_size=50, cuda=False, dataset='cora_full', decoder=0, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=45, weight_decay=0.0005)
random seed: 9
Found existing ad data, loading...
feature_dim: 8710 hidden1_dim: 4355 hidden2_dim: 2177 nodes_num: 19793 anomaly_num: 4500

Start modeling
using wavelet scattering transform
y_features shape after scatting (19793, 113230) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 47 steps!

y_features shape after FMS torch.Size([19793, 2177]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 89.40128 accuracy= 0.64806 time= 2.07322
Epoch: 0011 train_loss= 19.31987 accuracy= 0.61683 time= 2.00446
Epoch: 0021 train_loss= 9.14297 accuracy= 0.62168 time= 2.01866
Epoch: 0031 train_loss= 6.17191 accuracy= 0.64028 time= 1.99126
Epoch: 0041 train_loss= 5.86095 accuracy= 0.68423 time= 1.95756
Epoch: 0051 train_loss= 5.47014 accuracy= 0.69787 time= 2.02044
Epoch: 0061 train_loss= 5.29658 accuracy= 0.70919 time= 2.01020
Epoch: 0071 train_loss= 5.11725 accuracy= 0.72121 time= 2.00657
Epoch: 0081 train_loss= 4.93887 accuracy= 0.75830 time= 1.97015
Epoch: 0091 train_loss= 4.92974 accuracy= 0.67039 time= 2.02573
Epoch: 0101 train_loss= 3.75040 accuracy= 0.64139 time= 2.01669
Epoch: 0111 train_loss= 3.15802 accuracy= 0.68939 time= 1.99995
Epoch: 0121 train_loss= 2.88649 accuracy= 0.65220 time= 2.01687
Epoch: 0131 train_loss= 3.23891 accuracy= 0.69120 time= 1.98841
Epoch: 0141 train_loss= 4.01154 accuracy= 0.69464 time= 1.99938
Epoch: 0151 train_loss= 3.18092 accuracy= 0.62835 time= 1.97887
Epoch: 0161 train_loss= 2.96436 accuracy= 0.64351 time= 2.03755
Epoch: 0171 train_loss= 2.90618 accuracy= 0.62421 time= 2.03301
Epoch: 0181 train_loss= 3.41830 accuracy= 0.69383 time= 1.99853
Epoch: 0191 train_loss= 4.20625 accuracy= 0.67928 time= 2.00253
Epoch: 0201 train_loss= 4.00185 accuracy= 0.65968 time= 2.00608
Epoch: 0211 train_loss= 3.65547 accuracy= 0.64998 time= 2.00429
Epoch: 0221 train_loss= 3.64112 accuracy= 0.64887 time= 2.01135
Epoch: 0231 train_loss= 3.30809 accuracy= 0.61906 time= 2.00902
Epoch: 0241 train_loss= 3.22743 accuracy= 0.63189 time= 1.98974
Epoch: 0251 train_loss= 2.67052 accuracy= 0.65523 time= 2.02902
Epoch: 0261 train_loss= 2.57176 accuracy= 0.66483 time= 2.00958
Epoch: 0271 train_loss= 2.43660 accuracy= 0.67615 time= 2.02950
Epoch: 0281 train_loss= 2.36194 accuracy= 0.67534 time= 2.02361
Epoch: 0291 train_loss= 2.19038 accuracy= 0.67726 time= 2.00138

accuracy 0.67968
auc 0.56708
f1_score 0.29556
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=50, cuda=False, dataset='cora_full', decoder=1, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=45, weight_decay=0.0005)
random seed: 973
Found existing ad data, loading...
feature_dim: 8710 hidden1_dim: 4355 hidden2_dim: 2177 nodes_num: 19793 anomaly_num: 4500

Start modeling
using wavelet scattering transform
y_features shape after scatting (19793, 113230) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([19793, 2177]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 1567.76086 accuracy= 0.69848 time= 2.14109
Epoch: 0011 train_loss= 31.29501 accuracy= 0.62936 time= 2.12056
Epoch: 0021 train_loss= 14.21078 accuracy= 0.63684 time= 2.11154
Epoch: 0031 train_loss= 131.54279 accuracy= 0.63401 time= 2.10897
Epoch: 0041 train_loss= 14.28595 accuracy= 0.62209 time= 2.11932
Epoch: 0051 train_loss= 590.21356 accuracy= 0.64129 time= 2.11453
Epoch: 0061 train_loss= 3576.75610 accuracy= 0.64775 time= 2.11182
Epoch: 0071 train_loss= 1098.32300 accuracy= 0.63664 time= 2.14529
Epoch: 0081 train_loss= 380.41629 accuracy= 0.63745 time= 2.14335
Epoch: 0091 train_loss= 849.67755 accuracy= 0.64674 time= 2.14979
Epoch: 0101 train_loss= 754.02527 accuracy= 0.63977 time= 2.14304
Epoch: 0111 train_loss= 82.01816 accuracy= 0.65058 time= 2.15371
Epoch: 0121 train_loss= 68.52175 accuracy= 0.64513 time= 2.16232
Epoch: 0131 train_loss= 95.13302 accuracy= 0.64604 time= 2.15953
Epoch: 0141 train_loss= 15.97883 accuracy= 0.64169 time= 2.18144
Epoch: 0151 train_loss= 6.27663 accuracy= 0.60926 time= 2.18877
Epoch: 0161 train_loss= 2.64191 accuracy= 0.60885 time= 2.21397
Epoch: 0171 train_loss= 30.17025 accuracy= 0.64644 time= 2.20604
Epoch: 0181 train_loss= 14.90249 accuracy= 0.63421 time= 2.22208
Epoch: 0191 train_loss= 14.47261 accuracy= 0.64129 time= 2.21800
Epoch: 0201 train_loss= 5.45336 accuracy= 0.62199 time= 2.21095
Epoch: 0211 train_loss= 3.94858 accuracy= 0.60481 time= 2.20206
Epoch: 0221 train_loss= 8.41644 accuracy= 0.61865 time= 2.18167
Epoch: 0231 train_loss= 10.58641 accuracy= 0.63613 time= 2.20835
Epoch: 0241 train_loss= 102.12097 accuracy= 0.64624 time= 2.17693
Epoch: 0251 train_loss= 81.13324 accuracy= 0.64563 time= 2.18666
Epoch: 0261 train_loss= 21.72300 accuracy= 0.64058 time= 2.19324
Epoch: 0271 train_loss= 32.27330 accuracy= 0.64503 time= 2.17298
Epoch: 0281 train_loss= 3.03824 accuracy= 0.60562 time= 2.19067
Epoch: 0291 train_loss= 24.45303 accuracy= 0.63603 time= 2.19381

accuracy 0.63593
auc 0.46956
f1_score 0.19933
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=50, cuda=False, dataset='cora_full', decoder=0, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=45, weight_decay=0.0005)
random seed: 952
Found existing ad data, loading...
feature_dim: 8710 hidden1_dim: 4355 hidden2_dim: 2177 nodes_num: 19793 anomaly_num: 4500

Start modeling
using wavelet scattering transform
y_features shape after scatting (19793, 113230) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([19793, 2177]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 88.66193 accuracy= 0.67979 time= 2.78678
Epoch: 0011 train_loss= 5.17477 accuracy= 0.65392 time= 2.77538
Epoch: 0021 train_loss= 3.92982 accuracy= 0.70848 time= 2.76368
Epoch: 0031 train_loss= 43.91180 accuracy= 0.66372 time= 2.77652
Epoch: 0041 train_loss= 3.43868 accuracy= 0.68019 time= 2.77160
Epoch: 0051 train_loss= 3.51821 accuracy= 0.67362 time= 2.76595
Epoch: 0061 train_loss= 3.46659 accuracy= 0.67878 time= 2.76905
Epoch: 0071 train_loss= 3.44127 accuracy= 0.67837 time= 2.78997
Epoch: 0081 train_loss= 3.50501 accuracy= 0.67918 time= 2.78875
Epoch: 0091 train_loss= 3.57143 accuracy= 0.66362 time= 2.80775
Epoch: 0101 train_loss= 4.03339 accuracy= 0.68686 time= 2.82457
Epoch: 0111 train_loss= 3.77506 accuracy= 0.68181 time= 2.77785
Epoch: 0121 train_loss= 3.71580 accuracy= 0.68615 time= 2.75957
Epoch: 0131 train_loss= 3.34484 accuracy= 0.69413 time= 2.81026
Epoch: 0141 train_loss= 3.20039 accuracy= 0.69161 time= 2.77961
Epoch: 0151 train_loss= 3.05034 accuracy= 0.69242 time= 2.81111
Epoch: 0161 train_loss= 2.95271 accuracy= 0.69211 time= 2.76762
Epoch: 0171 train_loss= 2.87561 accuracy= 0.68544 time= 2.80465
Epoch: 0181 train_loss= 2.79558 accuracy= 0.68726 time= 2.80226
Epoch: 0191 train_loss= 2.72726 accuracy= 0.69171 time= 2.82178
Epoch: 0201 train_loss= 2.65410 accuracy= 0.69029 time= 2.81879
Epoch: 0211 train_loss= 2.58497 accuracy= 0.68797 time= 2.79270
Epoch: 0221 train_loss= 2.52473 accuracy= 0.69151 time= 2.79017
Epoch: 0231 train_loss= 2.44800 accuracy= 0.69262 time= 2.78991
Epoch: 0241 train_loss= 2.39036 accuracy= 0.68949 time= 2.80156
Epoch: 0251 train_loss= 2.32651 accuracy= 0.68817 time= 2.81360
Epoch: 0261 train_loss= 2.26725 accuracy= 0.69282 time= 2.78868
Epoch: 0271 train_loss= 2.19740 accuracy= 0.69605 time= 2.80802
Epoch: 0281 train_loss= 2.15915 accuracy= 0.69050 time= 2.77574
Epoch: 0291 train_loss= 2.10508 accuracy= 0.68817 time= 2.78840

accuracy 0.69141
auc 0.58737
f1_score 0.32133
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=50, cuda=False, dataset='cora_full', decoder=1, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=45, weight_decay=0.0005)
random seed: 766
Found existing ad data, loading...
feature_dim: 8710 hidden1_dim: 4355 hidden2_dim: 2177 nodes_num: 19793 anomaly_num: 4500

Start modeling
using wavelet scattering transform
y_features shape after scatting (19793, 113230) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 71 steps!

y_features shape after FMS torch.Size([19793, 2177]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 0.44984 accuracy= 0.61663 time= 2.13498
Epoch: 0011 train_loss= 0.83712 accuracy= 0.60673 time= 2.11488
Epoch: 0021 train_loss= 0.36842 accuracy= 0.60734 time= 2.12537
Epoch: 0031 train_loss= 0.36847 accuracy= 0.60723 time= 2.13653
Epoch: 0041 train_loss= 0.36775 accuracy= 0.60723 time= 2.16474
Epoch: 0051 train_loss= 0.36837 accuracy= 0.60713 time= 2.15364
Epoch: 0061 train_loss= 0.36810 accuracy= 0.60703 time= 2.16820
Epoch: 0071 train_loss= 0.36817 accuracy= 0.60703 time= 2.17969
Epoch: 0081 train_loss= 0.36814 accuracy= 0.60703 time= 2.19382
Epoch: 0091 train_loss= 0.36817 accuracy= 0.60703 time= 2.20245
Epoch: 0101 train_loss= 0.36820 accuracy= 0.60703 time= 2.21039
Epoch: 0111 train_loss= 0.36823 accuracy= 0.60703 time= 2.21930
Epoch: 0121 train_loss= 0.36825 accuracy= 0.60703 time= 2.22528
Epoch: 0131 train_loss= 0.36827 accuracy= 0.60703 time= 2.23058
Epoch: 0141 train_loss= 0.36828 accuracy= 0.60703 time= 2.22552
Epoch: 0151 train_loss= 0.36829 accuracy= 0.60703 time= 2.24102
Epoch: 0161 train_loss= 0.36830 accuracy= 0.60703 time= 2.26123
Epoch: 0171 train_loss= 0.36830 accuracy= 0.60703 time= 2.27254
Epoch: 0181 train_loss= 0.36831 accuracy= 0.60703 time= 2.27686
Epoch: 0191 train_loss= 0.36832 accuracy= 0.60703 time= 2.26320
Epoch: 0201 train_loss= 0.36833 accuracy= 0.60703 time= 2.25327
Epoch: 0211 train_loss= 0.36833 accuracy= 0.60703 time= 2.24861
Epoch: 0221 train_loss= 0.36834 accuracy= 0.60703 time= 2.25135
Epoch: 0231 train_loss= 0.36835 accuracy= 0.60703 time= 2.24042
Epoch: 0241 train_loss= 0.36836 accuracy= 0.60703 time= 2.24109
Epoch: 0251 train_loss= 0.36836 accuracy= 0.60703 time= 2.23593
Epoch: 0261 train_loss= 0.36837 accuracy= 0.60703 time= 2.23391
Epoch: 0271 train_loss= 0.36837 accuracy= 0.60703 time= 2.22717
Epoch: 0281 train_loss= 0.36838 accuracy= 0.60703 time= 2.22669
Epoch: 0291 train_loss= 0.36838 accuracy= 0.60703 time= 2.22974

accuracy 0.60703
auc 0.25647
f1_score 0.13578
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=1, beta=1, clique_size=50, cuda=False, dataset='cora_full', decoder=0, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=45, weight_decay=0.0005)
random seed: 816
Found existing ad data, loading...
feature_dim: 8710 hidden1_dim: 4355 hidden2_dim: 2177 nodes_num: 19793 anomaly_num: 4500

Start modeling
using wavelet scattering transform
y_features shape after scatting (19793, 113230) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 51 steps!

y_features shape after FMS torch.Size([19793, 2177]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 18.89071 accuracy= 0.68201 time= 2.79413
Epoch: 0011 train_loss= 0.99828 accuracy= 0.67787 time= 2.77372
Epoch: 0021 train_loss= 0.99267 accuracy= 0.67968 time= 2.75400
Epoch: 0031 train_loss= 0.99130 accuracy= 0.68120 time= 2.78278
Epoch: 0041 train_loss= 0.99085 accuracy= 0.68160 time= 2.76780
Epoch: 0051 train_loss= 0.99055 accuracy= 0.68120 time= 2.69388
Epoch: 0061 train_loss= 0.99047 accuracy= 0.68171 time= 2.75521
Epoch: 0071 train_loss= 0.99044 accuracy= 0.68231 time= 2.74264
Epoch: 0081 train_loss= 0.99044 accuracy= 0.68171 time= 2.73080
Epoch: 0091 train_loss= 0.99043 accuracy= 0.68201 time= 2.76282
Epoch: 0101 train_loss= 0.99043 accuracy= 0.68191 time= 2.76604
Epoch: 0111 train_loss= 0.99042 accuracy= 0.68171 time= 2.84353
Epoch: 0121 train_loss= 0.99042 accuracy= 0.68181 time= 2.86260
Epoch: 0131 train_loss= 0.99044 accuracy= 0.68201 time= 2.85268
Epoch: 0141 train_loss= 0.99043 accuracy= 0.68181 time= 2.89594
Epoch: 0151 train_loss= 0.99042 accuracy= 0.68191 time= 2.83654
Epoch: 0161 train_loss= 0.99042 accuracy= 0.68191 time= 2.81169
Epoch: 0171 train_loss= 0.99042 accuracy= 0.68191 time= 2.80423
Epoch: 0181 train_loss= 0.99043 accuracy= 0.68191 time= 2.78442
Epoch: 0191 train_loss= 0.99045 accuracy= 0.68181 time= 2.81447
Epoch: 0201 train_loss= 1.13070 accuracy= 0.64705 time= 2.82803
Epoch: 0211 train_loss= 1.11043 accuracy= 0.66079 time= 2.79987
Epoch: 0221 train_loss= 1.09311 accuracy= 0.65685 time= 2.84589
Epoch: 0231 train_loss= 1.07860 accuracy= 0.65058 time= 2.84664
Epoch: 0241 train_loss= 1.10761 accuracy= 0.64745 time= 2.82368
Epoch: 0251 train_loss= 1.05588 accuracy= 0.64765 time= 2.82330
Epoch: 0261 train_loss= 1.04721 accuracy= 0.65048 time= 2.81717
Epoch: 0271 train_loss= 1.03951 accuracy= 0.64978 time= 2.81813
Epoch: 0281 train_loss= 1.03272 accuracy= 0.64604 time= 2.78554
Epoch: 0291 train_loss= 1.02648 accuracy= 0.63967 time= 2.83608

accuracy 0.69353
auc 0.59639
f1_score 0.32600
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=50, cuda=False, dataset='cora_full', decoder=1, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=45, weight_decay=0.0005)
random seed: 942
Found existing ad data, loading...
feature_dim: 8710 hidden1_dim: 4355 hidden2_dim: 2177 nodes_num: 19793 anomaly_num: 4500

Start modeling
using wavelet scattering transform
y_features shape after scatting (19793, 113230) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([19793, 2177]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 301.76199 accuracy= 0.67706 time= 1.55988
Epoch: 0011 train_loss= 7.38177 accuracy= 0.62219 time= 1.54521
Epoch: 0021 train_loss= 0.54964 accuracy= 0.60744 time= 1.54045
Epoch: 0031 train_loss= 0.38557 accuracy= 0.60582 time= 1.53440
Epoch: 0041 train_loss= 0.36849 accuracy= 0.60673 time= 1.55047
Epoch: 0051 train_loss= 0.35786 accuracy= 0.60734 time= 1.55935
Epoch: 0061 train_loss= 0.35259 accuracy= 0.60723 time= 1.56042
Epoch: 0071 train_loss= 0.34879 accuracy= 0.60713 time= 1.56570
Epoch: 0081 train_loss= 0.34521 accuracy= 0.60663 time= 1.56206
Epoch: 0091 train_loss= 0.34167 accuracy= 0.60703 time= 1.57870
Epoch: 0101 train_loss= 0.33829 accuracy= 0.60723 time= 1.57973
Epoch: 0111 train_loss= 0.33476 accuracy= 0.60723 time= 1.58692
Epoch: 0121 train_loss= 0.33131 accuracy= 0.60794 time= 1.58993
Epoch: 0131 train_loss= 0.32785 accuracy= 0.60713 time= 1.59520
Epoch: 0141 train_loss= 0.32468 accuracy= 0.60673 time= 1.60172
Epoch: 0151 train_loss= 0.32119 accuracy= 0.60673 time= 1.60372
Epoch: 0161 train_loss= 0.31812 accuracy= 0.60703 time= 1.59956
Epoch: 0171 train_loss= 0.31479 accuracy= 0.60693 time= 1.59702
Epoch: 0181 train_loss= 0.31183 accuracy= 0.60713 time= 1.60342
Epoch: 0191 train_loss= 0.30871 accuracy= 0.60683 time= 1.61040
Epoch: 0201 train_loss= 0.30549 accuracy= 0.60673 time= 1.60967
Epoch: 0211 train_loss= 0.30236 accuracy= 0.60653 time= 1.61578
Epoch: 0221 train_loss= 0.29936 accuracy= 0.60633 time= 1.62142
Epoch: 0231 train_loss= 0.29589 accuracy= 0.60602 time= 1.61968
Epoch: 0241 train_loss= 0.29304 accuracy= 0.60582 time= 1.62131
Epoch: 0251 train_loss= 0.29009 accuracy= 0.60653 time= 1.63051
Epoch: 0261 train_loss= 0.28695 accuracy= 0.60703 time= 1.63076
Epoch: 0271 train_loss= 0.28424 accuracy= 0.60633 time= 1.63223
Epoch: 0281 train_loss= 0.28147 accuracy= 0.60713 time= 1.63248
Epoch: 0291 train_loss= 0.27888 accuracy= 0.60592 time= 1.63103

accuracy 0.60643
auc 0.27397
f1_score 0.13444
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=50, cuda=False, dataset='cora_full', decoder=0, device=device(type='cuda'), dim_reduce=0, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=45, weight_decay=0.0005)
random seed: 787
Found existing ad data, loading...
feature_dim: 8710 hidden1_dim: 4355 hidden2_dim: 2177 nodes_num: 19793 anomaly_num: 4500

Start modeling
using wavelet scattering transform
y_features shape after scatting (19793, 113230) <class 'numpy.ndarray'>
using PCA to reduce dim

y_features shape after PCA torch.Size([19793, 2177]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 88.54006 accuracy= 0.66422 time= 2.17426
Epoch: 0011 train_loss= 8.74283 accuracy= 0.65422 time= 2.20336
Epoch: 0021 train_loss= 3.76047 accuracy= 0.70303 time= 2.19965
Epoch: 0031 train_loss= 2.60691 accuracy= 0.64796 time= 2.20196
Epoch: 0041 train_loss= 2.13259 accuracy= 0.66847 time= 2.21356
Epoch: 0051 train_loss= 1.97722 accuracy= 0.67979 time= 2.17984
Epoch: 0061 train_loss= 1.91944 accuracy= 0.68191 time= 2.17539
Epoch: 0071 train_loss= 1.89364 accuracy= 0.68302 time= 2.19196
Epoch: 0081 train_loss= 1.87951 accuracy= 0.68272 time= 2.20232
Epoch: 0091 train_loss= 1.89200 accuracy= 0.68352 time= 2.18010
Epoch: 0101 train_loss= 1.86631 accuracy= 0.68949 time= 2.23244
Epoch: 0111 train_loss= 1.85436 accuracy= 0.68666 time= 2.20748
Epoch: 0121 train_loss= 1.84685 accuracy= 0.70151 time= 2.20235
Epoch: 0131 train_loss= 1.84115 accuracy= 0.69232 time= 2.20822
Epoch: 0141 train_loss= 1.85269 accuracy= 0.70798 time= 2.24582
Epoch: 0151 train_loss= 1.82880 accuracy= 0.69605 time= 2.26798
Epoch: 0161 train_loss= 1.82598 accuracy= 0.70990 time= 2.29586
Epoch: 0171 train_loss= 2.02674 accuracy= 0.70121 time= 2.30603
Epoch: 0181 train_loss= 1.92185 accuracy= 0.67756 time= 2.27952
Epoch: 0191 train_loss= 1.83809 accuracy= 0.68272 time= 2.29175
Epoch: 0201 train_loss= 2.14068 accuracy= 0.71333 time= 2.25490
Epoch: 0211 train_loss= 1.91181 accuracy= 0.69646 time= 2.26193
Epoch: 0221 train_loss= 2.28527 accuracy= 0.67190 time= 2.26024
Epoch: 0231 train_loss= 2.75468 accuracy= 0.66311 time= 2.28861
Epoch: 0241 train_loss= 2.90439 accuracy= 0.67847 time= 2.24130
Epoch: 0251 train_loss= 2.94202 accuracy= 0.66827 time= 2.22951
Epoch: 0261 train_loss= 2.19366 accuracy= 0.68150 time= 2.26035
Epoch: 0271 train_loss= 2.11370 accuracy= 0.67696 time= 2.27471
Epoch: 0281 train_loss= 2.04484 accuracy= 0.67079 time= 2.24036
Epoch: 0291 train_loss= 1.87420 accuracy= 0.68352 time= 2.25729

accuracy 0.67312
auc 0.55185
f1_score 0.28111
Job finished!



Initializing scat onedecoder
Namespace(alpha=1, att=2, beta=1, clique_size=50, cuda=False, dataset='cora_full', decoder=1, device=device(type='cuda'), dim_reduce=1, dropout=0.01, epochs=300, gamma=0.2, hidden1=0.5, hidden2=0.25, k=10, lr=0.002, num_clique=45, weight_decay=0.0005)
random seed: 896
Found existing ad data, loading...
feature_dim: 8710 hidden1_dim: 4355 hidden2_dim: 2177 nodes_num: 19793 anomaly_num: 4500

Start modeling
using wavelet scattering transform
y_features shape after scatting (19793, 113230) <class 'numpy.ndarray'>
using FMS to reduce dim
FMS fitted with 51 steps!

y_features shape after FMS torch.Size([19793, 2177]) <class 'torch.Tensor'>
Epoch: 0001 train_loss= 0.38968 accuracy= 0.60865 time= 1.56313
Epoch: 0011 train_loss= 0.83312 accuracy= 0.60542 time= 1.52218
Epoch: 0021 train_loss= 0.36791 accuracy= 0.60723 time= 1.54308
Epoch: 0031 train_loss= 0.36800 accuracy= 0.60723 time= 1.55579
Epoch: 0041 train_loss= 0.36771 accuracy= 0.60713 time= 1.54973
Epoch: 0051 train_loss= 0.36822 accuracy= 0.60703 time= 1.54545
Epoch: 0061 train_loss= 0.36772 accuracy= 0.60703 time= 1.57247
Epoch: 0071 train_loss= 0.36777 accuracy= 0.60703 time= 1.57348
Epoch: 0081 train_loss= 0.36773 accuracy= 0.60703 time= 1.57734
Epoch: 0091 train_loss= 0.36775 accuracy= 0.60703 time= 1.58295
Epoch: 0101 train_loss= 0.36778 accuracy= 0.60703 time= 1.58452
Epoch: 0111 train_loss= 0.36780 accuracy= 0.60703 time= 1.59352
Epoch: 0121 train_loss= 0.36783 accuracy= 0.60703 time= 1.60049
Epoch: 0131 train_loss= 0.36786 accuracy= 0.60703 time= 1.60254
Epoch: 0141 train_loss= 0.36788 accuracy= 0.60703 time= 1.60484
Epoch: 0151 train_loss= 0.36791 accuracy= 0.60703 time= 1.61202
Epoch: 0161 train_loss= 0.36793 accuracy= 0.60703 time= 1.61794
Epoch: 0171 train_loss= 0.36795 accuracy= 0.60703 time= 1.62270
Epoch: 0181 train_loss= 0.36797 accuracy= 0.60703 time= 1.62377
Epoch: 0191 train_loss= 0.36799 accuracy= 0.60703 time= 1.62388
Epoch: 0201 train_loss= 0.36801 accuracy= 0.60703 time= 1.62797
Epoch: 0211 train_loss= 0.36803 accuracy= 0.60703 time= 1.62653
Epoch: 0221 train_loss= 0.36805 accuracy= 0.60703 time= 1.63096
Epoch: 0231 train_loss= 0.36807 accuracy= 0.60703 time= 1.63560
Epoch: 0241 train_loss= 0.36809 accuracy= 0.60703 time= 1.63825
Epoch: 0251 train_loss= 0.36811 accuracy= 0.60703 time= 1.65057
Epoch: 0261 train_loss= 0.36813 accuracy= 0.60703 time= 1.65320
Epoch: 0271 train_loss= 0.36814 accuracy= 0.60703 time= 1.66337
Epoch: 0281 train_loss= 0.36816 accuracy= 0.60703 time= 1.66105
Epoch: 0291 train_loss= 0.36818 accuracy= 0.60703 time= 1.67223

accuracy 0.60703
auc 0.25644
f1_score 0.13578
Job finished!
